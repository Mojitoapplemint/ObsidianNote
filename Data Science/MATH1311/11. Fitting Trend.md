# Data Often Has Relationship
Scatter plot can visualize the [[6. Visualizing Data#Relationships between data|relationship]] between two information
- we can describe these relationships quantitatively with mathematical function
	- Line: $f(x)=ax+b$
	- Curve: $f(x)=ax+bx^{\frac{1}{2}}+c$
	- $a,b,c$ are the parameters
- The best function is the one that **minimizes the square error**
$$\min_{a_{i}}\left( \sum(y_{i}-f(x_{i};a_{1}, a_{2}, \dots))^{2} \right)$$
- The best model in any circumstance is the one that has the strongest theoretical underpinnings and makes sense with what we know of the world around us

## Steps of Modeling
1. Compute error at each point
2. Square them and add them up
3. Parameters with the smallest total error are the 'best' parameter

## R-Squared Parameter
We need to do better than decide which line is better by look
$\to$ we can use R-Square to compare two models and determine which one fits better
$$R^{2}=1-\frac{\sum(f(x_{i})-y_{i})^{2}}{\sum(y_{i}-\mu)^{2}}$$
Where $\mu$ is the mean
- R-Squared measured the amount of variability in the data explained by the independent variable
- Always between 0 and 1. Closer to the 1, the model is better 

It can be used for anything that has linear parameters 
- All the parameters have exponent 1
- Every terms looks like $\text{(parameter)}\cdot \text{(variables or other stuff)}$

But, highest R-Square *isn't necessarily the best* if the model doesn't **make any sense in terms of context**
- The best model in any circumstance is the one that has the **strongest theoretical underpinnings** and **make sense** with what we know of the world around us

## Fitting in Python
Use `lmfit` package
- Import only some: `from lmfit import mininize paramaters, fit_report`

### Parameter
- Parameters will be stored in a special kind of dictionary
	- `param = Paramaters()`
- And we can add parameter with
	- `params.add('Name', value = #)`
	- `value` can be anything because python will automatically adjust it

### Defining function
- Use `def` to let python that it is a function

```python
def f(x, params):
	
	a0 = params['a0'].value
	a1 = params['a1'].value
	
	y=a1*x+a0
	
	return y
```

### Error Function
- We also need to tell Python what type of error function we want to look at
$$E(a_{0}, a_{1})=\sum(y_{i}-f(x_{i};a_{0}, a_{1}))^{2}$$
```python
def E(params, x, y):
	
	model_values = f(x, params)
	
	# Python square it automatically 
	error = y-model_values
	
	return error
```

### Minimizing Error
`result = minimize(E, params, args=(x,y))`
- `args(x,y)` is defining the input and output data

Printing
`print(fit_report(result))`

### Print
![[Pasted image 20240324114818.png]]****

# Dummy Variable
We can plot categorical data by changing it into numerical data
- Since every event can be interpret as $A$(Event happen) and $A^{c}$(Not happen) 
- We can map these choice to 0 or 1
	- This is called **Dummy Variable**
- The number *means nothing other than indication* of presence
- Works great when we have exactly two categories
	- If we use more than 0 and 1, we automatically create the order
	- Instead, we can create more dummy variable
	- If we have $n$ categories, we need $n-1$ dummy variables

## Different Modeling
$W=f(t,s)=a_{1}t+a_{0}+bs$
- Assumes that the slope is the same and only depends on time
$W=f(t,s)=(a_{1}+cs)t+a_{0}$
- Assumes that the rate of increase of salary depends on gender
$W=f(t,s)=(a_{1}+cs)t+a_{0}+bs$
- Using both

## In Python
Python will create $n$ dummy variable columns when there are $n$ unique categories
- `pd.get_dummies(data, columns = ['NameOfColumn'])`
- `pd.get_dummies(data, dtype=int)`
- Original column will be replaced

Avoiding Numerical Column
- `pd.get_dummies(data, drop_first=True)`

### Dropping Column
`data.drop(['ColumnName'], axis=1)`



