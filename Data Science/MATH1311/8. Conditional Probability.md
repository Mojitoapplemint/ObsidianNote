The probability of an event happening, based on the existence of a previous event or outcome
- Probability keep updating/reallocating based on what we know

# Notation: $P(A|B)$
- Probability of $A$ given $B$
$$P(A|B)=\frac{P(A\cap B)}{P(B)}\implies P(A\cap B)=P(A|B)\cdot P(B)$$
**Note:** Dividing with $P(B)$ is re-normalizing the sample space

- If $A$ and $B$ are independent, 
$$P(A|B)=\frac{P(A)P(B)}{P(B)}=P(A)$$
- Asymmetric
$$P(A|B)\neq P(B|A)$$

## Bayes' Rule
$$P(A\cap B) = P(A|B)\cdot P(B)$$
$$P(B\cap A)=P(B|A)\cdot P(A)$$
Since $P(B\cap A)=P(A\cap B)$
$$P(A|B)\cdot P(B)=P(B|A)\cdot P(A)$$
$$\therefore P(A|B)=\frac{P(B|A)\cdot P(A)}{P(B)}$$
- Thus, not necessarily symmetric
- Conditional Probabilities are **sensitive to the prior**

# (Simplified) Law of Total Probability
- If we have two events $A$ and $B$ then, 
$$P(A)=P(A|B)\cdot P(B)+P(A|B^{c})\cdot P(B^{c})$$
- $A$ happened in the presence of $B$ , or in the presence of $\neg B$
- $P(A|B)$: [[True, False, Positive, Negative#True Positive|True Positive]]
- $P(A|B^{c})$: [[True, False, Positive, Negative#True Negative|True Negative]]
	- They are not complement to each other

## Proof
$$P(A|B)\cdot P(B)+P(A|B^{c})\cdot P(B^{c})=P(A\cap B)+P(A\cap B^{c})=P(A)$$

# Law of Total Probability
- $\Omega$ as our sample space, let's say we can break up our sample space into possible events
$$\Omega = A_{1}\cup A_{2}\cup\dots A_{n}$$
- If we have $B$, but only know $B$ in  the context of each event $A_{n}$
$$P(B)=P(B\cap A_{1})+P(B\cap A_{2})+\dots P(B\cap A_{n})=\sum^{n}_{i=1}P(B|A_{i})\cdot P(A_{i})$$
[[Chlamydia Example]]

# Bayesian Interpretation of Probability
- Bayesian Probability defines a probability as a **level of confidence in a belief**
	- 0: We do not believe that it is true
	- 1: We are absolutely certain that it is true
- Deductive
	- If we know nothing, then all outcomes have equal probability
		- We are forced to give all options equal probability
	- Adding new information requires us to adjust our beliefs
	- Future information must be considered *with the previous one*

## Terminology
$$P(A|B)=\frac{P(B|A)\cdot P(A)}{P(B)}$$
### $P(A)$: Prior probability
- Encompasses everything we know before the addition of new information
- After considering new information, the posterior becomes our new prior

### $P(A|B)$: Posterior probability of $A$
- Our belief in $A$ after considering any new information $B$ and our prior information
- Always a compromise of sorts between out old belief and our new information
	- If out prior belief is extremely strong, we would need even stronger information to change prior, or vise versa

### $P(B|A)$: Likelihood Function
- Gives a measure of how likely we are to see the information $B$
	- If B can be observed only $A$ is observed, then $P(B|A)=1$
	- If $A$ and $B$ are mutually exclusive, then $P(B|A)=0$

### $P(B)$: Normalization
- All other aspects requires presence of $B$
	- Only apply Baye's Theorem if $B$ is observed
	- We must know the probability with which we would observe it

# Belief
- All elements above comes together and give a measure of belief
- We can *build a belief iteratively*, as new information becomes available

