After plotting the data(either numerical or dummy variable), we can calculate the probability of class
![[Pasted image 20240324133311.png|500]]

And this curve that fits the data
![[Pasted image 20240324133515.png|500]]

# Logistic Curve
![[Pasted image 20240324135345.png]]
$$y=f(x;b_{0},b_{1})=\frac{1}{1+e^{-(b_{0}+b_{1}x)}}$$
We can't perfectly predict a categorical variable
- But we can predict it by predict the probability of associated dummy variable being 1
- If that probability is bigger than some threshold we set(orange line), we say that it belongs to the category
- It doesn't make sense anymore to consider error between the curve and the data because they are measuring different thing
	- Curve measures probability and data measure either $A$ or $A^{c}$

# Likelihood
Instead, we want the parameters $b_{0}$ and $b_{1}$ that most likely lead to the probability distribution we see in our data
- This is another way of saying we want to maximize the probability of our data given $b_{0}$ and $b_{1}$
- We call this **likelihood function**
$$L(b_{0}, b_{1})=\prod_{i}P(x_{i};b_{0},b_{1})$$
- Since we can either succeed (get 1) or fail (get 0) this our probability mass function is a special case of the binomial distribution called **Bernoulli Distribution**
$$\text{Bern}(k;p)=B(k,p,1)=p^{k}(1-p)^{1-k}$$
where, in this case,
$$p=f(x)=\frac{1}{1+e^{-(b_{0}+b_{1}x)}}$$
Thus, 
$$L(b_{0}, b_{1})=\prod_{i}\left(\frac{1}{1+e^{-(b_{0}+b_{1}x)}}\right)^{k}\left(1-\frac{1}{1+e^{-(b_{0}+b_{1}x)}}\right)^{1-k}$$
- Where $k=y_{i}$ either 0 or 1 (the dummy variable for our category)
- We want values of $b_{0}$ and $b_{1}$ **that gives largest** $L$ 

