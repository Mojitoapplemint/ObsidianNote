MARL algorithms are designed to learn a joint policy that satisfies the properties of a specific solution concept (e.g., Nash equilibrium)

1. Policies are conditioned on observation histories
	- How many possible observation each agent can have?
	- $\{ 1,2,3 \}\times \{ 3, \{ 4,5 \} \}\implies |O_{i}|=6$ local observation(projection)
2. All agents share the common reward function

Q table
- Column : All possible joint action
	- Row: All possible Observation

# Reward Function
1. 

# Limitation
The computation is not that efficient(?)
- [[marl-book.pdf#page=112&selection=50,0,52,38|The Complexity of Computing Equilibria]]



