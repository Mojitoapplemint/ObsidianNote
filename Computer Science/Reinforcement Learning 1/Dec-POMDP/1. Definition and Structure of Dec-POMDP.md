# Definition
Dec-POMDP is defined as tuple $\mathcal{M}=\langle\mathbb{D}, \mathbb{S, A}, T, \mathbb{O}, O, R, h, b_{0}\rangle$
- $\mathbb{D}=\{ 1,2,\dots n \}$ is the set of $n$ agents
- $\mathbb{S}$ is a (finite) set of states
- $\mathbb{A}$ is the set of joint actions
- $T$ is the transition probability function
	- $\text{Pr}(s'\text{ | }s,a)$
- $\mathbb{O}$ is the set of joint observations
- $O$ is the obervation probability function
- $R$ is the immediate reward function
- $h$ is the horizon of the problem
	- The number of time steps during agents interacting with their environment
- $b_{0}=\Delta(\mathbb{S})$ is the initial state distribution at time $t=0$
	- $\Delta(\cdot)$ refers to simplex, the set of probability distribution over set $(\cdot)$

## Additional Definition
### Action
$$\mathbb{A}=\times_{i\in \mathbb{D}}\mathbb{A}_{i}$$
- Where $\mathbb{A}_{i}$ is the set of actions available to agent $i$
- Assume that $\mathbb{A}_{i}$ does not depend on the state

Suppose each agent $i$ takes an action at time step $t$, $a^{t}_{i}$, then tuple of all actions made by agents $a^{t}=\langle a^{t}_{1}, a^{t}_{2}\dots a^{t}_{n} \rangle$
- They do not observe each other's actions

### Observation
$$\mathbb{O}=\times_{i\in \mathbb{D}}\mathbb{O}_{i}$$
- Where $\mathbb{O}_{i}$ is the set of actions available to agent $i$

Every time step, the environment emits one joint obervation $o=\langle o^{1}\dots o^{t} \rangle$
- The observation function $O$ specifies the probability $\text{Pr}(o\text{ | }a,s')$

### Reward
$$R:\mathbb{S}\times \mathbb{A}\to\mathbb{R}$$
- Maps states and joint actions to immediate reward

In Dec-POMDP, the agents are assumed not to observe the immediate rewards
- It may convery information regarding the true state which is not included in observation
- The only thing that matters is the expectation of cumulative future reward
	- Available in the offline planning phase
- It is not even assumed that the actual reward can be observed at the end of the episode
- If rewards are to be observed, they should be part of the obesrvation

# Observability of Dec-POMDP
Fully-Observable (Individually Observable)
- When observation function is such that individual observation for each of the agents will always uniquely identify the true state

Non-observable
- None of the agents obesrves any useful information

Jointly Observable
- Not individual, but the joint observation identifies the true state
	- Each agent still has a partial view
- Called Decentralized Markov Decision Process (Dec MDP)

# Markov Multiagent Environment
MME is defined as tuple $\mathcal{M}=\langle\mathbb{D}, \mathbb{S, A}, T, \mathbb{O}, O, \mathbb{R}, h, b_{0}\rangle$ where
- $\mathbb{R}=\{ R_{1}\dots R_{n} \}$ is the set of immediate reward functions for all agents
- All other components follows definition of Dec-POMDP

Since all agents in Dec-POMDP is collaborative,
$$\forall \text{ }i,j\quad R_{i}(s,a)=R_{j}(s,a)$$
- All agents have the same reward function

# Agent Model
A model for agent $i$ is a typle $m_{i}=\langle \mathbb{I}_{i}, I_{i}, \mathbb{A}_{i}, \mathbb{O}_{i}, \mathbb{Z}_{i}, \pi_{i}, l_{i} \rangle$ where
- $\mathbb{I}_{i}$ is the set of information states (or beliefs)
- $I_{i}$ is the current internal state of the agent
- $\mathbb{A}_{i}, \mathbb{O}_{i}$ : The actions ta

