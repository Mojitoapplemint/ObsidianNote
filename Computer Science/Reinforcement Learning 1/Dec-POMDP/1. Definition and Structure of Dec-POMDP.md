# Action
$$\mathbb{A}=\times_{i\in \mathbb{D}}\mathbb{A}_{i}$$
- Where $\mathbb{A}_{i}$ is the set of actions available to agent $i$
- Assume that $\mathbb{A}_{i}$ does not depend on the state

Suppose each agent $i$ takes an action at time step $t$, $a^{t}_{i}$, then tuple of all actions made by agents $a^{t}=\langle a^{t}_{1}, a^{t}_{2}\dots a^{t}_{n} \rangle$
- They do not observe each other's actions
# Observation
$$\mathbb{O}=\times_{i\in \mathbb{D}}\mathbb{O}_{i}$$
- Where $\mathbb{O}_{i}$ is the set of actions available to agent $i$

Every time step, the environment emits one joint obervation $o=\langle o^{1}\dots o^{t} \rangle$
- The observation function $O$ specifies the probability $\text{Pr}(o\text{ | }a,s')$
# Reward
$$R:\mathbb{S}\times \mathbb{A}\to\mathbb{R}$$
- Maps states and joint actions to immediate reward

In Dec-POMDP, the agents are assumed not to observe the immediate rewards
- It may convery information regarding the true state which is not included in observation
- The only thing that matters is the expectation of cumulative future reward
	- Available in the offline planning phase
- It is not even assumed that the actual reward can be observed at the end of the episode
- If rewards are to be observed, they should be part of the obesrvation

# Belief
In Partially Observable system, an agent no longer knows the state of the world, but rather has to maintain a belief over states
- It can use the history of observations to estimate the probability of each state and use this information to decide upon an action

$$\forall_{s_{t}} \text{ }b(s_{t})=\text{Pr}(s_{t}\text{ | }o_{t}, a_{t-1}, \dots a_{1}, o_{1}, a_{0})$$

In Multiagent POMDP, each agent can compute the joint belief, the probability distribution over states given the histories of joint actions and obervations

# Observability of Dec-POMDP
Fully-Observable (Individually Observable)
- When observation function is such that individual observation for each of the agents will always uniquely identify the true state

Non-observable
- None of the agents obesrves any useful information

Jointly Observable
- Not individual, but the joint observation identifies the true state
	- Each agent still has a partial view
- Called Decentralized Markov Decision Process (Dec MDP)

# Markov Multiagent Environment
MME is defined as tuple $\mathscr{M}=\langle\mathbb{D}, \mathbb{S, A}, T, \mathbb{O}, O, \mathbb{R}, h, b_{0}\rangle$ where
- $\mathbb{D}=\{ 1,2,\dots n \}$ is the set of $n$ agents
- $\mathbb{S}$ is a (finite) set of states
- $\mathbb{A}$ is the set of joint actions
- $T$ is the transition probability function
	- $\text{Pr}(s'\text{ | }s,a)$
- $\mathbb{O}$ is the set of joint observations
- $O$ is the obervation probability function
- $\mathbb{R}=\{ R_{1}\dots R_{n} \}$ is the set of immediate reward functions for all agents
- $h$ is the horizon of the problem
	- The number of time steps during agents interacting with their environment
- $b_{0}=\Delta(\mathbb{S})$ is the initial state distribution at time $t=0$
	- $\Delta(\cdot)$ refers to simplex, the set of probability distribution over set $(\cdot)$

Since all agents in Dec-POMDP is collaborative,
$$\forall \text{ }i,j\quad R_{i}(s,a)=R_{j}(s,a)$$
- All agents have the same reward function

# Agent Model
A model for agent $i$ is a tuple $m_{i}=\langle \mathbb{I}_{i}, I_{i}, \mathbb{A}_{i}, \mathbb{O}_{i}, \mathbb{Z}_{i}, \pi_{i}, l_{i} \rangle$ where
- $\mathbb{I}_{i}$ : the set of information states (or beliefs)
- $I_{i}$ : the current internal state of the agent
- $\mathbb{A}_{i}, \mathbb{O}_{i}$ : The actions taken by / observations that the environment provides to agent $i$
- $\mathbb{Z}_{i}$ : The set of auxiliary observations $\mathscr{z}_{i}$ available to agent $i$ (e.g communication)
- $\pi_{i}$ : Stochastic action selection policy $\pi_{i}$ : $\mathbb{I}_{i}\to\Delta(\mathbb{A}_{i})$
- $l_{i}$ : Stochatic information state function (or belief update function) $l_{i} : \mathbb{I}_{i}\times \mathbb{A}_{i}\times \mathbb{O}_{i}\times \mathbb{Z}_{i}\to\Delta(\mathbb{I}_{i})$

## Agent Component
Specifies how the agents update their internal state, which in turn dictates their action
- Handles the specification of the entire team of agents and their internal working

Fully-specified agent component, can be formalized as a tuple $m=\langle \mathbb{D}, \{ \mathbb{I}_{i} \}, \{ I_{i,0} \}, \{ \mathbb{A}_{i} \}, \{ \mathbb{O}_{i} \}, \{ \mathbb{Z}_{i} \}, \{ \pi_{i} \}, \{ l_{i} \} \rangle$ where
- $\mathbb{D}=\{ 1,\dots,n \}$ is the set of $n$ agents
- $\{ \mathbb{I}_{i} \}$ : the set of information states for each agent
- $\{ I_{i,0} \}$ : The initial internal states of each agent
- $\{ \mathbb{A}_{i} \}$ : Set of actions
- $\{ \mathbb{O}_{i} \}$ : Set of observations
- $\{ \mathbb{Z}_{i} \}$ : Set of auxiliary observations
- $\{ \pi_{i} \}$ : Policies, that map from internal states to actions
- $\{ l_{i} \}$ : Information state functions for each agent

# Definition of Dec-POMDP
Dec-POMDP is a tuple $\langle OC, \mathscr{M}, m \rangle$ where
- $OC$ is the optimality criterion
- $\mathscr{M}$ is an MME
- $m=\langle \mathbb{D}, \cdot, \cdot, \{ \mathbb{A}_{i} \}, \{ \mathbb{O}_{i} \}, \{ \mathbb{Z}_{i} =\emptyset\}, \cdot, \cdot \rangle$ is a partially specified agent component
	- There is no auxiliary observation
	- Each agent can form its internal state
	- Act based only on its local actions and observations

The goal for the problem optimizer for a Dec-POMDP is to specify the elements of m that are not specified
- The action selection policies need to be optimized and choices need to be made with respect to the represen- tation and updating of information state

