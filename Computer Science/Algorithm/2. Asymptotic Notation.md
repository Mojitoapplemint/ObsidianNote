# Asymptotic Notation
$$\text{"Suppress Constant Factors and Lower-Order Terms"}$$
- Constant factors generally highly depend on the environment
- Lower Oder terms become increasingly irrelevant as inputs is getting large
- Consider $n$ approaches infinity (When size of input is infinitely large)

$6n\log_{2}(n)+6n$
 $\implies \mathcal{O}(n\log(n))$ (Big-O Notation)

# Big-O Notation ($\leq$)
$T(n)=\mathcal{O}(f(n))$ iff there exist positive constant $c$ and $n_{0}$ such that 
$$T(n)\leq  c\cdot f(n)$$ for all $n\geq n_{0}$
- There is function $T(n)$, defined on the positive integers
- It will denote a bound on the worst-case running time of an algorithm as a function of input size $n$
- $T(n)=\mathcal{O}(f(n))$ iff $T(n)$ is eventually **bounded above** by a constant multiple of $f(n)$
	- No matter how big $n_{0}$ is since asymptotic will anyway consider even larger state

# Big-Omega Notation ($\geq$)
$T(n)=\Omega(f(n))$ iff there exist positive constants $c$ and $n_{0}$ such that
$$T(n)\geq  c\cdot f(n)$$ for all $n\geq n_{0}$
- $T(n)$ is big-omega of $f(n)$ iff $T(n)$ is eventually **bounded below** by a constant multiple of $f(n)$

# Big-Theta Notation (=)
$T(n)=\Theta(f(n))$ iff there exist positive constants $c_{1}$, $c_{2}$ and $n_{0}$ such that
$$c_{1}\cdot f(n)\leq  T(n)\leq  c_{2}\cdot f(n)$$ for all $n\geq n_{0}$
- $T(n)$ is eventually **sandwiched** between two different constant multiples of $f(n)$
	- $T(n)=\Theta(n)$ iff $T(n)=\mathcal{O}(n)$ and $T(n)=\Omega(n)$


