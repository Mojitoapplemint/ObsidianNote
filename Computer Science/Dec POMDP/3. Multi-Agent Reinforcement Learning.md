# General Learning Process
Model
- Defines MA environment and how agents may interact
- E.g. stochastic game, POSG, etc

Data
- A set of $\mathscr{z}$ histories
$$\mathcal{D}^{\mathscr{z}}=\{ h^{t_{e}}\text{ | }e=1,\dots\mathscr{z} \}, \mathscr{z}\geq0$$
- Each history $h^{t_{e}}$ was produced by a jiont policy $\pi^{e}$ used durint episode $e$

Learning Algorithm
- It takes collected data and current joint policy and return a new joint policy
$$\pi^{\mathscr{z}+1}=\mathbb{L}(\mathcal{D}^{\mathscr{z}}, \pi^{\mathscr{z}})$$
- Initial joint policy $\pi^{0}$ is typically random

Learning Goal
- Goal of learning is joint policy $\pi^{*}$ that satisfies the properties of chosen solution concept
- e.g. Nash Equilibrium

## Joint Policy Dependency
The chosen game model determines the conditioning of the learned joing policy
- [[marl-book.pdf#page=120&selection=96,0,97,8|marl-book, page 120]]
- In POSG, policies are conditioned on observation histories $h^{t}_{i}=(o^{0}_{i}\dots ,o^{t}_{i})$
	- We may use only the most recent $k$ observations


# Convergence Type

## Main Convergence
One of the criterion for learning is convergence of the joint policy $\pi^{\mathscr{z}}$ to a solution $\pi^{*}$ of the game in the limit of infinite data
$$\lim_{ \mathscr{z} \to \infty } \pi^{\mathscr{z}}=\pi^{*}$$
- $\pi^{*}$ may not be unique

Practically, learning typicallt stops after a predefined budget is reached or once the changes in policies are below some predetermined threshold

## Weaker types of convergence
Often used when a particular learning algorithm is not technically able to achieve the main convergence
- [[marl-book.pdf#page=121&selection=75,0,77,20|marl-book, page 121]]
### Convergence of empirical distribution

$$\forall \text{ }i\in I,\quad\lim_{ \mathscr{z} \to \infty } U_{i}(\pi^{\mathscr{z}})=U_{i}(\pi^{*})$$
In the limit of infinite data, the expected joint return under the learned joint policy will converge to the expected joint return of a solution

### Convergence of empirical distribution
$$\lim_{ \mathscr{z} \to \infty } \bar{\pi}^{\mathscr{z}}=\pi^{*}$$
where $\bar{\pi}^{\mathscr{z}}(a\text{ | }h)=\frac{1}{\mathscr{z}}\sum^{\mathscr{z}}_{e=1}\pi^{e}(a\text{ | }h)$
- Averaged joint policy across episodes

# Single-Agent RL Reductions
The most basic approach to using RL to learn agent policies in multi-agent systems is to essentially reduce the multi-agent learning problem to a single- agent learning problem
## Central Learning
Trains a single central policy $\pi_{c}$, which receives the local obesrvations of all agents and selects an action for each agent, by selecting joint actions from $A=A_{1}\times\dots \times A_{n}$

### Central Q-Learning (CQL)
This algorithm maintains joint action values $Q(s,a)$ for joint actions $a\in A$
- Circumvents the multi-agent aspects of non-stationaryity and credit assignment problems

Practical Limitation
- Requires tranforming the joint reward $(r_{1},\dots,r_{n})$ into a single scalar reward $r$
	- Subtle for zero-sum game and general-sum game
- Training a policy over the joint-action space, we now have to solve a decision problem with an action space that grows exponentially in the number of agents
- Agents are often localized entities that are physically or virtually distributed. In such settings, communication from a central policy $\pi_{c}$ to the agents and vice versa may not be possible or desirable
	- Instead, require local agent policies based on local observation and independently from other agents

Application in Common-Reward Game
- We can use $r=r_{i}$ for all $i$
- It is guaranteed for $\pi_{c}$ to reach Pareto-optimal correlated equilibrium
	- It achieves maximum expected returns in each state $s\in S$
- This also means that no agent can unilaterally deviate from its action given by $\pi_{c}$ to improve its returns, making $\pi_{c}$ a correlated equilibrium

### CQL algorithm for stochastic games
Initialize : $Q(s,a)=0$ for all $s\in S$ and $a\in A=A_{1}\dots A_{n}$
Repeat for every episodes:
**for** $t=0,1,\dots$ **do**
- Observe current state $s'$
- with probability $\epsilon$ : choose random joint action $a^{t}\in A$
- Otherwise : Choose joing action $a^{t}=\arg\max_{a}Q(s^{t}, a)$
- Apply joint action $a^{t}$, observe reward $r^{t}_{1}, \dots r^{t}_{n}$ and next state $s^{t+1}$
- Transform $r^{t}_{1}\dots r^{t}_{n}$ into scalar reward $r^{t}$
- $Q(s^{t}, a^{t})\leftarrow Q(s^{t}, a^{t})+\alpha[r^{t}+\gamma\displaystyle\max_{a'}Q(s^{t+1}, a')-Q(s^{t}, a^{t})]$

## Independent Learning
Each agent $i$ learns its own policy $\pi_{i}$ using only its local history of own observations, actions, and rewards, while ignoring the existence of other agents
- Effects of other agent's actions are simply part of the environment dynamics in perspective of $i$

Pros
- Avoids the exponential growth in action spaces
- Still application when system requires local agent policies
- Does not require scalar transformation of joint reward

Cons
- Can be significantly affected by non-stationarity caused by the concurrent learning of all agents

From the perspective of each agent, policies of other agents become part of the environment's state transition function
$$\mathcal{T}_{i}(s^{t+1}\text{ | }s^{t}, a^{6})\propto \sum_{a_{-i}\in A_{-i}}\mathcal{T}(s^{t+1}\text{ | }s^{t}\langle a^{t}_{i}, a_{-i} \rangle)\prod_{j\neq i}\pi_{j}(a_{j}\text{ | }s^{t})$$
### Independent Q Learning
// Algorithm controls agent $i$
1. Initialize : $Q_{i}(s, a_{i})=0$ for all $s\in S, a_{i}\in A_{i}$
2. Repeat for every episode : 
3. **For** $t=0,1,2\dots$ **do** 
	1. Observe current state $s'$
	2. With probability $\epsilon$ : choose random joint action $a^{t}_{i}\in A_{i}$
	3. Otherwise : Choose joing action $a^{t}_{i}=\arg\max_{a_{i}}Q_{i}(s^{t}, a_{i})$
	4. (Meanwhile, other agents choose their action $a_{j}^{t}$)
	5. Observe own reward $r^{t}$ and next state $s^{t+1}$
	6. $Q(s^{t}, a^{t})\leftarrow Q(s^{t}, a^{t})+\alpha[r^{t}+\gamma\displaystyle\max_{a'}Q(s^{t+1}, a')-Q(s^{t}, a^{t})]$

# Challenges of MARL
## Non-Stationarity
Non-stationarity resulting from the continual co-adaptation of multiple agents as they learn from interactions with one another
- This can lead to cyclic dynamics

# Self-Play & Mixed Play
## Self-Play
All agents use the same learning algorithm
