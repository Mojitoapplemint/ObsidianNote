# General Learning Process
Model
- Defines MA environment and how agents may interact
- E.g. stochastic game, POSG, etc

Data
- A set of $\mathscr{z}$ histories
$$\mathcal{D}^{\mathscr{z}}=\{ h^{t_{e}}\text{ | }e=1,\dots\mathscr{z} \}, \mathscr{z}\geq0$$
- Each history $h^{t_{e}}$ was produced by a jiont policy $\pi^{e}$ used durint episode $e$

Learning Algorithm
- It takes collected data and current joint policy and return a new joint policy
$$\pi^{\mathscr{z}+1}=\mathbb{L}(\mathcal{D}^{\mathscr{z}}, \pi^{\mathscr{z}})$$
- Initial joint policy $\pi^{0}$ is typically random

Learning Goal
- Goal of learning is joint policy $\pi^{*}$ that satisfies the properties of chosen solution concept
- e.g. Nash Equilibrium

## Joint Policy Dependency
The chosen game model determines the conditioning of the learned joing policy
- [[marl-book.pdf#page=120&selection=96,0,97,8|marl-book, page 120]]
- In POSG, policies are conditioned on observation histories $h^{t}_{i}=(o^{0}_{i}\dots ,o^{t}_{i})$
	- We may use only the most recent $k$ observations


# Convergence Type
One of the criterion for learning is convergence of the joint policy $\pi^{\mathscr{z}}$ to a solution $\pi^{*}$ of the game in the limit of infinite data
$$\lim_{ \mathscr{z} \to \infty } \pi^{\mathscr{z}}=\pi^{*}$$
- $\pi^{*}$ may not be unique

Practically, learning typicallt stops after a predefined budget is reached or once the changes in policies are below some predetermined threshold




