# General Learning Process
Model
- Defines MA environment and how agents may interact
- E.g. stochastic game, POSG, etc

Data
- A set of $\mathscr{z}$ histories
$$\mathcal{D}^{\mathscr{z}}=\{ h^{t_{e}}\text{ | }e=1,\dots\mathscr{z} \}, \mathscr{z}\geq0$$
- Each history $h^{t_{e}}$ was produced by a jiont policy $\pi^{e}$ used durint episode $e$

Learning Algorithm
- It takes collected data and current joint policy and return a new joint policy
$$\pi^{\mathscr{z}+1}=\mathbb{L}(\mathcal{D}^{\mathscr{z}}, \pi^{\mathscr{z}})$$
- Initial joint policy $\pi^{0}$ is typically random

Learning Goal
- Goal of learning is joint policy $\pi^{*}$ that satisfies the properties of chosen solution concept
- e.g. Nash Equilibrium

## Joint Policy Dependency
The chosen game model determines the conditioning of the learned joing policy
- [[marl-book.pdf#page=120&selection=96,0,97,8|marl-book, page 120]]
- In POSG, policies are conditioned on observation histories $h^{t}_{i}=(o^{0}_{i}\dots ,o^{t}_{i})$
	- We may use only the most recent $k$ observations


# Convergence Type

## Main Convergence
One of the criterion for learning is convergence of the joint policy $\pi^{\mathscr{z}}$ to a solution $\pi^{*}$ of the game in the limit of infinite data
$$\lim_{ \mathscr{z} \to \infty } \pi^{\mathscr{z}}=\pi^{*}$$
- $\pi^{*}$ may not be unique

Practically, learning typicallt stops after a predefined budget is reached or once the changes in policies are below some predetermined threshold

## Weaker types of convergence
Often used when a particular learning algorithm is not technically able to achieve the main convergence
- [[marl-book.pdf#page=121&selection=75,0,77,20|marl-book, page 121]]
### Convergence of empirical distribution

$$\forall \text{ }i\in I,\quad\lim_{ \mathscr{z} \to \infty } U_{i}(\pi^{\mathscr{z}})=U_{i}(\pi^{*})$$
In the limit of infinite data, the expected joint return under the learned joint policy will converge to the expected joint return of a solution

### Convergence of empirical distribution
$$\lim_{ \mathscr{z} \to \infty } \bar{\pi}^{\mathscr{z}}=\pi^{*}$$
where $\bar{\pi}^{\mathscr{z}}(a\text{ | }h)=\frac{1}{\mathscr{z}}\sum^{\mathscr{z}}_{e=1}\pi^{e}(a\text{ | }h)$
- Averaged joint policy across episodes

# Single-Agent RL Reductions
The most basic approach to using RL to learn agent policies in multi-agent systems is to essentially reduce the multi-agent learning problem to a single- agent learning problem
## Central Learning
Trains a single central policy $\pi_{c}$, which receives the local obesrvations of all agents and selects an action for each agent, by selecting joint actions from $A=A_{1}\times\dots \times A_{n}$

### Central Q-Learning
This algorithm maintains joint action values $Q(s,a)$ for joint actions $a\in A$
- Circumvents the multi-agent aspects of non-stationaryity and credit assignment problems

Practical Limitation
- Requires tranforming the joint reward $(r_{1},\dots,r_{n})$ into a single scalar reward $r$
	- Subtle for zero-sum game and general-sum game

Application in Common-Reward Game
- We can use $r=r_{i}$ for all $i$
- It is guaranteed for $\pi_{c}$ to reach Pareto-optimal correlated equilibrium
	- It achieves maximum expected returns in each state $s\in S$
- This also means that no agent can unilaterally deviate from its action given by $\pi_{c}$ to improve its returns, making $\pi_{c}$ a correlated equilibrium

### Pseudocode



## Independent Learning