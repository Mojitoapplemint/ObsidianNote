# Joint Policy and Expected Return
Let $\hat{h}^{t}=\{ (s^{\mathcal{T}}, o^{\mathcal{T}}, a^{\mathcal{T}})_{\mathcal{T}=0}^{t-1}, s^{t},o^{t} \}=\{ s^{0}, o^{0}, a^{0}, \dots a^{t-1}, s^{t}, o^{t}\}$ be a **full history** up to time $t$,  consisting of the states, joint obesrvations, and joint actions of all agents in each time step before $t$, and $s^{t}$, $o^{t}$.

Function $\sigma(\hat{h}^{t})=(o^{0}, o^{1}, \dots o^{t})$ returns the history of joint observations from full history

Probability of joint observation:
$$\mathcal{O}(o^{t}\text{ | }a^{t-1}, s^{t})=\prod_{i\in I}\mathcal{O}_{i}(o^{t}_{i}\text{ | }a^{t-1}, s^{t})$$

**Assumption)**
1. Use discounted return
2. Standard Convention of absorving state
	- When episode reaches terminal state or maximum number of time step, the game will always transition into the same state with probability 1 and give reward of 0 to all agents
3. Observation functions and policies of all agent become deterministic once an absorbing state has been reached
	- Assign probability 1 to certain observation and action

## History Based expected Return
Given a joint policy $\pi$, we can define the expected return for agent $i$ under $\pi$ by enumerating all possible full histories and summing the returns in each history, weighted by the probability of generating the history under the POSG and joint policy Ï€

$$U_{i}(\pi)=\lim_{ t \to \infty } \mathbb{E}_{\hat{h}^{t}}\Big[u_{i}(\hat{h}^{t})\Big]$$
$$=\sum_{\hat{h}^{t}\in \hat{H}}\text{Pr}(\hat{h}^{t}\text{ | }\pi)u_{i}(\hat{h}^{t})$$
- $\hat{H}$ : Set that contains all full histories $\hat{h}^{t}$ for $t\to \infty$
- $\text{Pr}(\hat{h}^{t}\text{ | }\pi)$ : Probability of full history $\hat{h}^{t}$ under $\pi$
- $u_{i}(\hat{h}^{t})$ : Discounted return for agent $i$ in $\hat{h}^{t}$

$$\text{Pr}(\hat{h}^{t}\text{ | }\pi)=\mu(s^{0})\mathcal{O}(o^{0}\text{ | }\emptyset, s^{0})\prod^{t-1}_{\mathcal{T}=0}\pi(a^{\mathcal{T}}\text{ | }h^{\mathcal{T}})\mathcal{T}(s^{\mathcal{T+1}}\text{ | }s^{\mathcal{T}}, a^{\mathcal{T}})\mathcal{O}(o^{\mathcal{T}+1}\text{ | }a^{\mathcal{T}}, s^{\mathcal{T}+1})$$
- $\pi(a^{\mathcal{T}}\text{ | }h^{\mathcal{T}})$ : Probability of joint action $a^{\mathcal{T}}$ under joint policy $\pi$ after joint-observation history $h^{\mathcal{T}}$
- If all agents act independently, then $\pi(a^{\mathcal{T}}\text{ | }h^{\mathcal{T}})=\prod_{j\in I}\pi_{j}(a^{\mathcal{T}}_{j}\text{ | }h^{\mathcal{T}}_{j})$

$$u_{i}(\hat{h}^{t})=\sum^{t-1}_{\mathcal{T}=0}\gamma^{\mathcal{T}}\mathcal{R}_{i}(s^{\mathcal{T}}, a^{\mathcal{T}}, s^{\mathcal{T}+1})$$
with discount factor $\gamma\in[0,1]$

## Recursive Expected Return(Value)
Defining based on Bellman Equation
$$V^{\pi}_{i}(\hat{h})=\sum_{a\in A}\pi(a\text{ | }\sigma(\hat{h}))\text{ }Q^{\pi}_{i}(\hat{h}, a)$$
Value for agent $i$ when agents follow the joint policy $\pi$ after the full history $\hat{h}$

$$Q^{\pi}_{i}(\hat{h}, a)=\sum_{s'\in S}\mathcal{T}(s'\text{ | }s(\hat{h}), a)\Big[ \mathcal{R}_{i}(s(\hat{h}), a, s')+\gamma \sum_{o'\in O}\mathcal{O}(o'\text{ | }a,s')V^{\pi}_{i}(\langle\hat{h}, a, s', o'\rangle) \Big]$$
Value for agent $i$ when agent execute the joint action $a$ after $\hat{h}$ and then follow $\pi$ subsequently

Expected return for agent $i$ from the initial state of the game as
$$U_{i}(\pi)=\mathbb{E}_{s_{0}\sim \mu, ,o^{0}\sim \mathcal{O}(\cdot \text{ | }\emptyset, s_{0})}\Big[V^{\pi}_{i}(\langle s^{0}, o^{0}\rangle )\Big]$$

# Best Response
Let $\pi_{-i}$ be a set of policies for all agents other than agent $i$
- $\pi_{-i}=\{ \pi_{1},\dots \pi_{i-1}, \pi_{i+1}\dots \pi_{n} \}$

Best Response for agent $i$ to $\pi_{-i}$ is a policy $\pi_{i}$ that maximizes the expected return for $i$ when played against $\pi_{-i}$
$$\text{BR}_{i}(\pi_{-i})=\arg\max_{\pi_{i}}U_{i}(\langle \pi_{i},\pi_{-i} \rangle)$$
- $\langle \pi_{i},\pi_{-i} \rangle$ denotes the complete joint policy

**Note)** Best Response may not be unique

# Minimax
Solution concept defined for two agents zero sum game
- Tic-Tac-Toe, Go, Chess, etc

Minimax solutions always exists for every
- Two-agent zero-sum normal-form game
- Two agents zero-sum stochastic game with finite episode lenth
- Two agents zero-sum stochastic games with infinite episode length using discounted returns

## Formal Definition
Joint Policy $\pi=(\pi_{i}, \pi_{j})$ is a minimax solution if 
$$U_{i}(\pi)=\max_{\pi_{i}'}\min_{\pi_{j}'}U(\pi_{i}', \pi_{j}')=\min_{\pi_{i}'}\max_{\pi_{j}'}U(\pi_{i}', \pi_{j}')=-U_{j}(\pi)$$

Each agent uses a policy that is optimized against a worst-case opponent that seeks to minimize the agent's return
- $\max_{\pi_{i}'}\min_{\pi_{j}'}U(\pi_{i}', \pi_{j}')$ : Minimum expected return that agent $i$ can guarantee against any opponent
	- $\pi_{i}$ is called agent $i$'s maxmin policy
	- $U_{i}(\pi)$ : $i$'s maxmin value
- $\min_{\pi_{i}'}\max_{\pi_{j}'}U(\pi_{i}', \pi_{j}')$ : Minimum expected return that agent $j$ can force on agent $i$
	- $\pi_{j}$ : Minmax policy against $i$
	- $U_{i}(\pi)$ : $i$'s Minmax value

## Characteristic
Agent $i$ first announcing its policy followed by agent $j$ selecting its policy, is equivalent to agent $j$ first announcing its policy followed by agent $i$ selecting its policy

Each Agent using a best response policy against the other agent's policy
- $(\pi_{i}, \pi_{j})$ is minimax if $\pi_{i}\in\text{BR}_{i}(\pi_{j})$ and $\pi_{j}\in\text{BR}_{j}(\pi_{i})$

More than one minimax solution may exists, but all minimax solutions yield the same unique value $U_{i}(\pi)$
- This is called *minimax value of the game*

# Nash Equilibrium
Applies the idea of a mutual best response to general-sum games with two or more agents

In a general-sum game with $n$ agents, a joint policy $\pi=(\pi_{1}, \dots,\pi_{n})$ is a Nash Equilibium if
$$\forall \text{ }i, \pi_{i}':U_{i}(\pi_{i}', \pi_{-i})\leq U_{i}(\pi)$$

No agent can improve its expected return by changing its policy $\pi_{i}$, assuming the others' policies stay fixed
- Nash Equilibrium is a best response to the policies of other agent
- $\forall \text{ }i\in I, \pi_{i}\in\text{BR}_{i}(\pi_{-i})$

## Important Aspect
### May Deterministic or Probabilistic
It can be determinstic in that each policy $\pi_{i}$ in the equilibrium $\pi$ is deterministic
- $\pi_{i}(a_{i})=1$ for some $a_{i}\in A_{i}$

In general, it may be probabilistic in that the policies in the equilibrium use randomization
- $\pi_{i}(a_{i})<1$ 
- Some games only have probabilistic Nash Equilibria

### Multiple Equilibria
A game may have multiple equilibria, and each equilibrium may entail different expected returns for the agents
- It is important to determine which equilibrium the agents shuold try to converge

## Checking Nash Equilibrium
For each agent $i$, keep the other policies $\pi_{-i}$ fixed and compute an optimal best-response policy $\pi_{i}'$ for $i$
- If optimal policies $\pi_{i}'$ gives higher values than $i$'s policy in joint policy $\pi_{i}\in \pi$, i.e. if $U_{i}(\pi_{i}', \pi_{-i})>U_{i}(\pi_{i}, \pi_{-i})$, then $\pi$ is not Nath equilibrium

# $\epsilon$-Nash Equilibrium
Deriving strict Nash Equilibrium may require a lot of computation
- It may be good enough to compute a sufficiently close to equilibrium
- Agents could technically try to improve their returns but any such gains are sufficiently small

$\epsilon$-Nash Equilibrium relaxes the strict Nash equilibrium by requiring that no agent can improve its extected returns by more than some amount $\epsilon>0$ when deviating from its policy in the equilibrium

In a general-sum game with $n$ agents, a joint policy $\pi=(\pi_{1},\dots, \pi_{n})$ is an $\epsilon$-Nash Equilibrium for $\epsilon>0$ if 
$$\forall \text{ }i, \pi_{i}':U_{i}(\pi_{i}', \pi_{-i})-\epsilon\leq U_{i}(\pi)$$

# Correlated Equilibrium
In a general-sum normal-form game with $n$ agents, let $\pi_{c}(a)$ be a joint policy that assigns probabilities to joint action$a\in A$. Then, $\pi_{c}$ is a correlated equilibrium if for every agent $i\in I$ and every action modifier $\xi_{i}:A_{i}\to A_{i}:$
$$\sum_{a\in A}\pi_{c}(a)\mathcal{R}_{i}(\langle\xi_{i}(a_{i}), a_{-i}\rangle)\leq \sum_{a\in A}\pi_{c}(a)\mathcal{R}_{i}(a)$$
- Every agent knows the probability distribution $\pi_{c}(a)$ and its own recommended action $a_{i}$, no agent can unilaterally deviate from its recommended action in order to increase its expected return

# Pareto Optimality
## Pareto Domination and Optimality
A joint policy $\pi$ is Pareto dominated by another joint policy $\pi'$ if
$$\forall \text{ }i:U_{i}(\pi')\geq U_{i}(\pi)\text{ and }\exists \text{ }i:U_{i}(\pi')>U_{i}(\pi)$$