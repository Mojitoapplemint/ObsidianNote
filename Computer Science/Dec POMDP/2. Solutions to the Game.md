# Joint Policy and Expected Return
Let $\hat{h}^{t}=\{ (s^{\mathcal{T}}, o^{\mathcal{T}}, a^{\mathcal{T}})_{\mathcal{T}=0}^{t-1}, s^{t},o^{t} \}=\{ s^{0}, o^{0}, a^{0}, \dots a^{t-1}, s^{t}, o^{t}\}$ be a **full history** up to time $t$,  consisting of the states, joint obesrvations, and joint actions of all agents in each time step before $t$, and $s^{t}$, $o^{t}$.

Function $\sigma(\hat{h}^{t})=(o^{0}, o^{1}, \dots o^{t})$ returns the history of joint observations from full history

Probability of joint observation:
$$\mathcal{O}(o^{t}\text{ | }a^{t-1}, s^{t})=\prod_{i\in I}\mathcal{O}_{i}(o^{t}_{i}\text{ | }a^{t-1}, s^{t})$$

**Assumption)**
1. Use discounted return
2. Standard Convention of absorving state
	- When episode reaches terminal state or maximum number of time step, the game will always transition into the same state with probability 1 and give reward of 0 to all agents
3. Observation functions and policies of all agent become deterministic once an absorbing state has been reached
	- Assign probability 1 to certain observation and action

## History Based expected Return
Given a joint policy $\pi$, we can define the expected return for agent $i$ under $\pi$ by enumerating all possible full histories and summing the returns in each history, weighted by the probability of generating the history under the POSG and joint policy Ï€

$$U_{i}(\pi)=\lim_{ t \to \infty } \mathbb{E}_{\hat{h}^{t}}\Big[u_{i}(\hat{h}^{t})\Big]$$
$$=\sum_{\hat{h}^{t}\in \hat{H}}\text{Pr}(\hat{h}^{t}\text{ | }\pi)u_{i}(\hat{h}^{t})$$
- $\hat{H}$ : Set that contains all full histories $\hat{h}^{t}$ for $t\to \infty$
- $\text{Pr}(\hat{h}^{t}\text{ | }\pi)$ : Probability of full history $\hat{h}^{t}$ under $\pi$
- $u_{i}(\hat{h}^{t})$ : Discounted return for agent $i$ in $\hat{h}^{t}$

$$\text{Pr}(\hat{h}^{t}\text{ | }\pi)=\mu(s^{0})\mathcal{O}(o^{0}\text{ | }\emptyset, s^{0})\prod^{t-1}_{\mathcal{T}=0}\pi(a^{\mathcal{T}}\text{ | }h^{\mathcal{T}})\mathcal{T}(s^{\mathcal{T+1}}\text{ | }s^{\mathcal{T}}, a^{\mathcal{T}})\mathcal{O}(o^{\mathcal{T}+1}\text{ | }a^{\mathcal{T}}, s^{\mathcal{T}+1})$$
- $\pi(a^{\mathcal{T}}\text{ | }h^{\mathcal{T}})$ : Probability of joint action $a^{\mathcal{T}}$ under joint policy $\pi$ after joint-observation history $h^{\mathcal{T}}$
- If all agents act independently, then $\pi(a^{\mathcal{T}}\text{ | }h^{\mathcal{T}})=\prod_{j\in I}\pi_{j}(a^{\mathcal{T}}_{j}\text{ | }h^{\mathcal{T}}_{j})$

$$u_{i}(\hat{h}^{t})=\sum^{t-1}_{\mathcal{T}=0}\gamma^{\mathcal{T}}\mathcal{R}_{i}(s^{\mathcal{T}}, a^{\mathcal{T}}, s^{\mathcal{T}+1})$$
with discount factor $\gamma\in[0,1]$



## 

A solution to a game is a joint policy, $\pi=(\pi_{1}\dots \pi_{n})$, which satisfies certain requirements defined by the solution concept in terms of the expected return $U_{i}(\pi)$