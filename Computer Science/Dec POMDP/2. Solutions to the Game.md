# Joint Policy and Expected Return
Let $\hat{h}^{t}=\{ (s^{\mathcal{T}}, o^{\mathcal{T}}, a^{\mathcal{T}})_{\mathcal{T}=0}^{t-1}, s^{t},o^{t} \}=\{ s^{0}, o^{0}, a^{0}, \dots a^{t-1}, s^{t}, o^{t}\}$ be a **full history** up to time $t$,  consisting of the states, joint obesrvations, and joint actions of all agents in each time step before $t$, and $s^{t}$, $o^{t}$.

Function $\sigma(\hat{h}^{t})=(o^{0}, o^{1}, \dots o^{t})$ returns the history of joint observations from full history

Probability of joint observation:
$$\mathcal{O}(o^{t}\text{ | }a^{t-1}, s^{t})=\prod_{i\in I}\mathcal{O}_{i}(o^{t}_{i}\text{ | }a^{t-1}, s^{t})$$

**Assumption)**
1. Use discounted return
2. Standard Convention of absorving state
	- When episode reaches terminal state or maximum number of time step, the game will always transition into the same state with probability 1 and give reward of 0 to all agents
3. Observation functions and policies of all agent become deterministic once an absorbing state has been reached
	- Assign probability 1 to certain observation and action

## History Based expected Return
Given a joint policy $\pi$, we can define the expected return for agent $i$ under $\pi$ by enumerating all possible full histories and summing the returns in each history, weighted by the probability of generating the history under the POSG and joint policy Ï€

$$U_{i}(\pi)=\lim_{ t \to \infty } \mathbb{E}_{\hat{h}^{t}}\Big[u_{i}(\hat{h}^{t})\Big]$$
$$=\sum_{\hat{h}^{t}\in \hat{H}}\text{Pr}(\hat{h}^{t}\text{ | }\pi)u_{i}(\hat{h}^{t})$$
- $\hat{H}$ : Set that contains all full histories $\hat{h}^{t}$ for $t\to \infty$
- $\text{Pr}(\hat{h}^{t}\text{ | }\pi)$ : Probability of full history $\hat{h}^{t}$ under $\pi$
- $u_{i}(\hat{h}^{t})$ : Discounted return for agent $i$ in $\hat{h}^{t}$

$$\text{Pr}(\hat{h}^{t}\text{ | }\pi)=\mu(s^{0})\mathcal{O}(o^{0}\text{ | }\emptyset, s^{0})\prod^{t-1}_{\mathcal{T}=0}\pi(a^{\mathcal{T}}\text{ | }h^{\mathcal{T}})\mathcal{T}(s^{\mathcal{T+1}}\text{ | }s^{\mathcal{T}}, a^{\mathcal{T}})\mathcal{O}(o^{\mathcal{T}+1}\text{ | }a^{\mathcal{T}}, s^{\mathcal{T}+1})$$
- $\pi(a^{\mathcal{T}}\text{ | }h^{\mathcal{T}})$ : Probability of joint action $a^{\mathcal{T}}$ under joint policy $\pi$ after joint-observation history $h^{\mathcal{T}}$
- If all agents act independently, then $\pi(a^{\mathcal{T}}\text{ | }h^{\mathcal{T}})=\prod_{j\in I}\pi_{j}(a^{\mathcal{T}}_{j}\text{ | }h^{\mathcal{T}}_{j})$

$$u_{i}(\hat{h}^{t})=\sum^{t-1}_{\mathcal{T}=0}\gamma^{\mathcal{T}}\mathcal{R}_{i}(s^{\mathcal{T}}, a^{\mathcal{T}}, s^{\mathcal{T}+1})$$
with discount factor $\gamma\in[0,1]$

## Recursive Expected Return(Value)
Defining based on Bellman Equation
$$V^{\pi}_{i}(\hat{h})=\sum_{a\in A}\pi(a\text{ | }\sigma(\hat{h}))\text{ }Q^{\pi}_{i}(\hat{h}, a)$$
Value for agent $i$ when agents follow the joint policy $\pi$ after the full history $\hat{h}$

$$Q^{\pi}_{i}(\hat{h}, a)=\sum_{s'\in S}\mathcal{T}(s'\text{ | }s(\hat{h}), a)\Big[ \mathcal{R}_{i}(s(\hat{h}), a, s')+\gamma \sum_{o'\in O}\mathcal{O}(o'\text{ | }a,s')V^{\pi}_{i}(\langle\hat{h}, a, s', o'\rangle) \Big]$$
Value for agent $i$ when agent execute the joint action $a$ after $\hat{h}$ and then follow $\pi$ subsequently

Expected return for agent $i$ from the initial state of the game as
$$U_{i}(\pi)=\mathbb{E}_{s_{0}\sim \mu, ,o^{0}\sim \mathcal{O}(\cdot \text{ | }\emptyset, s_{0})}\Big[V^{\pi}_{i}(\langle s^{0}, o^{0}\rangle )\Big]$$

# Best Response
Let $\pi_{-i}$ be a set of policies for all agents other than agent $i$
- $\pi_{-i}=\{ \pi_{1},\dots \pi_{i-1}, \pi_{i+1}\dots \pi_{n} \}$

Best Response for agent $i$ to $\pi_{-i}$ is a policy $\pi_{i}$ that maximizes the expected return for $i$ when played against $\pi_{-i}$
$$\text{BR}_{i}(\pi_{-i})=\arg\max_{\pi_{i}}U_{i}(\langle \pi_{i},\pi_{-i} \rangle)$$
