# Joint Policy and Expected Return
Let $\hat{h}^{t}=\{ (s^{\mathcal{T}}, o^{\mathcal{T}}, a^{\mathcal{T}})_{\mathcal{T}=0}^{t-1}, s^{t},o^{t} \}=\{ s^{0}, o^{0}, a^{0}, \dots a^{t-1}, s^{t}, o^{t}\}$ be a **full history** up to time $t$,  consisting of the states, joint obesrvations, and joint actions of all agents in each time step before $t$, and $s^{t}$, $o^{t}$.

Function $\sigma(\hat{h}^{t})=(o^{0}, o^{1}, \dots o^{t})$ returns the history of joint observations from full history

Probability of joint observation:
$$\mathcal{O}(o^{t}\text{ | }a^{t-1}, s^{t})=\prod_{i\in I}\mathcal{O}_{i}(o^{t}_{i}\text{ | }a^{t-1}, s^{t})$$

**Assumption)**
1. Use discounted return
2. Standard Convention of absorving state
	- When episode reaches terminal state or maximum number of time step, the game will always transition into the same state with probability 1 and give reward of 0 to all agents
	- Observation functions and policies of all agent become deterministic once an absorbing state has been reached
3



A solution to a game is a joint policy, $\pi=(\pi_{1}\dots \pi_{n})$, which satisfies certain requirements defined by the solution concept in terms of the expected return $U_{i}(\pi)$