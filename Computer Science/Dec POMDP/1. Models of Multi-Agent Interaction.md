# Hierarchy of Game Models
![[Pasted image 20250118195621.png|500]]

# Normal-Form Games
Defines a single interaction between two or more agents

A normal-form game consists of
- Finite set of agents: $I=\{ 1,2,\dots n \}$
- For each agent $i\in I$ :
	- Finite set of action $A_{i}$
	- Reward function $\mathcal{R_{i}}:A\to\mathbb{R}$ where $A=A_{1}\times A_{2}\dots \times A_{n}$

## Procedure
1. Each agent $i\in I$ selects a policy $\pi_{i}:A_{i}\to[0,1]$
	- Policy assigns probability to the actions available to the agent
	- $\sum_{a_{i}\in A_{i}}\pi_{i}(a_{i})=1$
	- 
2. The resulting actions of all agents form a joint policy $a=(a_{1},a_{2}\dots a_{n})$
3. Each agent receives a reward based on its reward function and the joint action, $r_{i}=\mathcal{R}_{i}(a)$

## Classification
Zero-sum game: The sum of the agents' rewards is always 0
- If someone get positive reward, then someone gets negative
- $\sum_{i\in I}\mathcal{R}_{i}(a)=0$

Common-reward game: All agents receive the same reward
- $\forall \text{ }i,j\in I,\quad \mathcal{R}_{i}=\mathcal{R}_{j}$

Matrix game: Normal-form game with two agents
- In this case, the reward function can be compactly represented as matrix

General game: No restriction

## Repeated Normal-Form Games
By repeating normal-form game, we can extend this into sequential multi-agent interactions

For normal-form $\Gamma=(I,\{ A_{i} \}_{i\in I}, \{ \mathcal{R_{i}} \}_{{i\in I}})$, repeated normal-form game repeat this for $T$ times
- Each agent $i\in I$ samples an action $a^{t}_{i}\in A_{i}$ with probability given by its policy $\pi_{i}(a^{t}_{i}\text{ | }h^{t})$
- Where $h^{t}$ refers to *joint-action history* 
	- $h^{t}=(a^{0}, a^{1}\dots a^{t-1})$ conatains all joint actions before the current time step $t$
- Each agent receives a reward $r^{t}_{i}=\mathcal{R}_{i}(a^{t})$

In repeated normal-form games, policies can now make decisions based on the entire history of past joint actions
- Note that policies can be conditioned on past joint actions *in various way*
	- e.g. All joint actions are considered equally
	- e.g. More recent considerd more

### Finite / Infinite Repetition
In finitely repeated games, there can be "end game" effects
- If the agents know that a game will finish after T time steps, they may choose different actions closer to the end of the game compared to earlier in the game

Some infinitely repeated games specify a probability with which each time step terminates game
- The probability is related to the discount factor $\gamma\in[0,1]$ 
	- $1-\gamma$ specifies the probability of termination in each time step

# Stochastic Games
State-based environment in which the state evolves over time based on the agents' actions and probabilistis state transition

A stochastic game consists of
- Finite set of agents $I=\{ 1,2,\dots n \}$
- Finite set of states $S$, with subset of terminal states $\bar{S}\subset S$
- For each agent $i\in I$ : 
	- Finite set of actions $A_{i}$
	- Reward Function $\mathcal{R_{i}} : S\times A\times S\to\mathbb{R}$ where $A=A_{1}\times A_{2}\dots A_{n}$
- State transition probability functoin $\mathcal{T}:S\times A\times S\to[0,1]$
$$\forall \text{ }s\in S,a\in A, \quad\sum_{s'\in S}\mathcal{T}(s,a,s')=1$$
- Initial state distribution $\mu:S\to[0,1]$
$$\sum_{s\in S}\mu(s)=1\quad\text{and}\quad \forall \text{ }s\in \bar{S}, \text{ }\mu(s)=0$$

## Game Procedure
1. The game starts in an initial state $s^{0}\in S$ sampled from $\mu$
2. At time $t$, each agent $i\in I$ observes the current state $s^{t}\in S$ and chooses an action $a^{t}_{i}\in A_{i}$ with probability given by its policy $\pi_{i}(a^{t}_{i}\text{ | }h^{t})$
	- All actions from all agents form joint action $a^{t}=(a^{t}_{1}, a^{t}_{2}\dots a^{t}_{n})$
	- The policy is conditioned on *state-action history* $h^{t}=(s^{0}, a^{0}, s^{1}, a^{1}\dots s^{t})$
	- Full Observability: This history is observed by all agents
3. Given the state $s^{t}$ and joint action $a^{t}$, the game transitions into a next state $s^{t+1}\in S$ with probability given by $\mathcal{T}(s^{t}, a^{t}, s^{t+1})$, and each agent $i$ receives a reward $r^{t}_{i}=\mathcal{R_{i}}(s^{t}, a^{t}, s^{t+1})$
	- We also write $\mathcal{T}(s^{t+1}\text{ | }s^{t}, a^{t})$
4. Repeat these steps until reaching terminal state, or completing a maximum number of $T$ time steps

## Markov Property
Stochastic games have Markov property
- Probability of the next state and reward is conditionally independent of the past states and joint actions
$$\text{Pr}(s^{t+1}, r^{t}\text{ | }s^{t}, a^{t}, s^{t-1}, a^{t-1}\dots s^{0}, a^{0})=\text{Pr}(s^{t+1}, r^{t}\text{ | }s^{t}, a^{t})$$
where $r^{t}=(r^{t}_{1}\dots r^{t}_{n})$ is the joint reward at time $t$

# Partially Observable Stochastic Games
The agents receive "observations" that carry some incomplete information about the environment state and agent's actions

Defines state-observation probabilities $\text{Pr}(s^{t},o^{t}\text{ | }s^{t-1}, a^{t-1})$, where $o^{t}=(o^{t}_{1}, \dots o^{t}_{n})$ is joint observation at time $t$
- Contains agents' individual observations, $o^{t}_{i}$

Observations often only depend on the new environment $s^{t}$ and the joint actgion $a^{t-1}$
- It is common to define observation function $\mathcal{O}_{i}$ for each agent $i$ that specifies probabilites over the agent's possible observations $o^{t}_{i}$ given the state and joint action

## Formal Definition
POSG follows definition of stochastic game, but with additional definition for each agent $i\in I$ : 
- Finite set of observation $O_{i}$
- Observation function $\mathcal{O}_{i}:A\times S\times O_{i}\to[0,1]$ such that
$$\forall \text{ }a\in A, s\in S:\sum_{o_{i}\in O_{i}}\mathcal{(a,s,o_{i})}=1$$

$\mathcal{O}_{i}(a^{t-1}, s^{t}, o^{t}_{i})$ is also written as $\mathcal{O}_{i}(o^{t}_{i}\text{ | }a^{t-1}, a^{t})$ to emphasize that the probability is conditioned on the state and joint action

## Game Procedure
1. The game starts in an initial state $s^{0}\in S$ sampled from $\mu$
2. At time $t$, given the current state $s^{t}\in S$ and previous joint action $a^{t-1}\in A$ (for $t=0$, we est $a^{t-1}=\emptyset$), each agent receives an observation $o^{t}_{i}\in O_{i}$
3. Each agent chooses an action $a^{t}_{i}\in A_{i}$ based on action probabilities given by its policy $\pi_{i}(a^{t}_{i}\text{ | }h^{t}_{i})$ which forms joint action
	- $h^{t}$ here refers to agent $i$'s *observation history* $h^{t}_{i}=(o^{0}_{i}, \dots o^{t}_{i})$
	- Note that agent





1. Given the state $s^{t}$ and joint action $a^{t}$, the game transitions into a next state $s^{t+1}\in S$ with probability given by $\mathcal{T}(s^{t}, a^{t}, s^{t+1})$, and each agent $i$ receives a reward $r^{t}_{i}=\mathcal{R_{i}}(s^{t}, a^{t}, s^{t+1})$
	- We also write $\mathcal{T}(s^{t+1}\text{ | }s^{t}, a^{t})$
2. Repeat these steps until reaching terminal state, or completing a maximum number of $T$ time steps
