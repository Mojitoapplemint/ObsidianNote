# Hierarchy of Game Models
![[Pasted image 20250118195621.png|500]]

# Normal-Form Games
Defines a single interaction between two or more agents

A normal-form game consists of
- Finite set of agents: $I=\{ 1,2,\dots n \}$
- For each agent $i\in I$ :
	- Finite set of action $A_{i}$
	- Reward function $\mathcal{R_{i}}:A\to\mathbb{R}$ where $A=A_{1}\times A_{2}\dots \times A_{n}$

## Procedure
1. Each agent $i\in I$ selects a policy $\pi_{i}:A_{i}\to[0,1]$
	- Policy assigns probability to the actions available to the agent
	- $\sum_{a_{i}\in A_{i}}\pi_{i}(a_{i})=1$
2. The resulting actions of all agents form a joint policy $a=(a_{1},a_{2}\dots a_{n})$
3. Each agent receives a reward based on its reward function and the joint action, $r_{i}=\mathcal{R}_{i}(a)$

## Classification
Zero-sum game: The sum of the agents' rewards is always 0
- If someone get positive reward, then someone gets negative
- $\sum_{i\in I}\mathcal{R}_{i}(a)=0$

Common-reward game: All agents receive the same reward
- $\forall \text{ }i,j\in I,\quad \mathcal{R}_{i}=\mathcal{R}_{j}$

Matrix game: Normal-form game with two agents
- In this case, the reward function can be compactly represented as matrix

General game: No restriction

## Repeated Normal-Form Games
By repeating normal-form game, we can extend this into sequential multi-agent interactions

For normal-form $\Gamma=(I,\{ A_{i} \}_{i\in I}, \{ \mathcal{R_{i}} \}_{{i\in I}})$, repeated normal-form game repeat this for $T$ times
- Each agent $i\in I$ samples an action $a^{t}_{i}\in A_{i}$ with probability given by its policy $\pi_{i}(a^{t}_{i}\text{ | }h^{t})$
- Where $h^{t}$ refers to *joint-action history* 
	- $h^{t}=(a^{0}, a^{1}\dots a^{t-1})$ conatains all joint actions before the current time step $t$
- Each agent receives a reward $r^{t}_{i}=\mathcal{R}_{i}(a^{t})$

In repeated normal-form games, policies can now make decisions based on the entire history of past joint actions
- Note that policies can be conditioned on past joint actions *in various way*
	- e.g. All joint actions are considered equally
	- e.g. More recent considerd more

### Finite / Infinite Repetition
In finitely repeated games, there can be "end game" effects
- If the agents know that a game will finish after T time steps, they may choose different actions closer to the end of the game compared to earlier in the game

Some infinitely repeated games specify a probability with which each time step terminates game
- The probability is related to the discount factor $\gamma\in[0,1]$ 
	- $1-\gamma$ specifies the probability of termination in each time step

# Stochastic Games
State-based environment in which the state evolves over time based on the agents' actions and probabilistis state transition

A stochastic game consists of
- Finite set of agents $I=\{ 1,2,\dots n \}$
- Finite set of states $S$, with subset of terminal states $\bar{S}\subset S$
- For each agent $i\in I$ : 
	- Finite set of actions $A_{i}$
	- Reward Function $\mathcal{R_{i}} : S\times A\times S\to\mathbb{R}$ where $A=A_{1}\times A_{2}\dots A_{n}$
- State transition probability functoin $\mathcal{T}:S\times A\times S\to[0,1]$
$$\forall \text{ }s\in S,a\in A, \quad\sum_{s'\in S}\mathcal{T}(s,a,s')=1$$
- Initial state distribution $\mu:S\to[0,1]$
$$\sum_{s\in S}\mu(s)=1\quad\text{and}\quad \forall \text{ }s\in \bar{S}, \text{ }\mu(s)=0$$

## Game Procedure
1. The game starts in an initial state $s^{0}\in S$ sampled from $\mu$
2. At time $t$, each agent $i\in I$ observes the current state $s^{t}\in S$ and chooses an action $a^{t}_{i}\in A_{i}$ with probability given by its policy $\pi_{i}(a^{t}_{i}\text{ | }h^{t})$
	- All actions from all agents form joint action $a^{t}=(a^{t}_{1}, a^{t}_{2}\dots a^{t}_{n})$
	- The policy is conditioned on *state-action history* $h^{t}=(s^{0}, a^{0}, s^{1}, a^{1}\dots s^{t})$
	- Full Observability: This history is observed by all agents
3. Given the state $s^{t}$ and joint action $a^{t}$, the game transitions into a next state $s^{t+1}\in S$ with probability given by $\mathcal{T}(s^{t}, a^{t}, s^{t+1})$, and each agent $i$ receives a reward $r^{t}_{i}=\mathcal{R_{i}}(s^{t}, a^{t}, s^{t+1})$
	- We also write $\mathcal{T}(s^{t+1}\text{ | }s^{t}, a^{t})$
4. Repeat these steps until reaching terminal state, or completing a maximum number of $T$ time steps

## Markov Property
Stochastic games have Markov property
- Probability of the next state and reward is conditionally independent of the past states and joint actions
$$\text{Pr}(s^{t+1}, r^{t}\text{ | }s^{t}, a^{t}, s^{t-1}, a^{t-1}\dots s^{0}, a^{0})=\text{Pr}(s^{t+1}, r^{t}\text{ | }s^{t}, a^{t})$$
where $r^{t}=(r^{t}_{1}\dots r^{t}_{n})$ is the joint reward at time $t$

# Partially Observable Stochastic Games
The agents receive "observations" that carry some incomplete information about the environment state and agent's actions

Defines state-observation probabilities $\text{Pr}(s^{t},o^{t}\text{ | }s^{t-1}, a^{t-1})$, where $o^{t}=(o^{t}_{1}, \dots o^{t}_{n})$ is joint observation at time $t$
- Contains agents' individual observations, $o^{t}_{i}$

Observations often only depend on the new environment $s^{t}$ and the joint actgion $a^{t-1}$
- It is common to define observation function $\mathcal{O}_{i}$ for each agent $i$ that specifies probabilites over the agent's possible observations $o^{t}_{i}$ given the state and joint action

## Formal Definition
POSG follows definition of stochastic game, but with additional definition for each agent $i\in I$ : 
- Finite set of observation $O_{i}$
- Observation function $\mathcal{O}_{i}:A\times S\times O_{i}\to[0,1]$ such that
$$\forall \text{ }a\in A, s\in S:\sum_{o_{i}\in O_{i}}\mathcal{(a,s,o_{i})}=1$$

$\mathcal{O}_{i}(a^{t-1}, s^{t}, o^{t}_{i})$ is also written as $\mathcal{O}_{i}(o^{t}_{i}\text{ | }a^{t-1}, a^{t})$ to emphasize that the probability is conditioned on the state and joint action

## Game Procedure
1. The game starts in an initial state $s^{0}\in S$ sampled from $\mu$
2. At time $t$, given the current state $s^{t}\in S$ and previous joint action $a^{t-1}\in A$ (for $t=0$, we est $a^{t-1}=\emptyset$), each agent receives an observation $o^{t}_{i}\in O_{i}$
3. Each agent chooses an action $a^{t}_{i}\in A_{i}$ based on action probabilities given by its policy $\pi_{i}(a^{t}_{i}\text{ | }h^{t}_{i})$ which forms joint action
	- $h^{t}$ here refers to agent $i$'s *observation history* $h^{t}_{i}=(o^{0}_{i}, \dots o^{t}_{i})$
	- Note that agent's observation may or may not include other's action
4. With joint action, the game transitions into the next state $s^{t+1}\in S$ with probability $\mathcal{T}(s^{t+1}\text{ | }s^{t}, a^{t})$ and each agent $i$ receives reward $r_{i}=\mathcal{R}_{i}(s^{t}, a^{t}, s^{t+1})$
5. Repeat 1~4 until reaching a terminal state $s^{t}\in \bar{S}$

## Different Kinds
Dec-POMDP
- POSGs with common reward (all agents have the same reward function)

POMDP
- There is only a single agent

Stochastic Game
- When $o^{t}_{i}=(s^{t}, a^{t-1})$

Unobserved actions of other agents
- Agents may observe the state and their own previous action, but not the previous actions of the other agents
- $o^{t}_{i}=(s^{t}, a^{t-1}_{i})$

Limited view region
- Agents may observe a subset of the state and joint action, that is
	- e.g. Limited view of their surrounding environment
- $o^{t}_{i}=(\bar{s}^{t}, \bar{a}^{t})$ where $\bar{s}^{t}\subset s^{t}$ and $\bar{a}^{t}\subset a^{t}$

## Belief State and Filtering
In POSG, due to lack of complete obesrvation, the agent typically cannot choose optimal actions
- In general, the agents must maintain estimates of possible current staets and their relative likelihoods based on the history of observation

One way of defining such estimates about environment states from the perspective of agent $i$ is as a **belief state** $b^{t}_{i}$
- Probability distribution over possible states $s\in S$ that the environment may be in at time $t$

Updating belief state by computing Bayesian posterior distribution
$$b^{t+1}_{i}(s')\propto \sum_{s\in S}b^{t}_{i}(s)\text{ }\mathcal{T}(s'\text{ | }s, a^{t}_{i})\text{ }\mathcal{O_{i}}(o^{t+1}_{i}\text{ | }a^{t}_{i}, s')$$
- Updating based on observation is known as (belief state) filtering

When belief state carries enough information to choose optimal actions and make predictions about the future, it is called sufficient statistic

**Note)** We typically assume that agents do not possess complete knowledge about the elements of POSG, $S, \mathcal{T}, \mathcal{O_{i}}$ which are required for equation above.
- Thus, we will not dwell further on the complexity of defining exact belief states in multi-agent context

---
# Modeling Communication
We can model communication by including communication actions that agents can use to send messages to other agents

Consider communication as a type of action that can be observed by other agents but does not affect the environment.

Let action space of agent $i$ be
$$A_{i}=X_{i}\times M_{i}$$
- $X_{i}$ : Action that affect the environment state
- $M_{i}$ : Communication

Formally, let $M=\times_{i\in I}M_{i}$, then state transition probabilities are independent of $M$
$$\forall \text{ }s,s'\in S, a\in A, m\in M:\quad\mathcal{T}(s'\text{ | }s,a)=\mathcal{T}(s'\text{ | }a, \big((a^{x}_{1}, m_{1}), \dots (a^{x}_{n}, m_{n})\big))$$
where $a^{x}_{i}$ refers to the environment action component of agent $i$ in joint action $a$, and $\big((a^{x}_{1}, m_{1}), \dots (a^{x}_{n}, m_{n})\big)$ is a joint action

## Noisy and Unreliable Communication
We can model this via modifying observation function $\mathcal{O}_{i}(o^{t}_{i}\text{ | }a^{t-1}, s^{t})$.

Suppose $o^{t}_{i}=[\bar{s}^{t}, w^{t-1}_{1}, w^{t-1}_{2}\dots w^{t-1}_{n}]$
- $\bar{s}^{t}$ contains some partial information about state $s^{t}$
- $w^{t-1}_{j}$ is modified communication message $w^{t-1}_{j}=f(m^{t-1}_{j})$
	- $f$ is modifying the message
- $w^{t-1}_{j}$ may qu
