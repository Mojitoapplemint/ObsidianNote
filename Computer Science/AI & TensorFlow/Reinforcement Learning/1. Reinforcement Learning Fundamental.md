# Definition
Learning what to do -how to map situations to actions- so as *to maximize the numerical reward signal*
- **Trial-and-Error Search:** The learner is not told which actions to take, but discover which actions yield the most reward by trying them
- **Delayed Rewards:** Actions may affect not onlt the immediate rewards but also next situation and, through that, all subsequent rewards

## Markov Decision Process
Learning agent must 
1) Be able to *sense the state of its environment* to some extent
2) Be able to *take actions* that affect the state
3) *Have a goal* or goals relating to the state of environment

Will discuss deeply in Chapter 3

## Neither supervised nor unsupervised
In interactive problems, it is often impracical to obtain examples of desired behaviour that are both correvt and representative of all the situations in which the agent has to act
- Supervised Learning is not suitable

Since Reinforcement Learning is trying to maximize a reward signal instead of trying to find hidden structure, it is not unsupervised as well.

# Elements
## Policy
Agent's way of behaving at a given time
- Mapping from perceived states of the environment to actions to be taken when in those states
- Alone is sufficient to determine behaviour
	- Specifying probabilities for each action, etc)

## Reward Signal
Defines the goal of the problem
- Environment send a single number, reward, to the reinforcement learning agent
- Define good or bad
- Agent's objective is to maximize the total reward

### Reward Hypothesis
All goals can be described by the maximisation of expected cumulative reward

## Value Function
Long-Term desirability of states after taking into account the states that are likely to follow
- Total amount of reward an agent can expect to accumulate over the future
- Estimates values only to acheive more reward
- Actions choices are made based on value judgement
	- Seek actions led to highest value, not highest rewards for greatest reward over the long run

## Model (Optional)
Something that mimics/represents the behaviour of the environment
- Predicts what the environment will do next
- Allows inferences to be made about how the environment will behave
- Used for planning: predict the resultant next state and reward

Model-Based Methods
- Methods for solving reinforcement learning problems that use models and planning

Model-Free Methods
- Opposing Model-Based Methods

# Fundamental Concepts
## Agents
- The actor operating within the environment, it is usually governed by a policy
- Its actions affect the subsequent data it receives
	- Executes Action
	- Receives Observation and Scalar Reward

Value Based
- Has value function
- No policy

Policy Based
- Has Policy
- No value function

Actor Critic
- Has both

Model Free
- Policy and/or Value Function
- No Model

Model Based
- Policy and/or Value Function
- Model

## Environment
- The world in which the agent can operate in
	- Receives Action
	- Emits Observation and Scalar Reward

## Action $(A_{t})$
- The agent can do something within environment known as an action
- The only way to change the environment

## Reward $(R_{t})$ and Observation $(O_{t})$
- In return the agent receives a reward and a view of what the environment looks like after acting on it

## History $(H_{t})$
$$H_{t}=A_{1}, O_{1}, R_{1}\dots A_{t}, O_{t}, R_{t}$$
- Sequence of observations, actions, rewards
	- All observable variables up to time $t$
- What happens next depends on the history
	- The agent selects actions
	- THe environment selects observation/rewards

## State
$$S_{t}=f(H_{t})$$
Information used to determine what happens next
	- Function of History
	- Imformally means signal conveying to the agent some sense of "how the environment is" at a particular time
	- Whatever information is available to the agent about its environment

### Environment State $(S^{e}_{t})$
Environment's private representation
- Whatever data the environment uses to pick the next observation/reward
- Usually invisible to the agent

### Agent State $(S^{a}_{t})$
Agent's internal representation
- Information used by reinforcement learning algorithm

### Information State (Markov State)
Contains all useful information from the history
- The state $S_{t}$ is Markov iff
$$\mathbb{P}[S_{t+1}|S_{t}]=\mathbb{P}[S_{t+1}| S_{1}, S_{2}\dots S_{t}]$$
- Future is independent of the past, given the present

### Fully Observability
Agent directly observes environment state
- $O_{t}=S^{a}_{t}=S^{e}_{t}$
- Agent State=Environment State=Information State
- This is Markox Decision Process

### Partial Observability
Agent indirectly observes environment
- Know only partial information

Agent must construct its own state representation $S^{a}_{t}$
- Complete History: $S^{a}_{t}=H_{t}$
- Beliefs of environment state: $S^{a}_{t}=(\mathbb{P}[S^{e}_{t}=s^{1}],\dots,\mathbb{P}[S^{e}_{t}=s^{n}])$

