Classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards
- Involve delayed reward and need to tradeoff immediate and delayed reward
- Estimate the value $q_{*}(s,a)$ of each action a in each state $s$, or we estimate the value $v_{*}(s)$ of each state given optimal action selections

MDPs are a mathematically idealized form of the RL problem for which precise theoretical statements can be made. 

# MDP Framework
The MDP framework is a considerable abstraction of the problem of goal-directed learning from interaction. It proposes that whatever the details of the sensory, memory, and control apparatus, and whatever objective one is trying to achieve, any problem of learning goal-directed behavior can be reduced to three signals passing back and forth between an agent and its environment: one signal to represent the choices made by the agent (the actions), one signal to represent the basis on which the choices are made (the states), and one signal to define the agent’s goal (the rewards). This framework may not be su cient to represent all decision-learning problems usefully, but it has proved to be widely useful and applicable.

# The Agent-Environment Interface

![[Pasted image 20240927105631.png|500]]

Agent Selecting actions and the environment responding to these actions and presenting new situations to the agent

At each time step $t$, the agent receives some representation of the environment's [[1. Reinforcement Learning Fundamental#State|state]], $S_{t}\in \mathcal{S}$, and on that basis selects an [[1. Reinforcement Learning Fundamental#Action $(A_{t})$|action]], $A_{t}\in\mathcal{}$

One time step later, in part as a consequence of its action, the agent receives a numerical reward, $R_{t+1}\in\mathcal{R}\subset\mathbb{R}$, and finds itself in a new state, $S_{t+1}$

MDP and agent interact to each other to rise *tragectory* that begins like this:
$$S_{0}, A_{0}, R_{1}, S_{1}, A_{1}, R_{2}, S_{2}, A_{2}, \dots$$

# Finite MDP
The sets of states, actions and rewards all have a finite number of elements
## Dynamics
Random variable $R_{t}$ and $S_{t}$ have well defined discrete probability distribution *dependent only only on the preceding state and action*
- $p$ is defined as probability of those values occuring at time $t$ given particular values of the preceding state and action
$$p(s',r |s,a) := Pr\{s'=S_{t}, r=R_{t}\text{ | }s=S_{t-1}, a=A_{t-1}\}$$

$p$ defines the dynamic of the MDP
- $\forall \text{ }s\in\mathcal{S}, a\in\mathcal{A}(s)$, sum of all $p$ value is 1
- Each possible value for $S_{t}$ and $R_{t}$ *depends only on the immediately preceding state and action*, $S_{t-1}$ and $A_{t-1}$
	- State is restricted that it must include information about all aspects of the past agent-environment interaction that make a difference for the future
	- This is called have a **Markov Property**

**State-Transition Probability)**
$$ p(s'|s,a):= Pr\{ S_{t}=s' \text{ | }S_{t-1}=s, A_{t-1}=a\}=\sum_{r\in\mathcal{R}}p(s', r\text{ | }s,a)$$

**Expected rewards with respect to state-action pairs)**
$$r(s,a):=\mathbb{E}[R_{t}\text{ | }S_{t-1}=a, A_{t-1}=a]=\sum_{r\in\mathcal{R}}r\sum_{s'\in\mathcal{S}} p(s', r\text{ | }s,a)$$

**Expected rewards with respect to state-action-next state triple)**
$$r(s,a, s'):=\mathbb{E}[R_{t}\text{ | }S_{t-1}=a, A_{t-1}=a, S_{t}=s']=\sum_{r\in\mathcal{R}}r\frac{p(s', r\text{ | }s,a)}{p(s'\text{ | }s,a)}$$

## Agent-Environment Boundary
The agent-environment boundary represents the limit of the agent's **absolute control, not of its knowledge**
- Even if the agent knows everything, it still might have a difficulty to solve the problem

Anything that cannot be changed arbitrarily by the agent is consiered to be outside of it, thus part of environment

Reward computation is alway being external to the agent because it defines the task facing the agent, thus must be beyond its ability to change arbitrarily

# Goals and Rewards
[[1. Reinforcement Learning Fundamental#Reward Signal|Definition]]
The reward signal is your way of communicating to the robot what you want it to achieve, not how you want it achieved
- Subgoal, how the agent should work, must not be rewarded
- If achieving these sorts of subgoals were rewarded, then the agent might find a way to achieve them without achieving the real goal

**Ex)**
A chess-playing agent should be rewarded only for actually winning, not for taking its opponent’s pieces or gaining control of the center of the board.
- It might find a way to take the opponent’s pieces even at the cost of losing the game.

## Example: Bioreactor


## Example: Pick & Place Robot


# Returns and Episodes
We know that agent's goal is to maximize the long-run cumulative rewards
- How can we formally define this?
- From the sequence of rewards after time step $t$, $\{R_{t+1}, R_{t+2}, R_{t+3}\dots\}$, what precise aspect of this sequence do we wish to maximize?

 $\implies$ We seek to maximize the exprected return 

**Return** is some specific function of the reward sequence

## Episodic Tasks
One example of Return is the sum of the rewards$$G_{t}:=R_{t+1}+ R_{t+2}+R_{t+3}\dots+R_{T}$$
	- Where $R_{T}$ is a final time step
- If there is a natural notion of final time step, agent-environment interaction breaks naturally into subsequences, called **episodes**

**Q)**
Each episode ends in a special state called the *terminal state*, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states

Since next episode begins independently of how the previous one ended, all episode can all be considered to end in the same terminal state, with different rewards for the different outcomes

Tasks with episodes of this kind are **Episodic Tasks**

## Continuing Tasks
Some agent-environment interaction does not break naturally into episodes, but goes on continually without limit
- There is no terminal state, which means that what we are trying to maximize can easily be infinite

We need concept of *discounting* 
- Agent tries to select actions so that the sum of the *discounted rewards* it receives over the future is maximized
$$G_{t}:=R_{t+1}+ \gamma R_{t+2}+\gamma^{2}R_{t+3}\dots$$
$$=\sum^{\infty}_{k=0}\gamma^{k}R_{t+k+1} $$
- Where $\gamma$ is a parameter, $0\leq \gamma \leq 1$, called the discount rate

$$G_{t}:=R_{t+1}+ \gamma R_{t+2}+\gamma^{2}R_{t+3}\dots$$
$$=R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+\gamma^{2}R_{t+4}\dots)$$
$$=R_{t+1}+\gamma G_{t+1}$$

## Example: Pole-Balancing

# Policies and Value Function
**Value Function**
- Function of states / state-action pairs that estimate how good it is for the agent to be in a given state
- How good it is to perform a given action in a given state
- How good means future rewards that can be expected

Value functions are defined with respect to particular ways of acting, called policies

**Policy**
- Mapping from state to probabilities of selecting each possible action
- $\pi(a|s)$: policy $\pi$ at time $t$
	- $a=A_{t}$ and $s=S_{t}$

The **state-value function** 
- Expected return when starting in $s$ and following $\pi$ 
![[Pasted image 20241015010803.png]]
- $\mathbb{E}_{\pi}[\cdot]$ denotes the expected value of a random variable given that the agent follows policy $\pi$ at the some time step $t$
	- The value of the terminal state is always zero

The **action-value function** 
- Value of taking action $a$ in state $s$ under policy $\pi$
![[Pasted image 20241015011030.png]]

If an agent follows policy $\pi$ and maintains an average, for each state encountered, of the actual returns that have followed that state.
- Then the average will converge to the state’s value, $v_{\pi}(s)$, as the number of times that state is encountered approaches infinity. 
- If separate averages are kept for each action taken in each state, then these averages will similarly converge to the action values, $q_{\pi}(s,a)$
- Since this involve averaging over many random samples of actual returns, this kind of method is called, Monte Carlo Method

## Optimal Policies and Optimal Value Functions
We call one policy is better than another policy if the expected return for every state is greater
- Then there is always at least one policy that is btter than or equal to all other policy, this is an **optimal policy**
- Denoted as $\pi_{*}$ regardless number of it

Optimal state-value function
$$v_{*}(s):=\max_{\pi}v_{\pi}(s)$$
Optimal action-value function
$$q_{*}(s,a):=\max_{\pi}q_{\pi}(s,a)=\mathbb{E}[R_{t+1}+\gamma v_{*}(S_{t+1})\text{ | }S_{t}=s, A_{t}=a]$$
for all $s\in\mathcal{S}$ and $a\in\mathcal{A}(s)$

