Classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards
- Involve delayed reward and need to tradeoff immediate and delayed reward
- Estimate the value $q_{*}(s,a)$ of each action a in each state $s$, or we estimate the value $v_{*}(s)$ of each state given optimal action selections

# The Agent-Environment Interface

![[Pasted image 20240927105631.png|500]]

Agent Selecting actions and the environment responding to these actions and presenting new situations to the agent

At each time step $t$, the agent receives some representation of the environment's [[1. Reinforcement Learning Fundamental#State|state]], $S_{t}\in \mathcal{S}$, and on that basis selects an [[1. Reinforcement Learning Fundamental#Action $(A_{t})$|action]], $A_{t}\in\mathcal{}$

One time step later, in part as a consequence of its action, the agent receives a numerical reward, $R_{t+1}\in\mathcal{R}\subset\mathbb{R}$, and finds itself in a new state, $S_{t+1}$

MDP and agent interact to each other to rise *tragectory* that begins like this:
$$S_{0}, A_{0}, R_{1}, S_{1}, A_{1}, R_{2}, S_{2}, A_{2}, \dots$$

# Finite MDP
The sets of states, actions and erwards all have a finite nunmber of elements
- $R_{t}$ and $S_{t}$ have well defined discrete probability distribution *dependent only only on the preceding state and action*
$$p(s',r |s,a) := Pr\{s'=S_{t}, r=R_{t}\text{ | }s=S_{t-1}, a=A_{t-1}\}$$

- $\forall \text{ }s\in\mathcal{S}, a\in\mathcal{A}(s)$, sum of all $p$ value is 1
- Each possible value for $S_{t}$ and $R_{t}$ *depends only on the immediately preceding state and action*, $S_{t-1}$ and $A_{t-1}$
	- State is restricted that it must include information about all aspects of the past agent-environment interaction that make a difference for the future
	- This is called have a **Markov Property**

## Agent-Environment Boundary
The agent-environment boundary represents the limit of the agent's **absolute control, not of its knowledge**
- Even if the agent knows everything, it still might have a difficulty to solve the problem

Anything that cannot be changed arbitrarily by the agent is consiered to be outside of it, thus part of environment

Reward computation is alway being external to the agent because it defines the task facing the agent, thus must be beyond its ability to change arbitrarily

# Goals and Rewards
[[1. Reinforcement Learning Fundamental#Reward Signal|Definition]]
The reward signal is your way of communicating to the robot what you want it to achieve, not how you want it achieved
- Subgoal, how the agent should work, must not be rewarded
- If achieving these sorts of subgoals were rewarded, then the agent might find a way to achieve them without achieving the real goal

**Ex)**
A chess-playing agent should be rewarded only for actually winning, not for taking its opponent’s pieces or gaining control of the center of the board.
- It might find a way to take the opponent’s pieces even at the cost of losing the game.

## Example: Bioreactor


## Example: Pick & Place Robot


# Returns and Episodes
We know that agent's goal is to maximize the long-run cumulative rewards
- How can we formally define this?
- From the sequence of rewards after time step $t$, $\{R_{t+1}, R_{t+2}, R_{t+3}\dots\}$, what precise aspect of this sequence do we wish to maximize?

 $\implies$ We seek to maximize the exprected return 

**Return** is some specific function of the reward sequence

## Episodic Tasks
One example of Return is the sum of the rewards$$G_{t}:=R_{t+1}+ R_{t+2}+R_{t+3}\dots+R_{T}$$
	- Where $R_{T}$ is a final time step
- If there is a natural notion of final time step, agent-environment interaction breaks naturally into subsequences, called **episodes**

**Q)**
Each episode ends in a special state called the *terminal state*, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states

Since next episode begins independently of how the previous one ended, all episode can all be considered to end in the same terminal state, with different rewards for the different outcomes

Tasks with episodes of this kind are **Episodic Tasks**

## Continuing Tasks
Some agent-environment interaction does not break naturally into episodes, but goes on continually without limit
- There is no terminal state, which means that what we are trying to maximize can easily be infinite

We need concept of *discounting* 
- Agent tries to select actions so that the sum of the *discounted rewards* it receives over the future is maximized
$$G_{t}:=R_{t+1}+ \gamma R_{t+2}+\gamma^{2}R_{t+3}\dots$$
$$=\sum^{\infty}_{k=0}\gamma^{k}R_{t+k+1} $$
- Where $\gamma$ is a parameter, $0\leq \gamma \leq 1$, called the discount rate

$$G_{t}:=R_{t+1}+ \gamma R_{t+2}+\gamma^{2}R_{t+3}\dots$$
$$=R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+\gamma^{2}R_{t+4}\dots)$$
$$=R_{t+1}+\gamma G_{t+1}$$

## Example: Pole-Balancing

