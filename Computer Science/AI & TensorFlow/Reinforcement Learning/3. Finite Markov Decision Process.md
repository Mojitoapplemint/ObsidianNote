Classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards
- Involve delayed reward and need to tradeoff immediate and delayed reward
- Estimate the value $q_{*}(s,a)$ of each action a in each state $s$, or we estimate the value $v_{*}(s)$ of each state given optimal action selections

# The Agent-Environment Interface

![[Pasted image 20240927105631.png|500]]

Agent Selecting actions and the environment responding to these actions and presenting new situations to the agent

At each time step $t$, the agent receives some representation of the environment's [[1. Reinforcement Learning Fundamental#State|state]], $S_{t}\in \mathcal{S}$, and on that basis selects an [[1. Reinforcement Learning Fundamental#Action $(A_{t})$|action]], $A_{t}\in\mathcal{}$

One time step later, in part as a consequence of its action, the agent receives a numerical reward, $R_{t+1}\in\mathcal{R}\subset\mathbb{R}$, and finds itself in a new state, $S_{t+1}$

MDP and agent interact to each other to rise *tragectory* that begins like this:
$$S_{0}, A_{0}, R_{1}, S_{1}, A_{1}, R_{2}, S_{2}, A_{2}, \dots$$

# Finite MDP
The sets of states, actions and erwards all have a finite nunmber of elements
- $R_{t}$ and $S_{t}$ have well defined discrete probability distribution *dependent only only on the preceding state and action*
$$p(s',r |s,a) := Pr\{s'=S_{t}, r=R_{t}\text{ | }s=S_{t-1}, a=A_{t-1}\}$$

