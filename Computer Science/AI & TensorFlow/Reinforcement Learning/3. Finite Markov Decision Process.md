Classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards
- Involve delayed reward and need to tradeoff immediate and delayed reward
- Estimate the value $q_{*}(s,a)$ of each action a in each state $s$, or we estimate the value $v_{*}(s)$ of each state given optimal action selections

# The Agent-Environment Interface

![[Pasted image 20240927105631.png|500]]

Agent Selecting actions and the environment responding to these actions and presenting new situations to the agent

At each time step $t$, the agent receives some representation of the environment's [[1. Reinforcement Learning Fundamental#State|state]], $S_{t}\in \mathcal{S}$, and on that basis selects an [[1. Reinforcement Learning Fundamental#Action $(A_{t})$|action]], $A_{t}\in\mathcal{}$
