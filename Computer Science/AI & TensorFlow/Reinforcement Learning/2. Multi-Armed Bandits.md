Reinforcement learning is distinguished from other learning since it uses training information that *evaluates* the actions taken rather than *instructs* by giving correct actions

Evaluative feedback
- Indicates how good the action taken was, but not whether it was the best or the worst action possible
- Depends entirely on the action taken

Instructive feedback
- Indicates the correct action to take, independently of the action actually taken
- Independent of the action taken

# $k$-armed Bandit Problem

One-armed Bandit
1. Facing repeatedly with a choice among $k$ different actions
2. After action, receive a numerical reward from probability distribution that depends on the choice
3. Ojective is to maximize the total rewards

Instead of one level, we do this $k$ times
- Each action has an expected or mean reward, called *value*
## Notation
$A_{t}$
- Action selected on time step $t$
 
$R_{t}$
- Corresponding reward

$q_{*}(a)$
- Value of an arbitrary action $a$
- Expected reward if $a$ is selected

## Exploitation and Exploration
Greedy Action
- Action that has the highest expected value

When you select those greedy actions, we say you are **Exploiting**
the current knowledge of the values of the action
- To maximize the expected reward

When you select one of the nongreedy actions, then you are **Exporing**
- Enables to improve estimate of the nongreedy action's value
- May produce the greater total reward in the long run

