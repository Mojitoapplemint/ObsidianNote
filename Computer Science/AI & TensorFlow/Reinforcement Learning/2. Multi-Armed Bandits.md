Reinforcement learning is distinguished from other learning since it uses training information that *evaluates* the actions taken rather than *instructs* by giving correct actions

Evaluative feedback
- Indicates how good the action taken was, but not whether it was the best or the worst action possible
- Depends entirely on the action taken

Instructive feedback
- Indicates the correct action to take, independently of the action actually taken
- Independent of the action taken

# $k$-armed Bandit Problem

One-armed Bandit
1. Facing repeatedly with a choice among $k$ different actions
2. After action, receive a numerical reward from probability distribution that depends on the choice
3. Ojective is to maximize the total rewards

Instead of one level, we do this $k$ times
- Each action has an expected or mean reward, called *value*
- True value of an action is the mean reward when that action si selected

## Notation
$A_{t}$
- Action selected on time step $t$
 
$R_{t}$
- Corresponding reward

$q_{*}(a)$
- Value of an arbitrary action $a$
- Expected reward if $a$ is selected

## Exploitation and Exploration
Greedy Action
- Action that has the highest expected value

When you select those greedy actions, we say you are **Exploiting**
the current knowledge of the values of the action
- To maximize the expected reward

When you select one of the nongreedy actions, then you are **Exporing**
- Enables to improve estimate of the nongreedy action's value
- May produce the greater total reward in the long run

# Action-value Method
Methods for estimating the values of actions and for using the estimates to make action selection decisions

## Sample-average Method
Take the estimate action values as sample average of observed rewards
$$Q_{t}(a)\approx\frac{\text{Sum of rewards when "a" taken prior to "t"}}{\text{number of times "a" taken prior to "t"}}$$
$$=\frac{\sum^{t-1}_{i=1}R_{i}\cdot \mathbb{1}_{A_{i}=a}}{\sum^{t-1}_{i=1}\mathbb{1}_{A_{i}=a} }$$
$\mathbb{1}_{\text{predicate}}$ denotes the random variable that is 
- 1 if predicate is true
- 0 if it is not

If the denominator is zero, then define $Q_{t}(a)$ as default value

As the denominator does to infinity, $Q_{t}(a)$ converges to $q_{*}(a)$

## Greedy Action Selection
Select actions with the highest estimated value
$$A_{t}\approx \text{argmax}_{a}Q_{t}(a)$$
- $\text{argmax}_{a}$ denotes the action $a$ for which the expression that follows is maximized
- Always exploits current knowledge to maximize immediate reward
- Spends no time at all sampling apparently inferior actions to see if they might really be better

### $\epsilon$-greedy selection
Behave greedily most of the time, but select randomly with small probability $\epsilon$, independently of the action-value estimates
- Every action will be sample an infinite number of times, as epoch increases, thus ensuring $Q_{t}(a)$ converge to $q_{*}(a)$

### Which is Better?
 $\epsilon$-greedy
- Problem with noisier rewards (Larger rewards) variance that 
	- Requires more exploration to find the optimal action
- Nonstationary bandit task
	- True value of the action changes over time

Reward variances were zero
- The greedy method would know the true value of each action after trying it once
- It would soon find the optimal action and then never explore.

**Reinforcement learning requires a balance between exploration and exploitation**

# Incremental Implementation
Let 
- $R_{i}=$ Reward received after the $i$th selection of this action
- $Q_{n}$ Estimate of its action value after it has been selected $n-1$ times
Then,
$$Q_{n}=\frac{R_{1}+R_{2}+\dots R_{n-1}}{n-1}$$

If we store the record of all the rewards to perform this computation will require a lot of memory and computational requirement, thus we can caculate by
$$Q_{n+1}=\frac{1}{n}\sum^{n}_{i=1}R_{i} =\frac{1}{n}\left( R_{n}+\sum^{n-1}_{i=1}R_{i}  \right)=\frac{1}{n}\left( R_{n}+(n-1)Q_{n-1}  \right)$$
$$=Q_{n}+\frac{1}{n}\left[R_{n}-Q_{n}\right]$$
**General Form)**
$\text{New Estimate}\leftarrow\text{Old Estimate}+\text{Step Size}[\text{ Target - Old Estimate }]$
- $[\text{ Target - Old Estimate }]$: Error in the estimate
- $\text{Target}$: Desirable direction in which to move
- $\text{Step Size}$: Denoted by $\alpha$, or $\alpha_{t}(a)$

## For Nonstationary Problem
For nonstationary problem, we give more weight to recent rewards than to long-past rewards

### Constant Step-size Parameter
$$Q_{n+1}\approx Q_{n}+\alpha\left[R_{n}-Q_{n}\right]$$
$$= (1-\alpha)^{n}Q_{1}+\sum^{n}_{i=1}\alpha(1-\alpha)^{n-i}R_{i} $$
This is called, weighted average

# Optimistic Initial Values
Methods that are dependent on the initial action-value estimates, $Q_1(a)$, are biased by their initial estimates
- In practice, this kind of biased is uaually not a problem, but sometimes are helpful

Let the initial estimate wildly optimistic(higher than how it supposed to be)
- Then, the learner would be disappointed that the actual value is much lower than the estimation
- The result is that all actions are tried several times before the value estimates converge
- The system does a fair amount of exploration even if greedy actions are selected all the time
- It might take more time at first, 



