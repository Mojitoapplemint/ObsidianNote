Reinforcement learning is distinguished from other learning since it uses training information that *evaluates* the actions taken rather than *instructs* by giving correct actions

Evaluative feedback
- Indicates how good the action taken was, but not whether it was the best or the worst action possible
- Depends entirely on the action taken

Instructive feedback
- Indicates the correct action to take, independently of the action actually taken
- Independent of the action taken

# $k$-armed Bandit Problem

One-armed Bandit
1. Facing repeatedly with a choice among $k$ different actions
2. After action, receive a numerical reward from probability distribution that depends on the choice
3. Ojective is to maximize the total rewards

Instead of one level, we do this $k$ times
- Each action has an expected or mean reward, called *value*
- True value of an action is the mean reward when that action si selected

## Notation
$A_{t}$
- Action selected on time step $t$
 
$R_{t}$
- Corresponding reward

$q_{*}(a)$
- Value of an arbitrary action $a$
- Expected reward if $a$ is selected

## Exploitation and Exploration
Greedy Action
- Action that has the highest expected value

When you select those greedy actions, we say you are **Exploiting**
the current knowledge of the values of the action
- To maximize the expected reward

When you select one of the nongreedy actions, then you are **Exporing**
- Enables to improve estimate of the nongreedy action's value
- May produce the greater total reward in the long run

# Action-value Method
Methods for estimating the values of actions and for using the estimates to make action selection decisions

## Averaging the Rewards actually received
$$Q_{t}(a)\approx\frac{\text{Sum of rewards when "a" taken prior to "t"}}{\text{number of times "a" taken prior to "t"}}$$
$$=\frac{\sum^{t-1}_{i=1}R_{i}\cdot \mathbb{1}_{A_{i}=a}}{\sum^{t-1}_{i=1}\mathbb{1}_{A_{i}=a} }$$
$\mathbb{1}_{\text{predicate0}}$