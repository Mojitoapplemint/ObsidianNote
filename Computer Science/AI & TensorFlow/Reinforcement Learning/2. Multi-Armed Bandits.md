Reinforcement learning is distinguished from other learning since it uses training information that *evaluates* the actions taken rather than *instructs* by giving correct actions

Evaluative feedback
- Indicates how good the action taken was, but not whether it was the best or the worst action possible
- Depends entirely on the action taken

Instructive feedback
- Indicates the correct action to take, independently of the action actually taken
- Independent of the action taken

# $k$-armed Bandit Problem

One-armed Bandit
1. Facing repeatedly with a choice among $k$ different actions
2. After action, receive a numerical reward from probability distribution that depends on the choice
3. Ojective is to maximize the total rewards

Instead of one level, we do this $k$ times
- Each action has an expected or mean reward, called *value*
- True value of an action is the mean reward when that action si selected

## Notation
$A_{t}$
- Action selected on time step $t$
 
$R_{t}$
- Corresponding reward

$q_{*}(a)$
- Value of an arbitrary action $a$
- Expected reward if $a$ is selected

## Exploitation and Exploration
Greedy Action
- Action that has the highest expected value

When you select those greedy actions, we say you are **Exploiting**
the current knowledge of the values of the action
- To maximize the expected reward

When you select one of the nongreedy actions, then you are **Exporing**
- Enables to improve estimate of the nongreedy action's value
- May produce the greater total reward in the long run

# Action-value Method
Methods for estimating the values of actions and for using the estimates to make action selection decisions

## Sample-average Method
Take the estimate action values as sample average of observed rewards
$$Q_{t}(a):=\frac{\text{Sum of rewards when "a" taken prior to "t"}}{\text{number of times "a" taken prior to "t"}}$$
$$=\frac{\sum^{t-1}_{i=1}R_{i}\cdot \mathbb{1}_{A_{i}=a}}{\sum^{t-1}_{i=1}\mathbb{1}_{A_{i}=a} }$$
$\mathbb{1}_{\text{predicate}}$ denotes the random variable that is 
- 1 if predicate is true
- 0 if it is not

If the denominator is zero, then define $Q_{t}(a)$ as default value

As the denominator does to infinity, $Q_{t}(a)$ converges to $q_{*}(a)$

## Greedy Action Selection
Select actions with the highest estimated value
$$A_{t}\approx \text{argmax}_{a}Q_{t}(a)$$
- $\text{argmax}_{a}$ denotes the action $a$ for which the expression that follows is maximized
- Always exploits current knowledge to maximize immediate reward
- Spends no time at all sampling apparently inferior actions to see if they might really be better

### $\epsilon$-greedy selection
Behave greedily most of the time, but select randomly with small probability $\epsilon$, independently of the action-value estimates
- Every action will be sample an infinite number of times, as epoch increases, thus ensuring $Q_{t}(a)$ converge to $q_{*}(a)$

### Which is Better?
 $\epsilon$-greedy
- Problem with noisier rewards (Larger rewards) variance that 
	- Requires more exploration to find the optimal action
- Nonstationary bandit task
	- True value of the action changes over time

Reward variances were zero
- The greedy method would know the true value of each action after trying it once
- It would soon find the optimal action and then never explore.

**Reinforcement learning requires a balance between exploration and exploitation**

# Incremental Implementation
Let 
- $R_{i}=$ Reward received after the $i$th selection of this action
- $Q_{n}$ Estimate of its action value after it has been selected $n-1$ times
Then,
$$Q_{n}=\frac{R_{1}+R_{2}+\dots R_{n-1}}{n-1}$$

If we store the record of all the rewards to perform this computation will require a lot of memory and computational requirement, thus we can caculate by
$$Q_{n+1}=\frac{1}{n}\sum^{n}_{i=1}R_{i} =\frac{1}{n}\left( R_{n}+\sum^{n-1}_{i=1}R_{i}  \right)=\frac{1}{n}\left( R_{n}+(n-1)Q_{n-1}  \right)$$
$$=Q_{n}+\frac{1}{n}\left[R_{n}-Q_{n}\right]$$
**General Form)**
$\text{New Estimate}\leftarrow\text{Old Estimate}+\text{Step Size}[\text{ Target - Old Estimate }]$
- $[\text{ Target - Old Estimate }]$: Error in the estimate
- $\text{Target}$: Desirable direction in which to move
- $\text{Step Size}$: Denoted by $\alpha$, or $\alpha_{t}(a)$

## For Nonstationary Problem
For nonstationary problem, we give more weight to recent rewards than to long-past rewards

### Constant Step-size Parameter
$$Q_{n+1}:= Q_{n}+\alpha\left[R_{n}-Q_{n}\right]$$
$$= (1-\alpha)^{n}Q_{1}+\sum^{n}_{i=1}\alpha(1-\alpha)^{n-i}R_{i} $$
This is called, weighted average

# Optimistic Initial Values
Methods that are dependent on the initial action-value estimates, $Q_1(a)$, are biased by their initial estimates
- In practice, this kind of biased is uaually not a problem, but sometimes are helpful
- Effective on stationary problems

Let the initial estimate wildly optimistic(higher than how it supposed to be)
- Then, the learner would be disappointed that the actual value is much lower than the estimation
- The result is that all actions are tried several times before the value estimates converge
- The system does a fair amount of exploration even if greedy actions are selected all the time
- It might take more time at first, but eventually it performs better because its exploration decreases with time


# Upper-Confidence-Bound Action Selection (UCB)
How should we explore?
- It would be better to select among the non-greedy actions according to their potential
	- How close their estimates are to being maximal 
	- The uncertainties in those estimates


$$A_{t}:= \text{argmax}_{a}\left[ Q_{t}(a)+c\sqrt{ \frac{\ln t}{N_{t}(a)} } \right]$$
- Square-root term is the *measure of the uncertainty* or variance in the estimate of $a$'s value

$N_{t}(a)$
- The number of times that action a has been selected prior to time $t$

$a$ is selected
- Denominator, $N_{t}(a)$, increments
- Uncertainty is reduced

Action other than $a$ is selected
- $t$ increases, $N_{t}(a)$ does not
- Uncertainty increases

# Action Preference
$H_{t}(a)$
- Numerical Preference for each action $a$
- Larger the preferenc,e the more often that action is taken
- No interpretation in terms of reward

Relative preference of one action over another is importnat
- Determined according to a *soft-max* 
$$\text{Pr}\{A_{t}=a\}\approx\frac{e^{H_{t}(a)}}{\sum^{k}_{b=1}e^{H_{t}(b)}}\approx \pi_{t}(a)$$
$\pi_{t}(a)$
- Probability of taking action $a$ at time $t$

## Gradient Bandit Algorithm
On each step, after selecting action At and receiving the reward Rt, the action preferences are updated by
$$H_{t+1}(A_{t}):= H_{t}(A_{t})+\alpha(R_{t}-\bar{R_{t}})(1-\pi_{t}(A_{t}))$$
$$H_{t+1}(a):= H_{t}(a)-\alpha(R_{t}-\bar{R_{t}})\pi_{t}(a)$$
for all $a\neq A_{t}$

$\bar{R_{t}}\in\mathbb{R}$
- The average of all the rewards up through and including time $t$
- Base with which the reward is compared
	- If reward is higher than the baseline, the probability of taking $A_{t}$ is increased

# Associative Search (Contextual Bandits)
Involves both trial-and-error learning to search for the best actions, and association of those action with the situation in which they are best