# Recurrent Neural Network

| Hyperparameter           | Functionality                                                                             | Typical values                                                                         |
| ------------------------ | ----------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- |
| Input layers             | Takes in target sequence                                                                  | `input_shape`=`[batch_size, embedding_size]` or `[batch_size, sequence_shape]`         |
| Text Vectorization Layer | Maps input sequence to numbers                                                            | `tf.keras.layers.TextVectorization`                                                    |
| Embedding                | Turns mapping of text vectors to embedding matrix(**represeneation of how words relate**) | `tf.keras.layers.Embedding`                                                            |
| RNN cells                | FFind patterns in sequences                                                               | `SimpleRNN`, `LSTM`, `GRU`                                                             |
| Hidden activation        | Adds non-linearity to learned features                                                    | Usually [[Activation Functions#Tanh\|Tanh]]                                            |
| Pooling later            | Reduces the dimensionality of learned sequence features                                   | Global Average Pool, or Max Pool (1D)                                                  |
| Fully connected layer    | Further refines learned features from recurrent layer                                     | Dense layer                                                                            |
| Output layer             | Takes learned featuyres and oiutputs them in shape of target labels                       | `[number_of_classes]`                                                                  |
| Output activation        | Adds non-linearities to output layer                                                      | [[Activation Functions#Sigmoid\|Sigmoid]] or [[Activation Functions#Softmax\|Softmax]] |

# Convert Text into Numerical Form

## Tokenization
Direct mapping from word/character/sub-word to numbers
- Can be modelled but quickly gets too big
- `Sub-Word`: MIxture of word-level and character-level; it breaks individual workds into smaller parts and then convert them into numbers

[`tf.keras.layers.TextVectorization`]([tf.keras.layers.TextVectorization  |  TensorFlow v2.16.1](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization))
- `max_tokens`: Multiples of 10,000 or exact number of unique words in text are common values

## Embedding
[Word Embedding Documentation]([Word embeddings  |  Text  |  TensorFlow](https://www.tensorflow.org/text/guide/word_embeddings))

Representation of natural language which can be learned
- Returns `feature vector`
- [`tf.keras.layers.Embedding`]([tf.keras.layers.Embedding  |  TensorFlow v2.16.1](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding))

### Parameter
`input_dim`: The size of vocabulary
`output_dim`: The size of output embedding vector per word
`embedding_initializer`: How to initialize the embedding matrix
- This will eventually change during training
`input_length`: Length of sequences being passed to embedding layer

### Getting Weights
`mode.get_layer("layer_name").get_weights()`

### Visualize Embedding with TF Projector
1. Create `.tsv` files for metadata and vector
```python
import io

# Create output writers
out_v = io.open("embedding_vectors.tsv", "w", encoding="utf-8")
out_m = io.open("embedding_metadata.tsv", "w", encoding="utf-8")

# Write embedding vectors and words to file
for num, word in enumerate(words_in_vocab):
	if num == 0: 
		continue # skip padding token
	vec = embed_weights[num]
	out_m.write(word + "\n") # write words to file
	out_v.write("\t".join([str(x) for x in vec]) + "\n") # write corresponding word vector to file
out_v.close()
out_m.close()
```
2. Download them and upload them in [TF Projector]([Embedding projector - visualization of high-dimensional data (tensorflow.org)](https://projector.tensorflow.org/))

**Note**
- Since the projector is compressing the multi-dimensional result into 3D, words that are close to each other do not necessarily have the similar meaning

# Recurrent Neural Networks (RNN's)


The premise of RNN is to use the representation of a previous input to aid the representation of a later input
- Suit for sequence data
- 