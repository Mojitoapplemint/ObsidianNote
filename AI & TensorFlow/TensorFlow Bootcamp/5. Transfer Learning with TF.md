# What is Transfer Learning?
Technique that brings existing model's architecture and apply it to our own problem

## Why use Transfer Learning
- Can use proved architecture
- Can use working architecture which has already leanred patterns

## Types
`As is`
- Using an existing model with *no changes* what so ever
`Feature Extraction`
- Use the prelearned patterns of an existing model and *adjust the output layer* for own problem
`Fine-tuning`
- Use the prelearned patterns of an existing model and *"fine-tune" many or all of the underlying layers*, including output layers
- Usually requires more data than feature extraction

# Keras Functional API
Functional API is another way to construct the model than Sequential API
- Compiling, fitting, evaluating process do not change
- More flexible and able to produce more sophisticated models

**Steps**
1. Create base model with `tf.keras.applications`
	- With `include_top=False`
2. Freeze the base model
3. Create inputs into model
4. Normalize inputs (If necessary)
5. Pass the input
6. Average pool the outputs of the base model
7. Create the output activation layers
8. Combine the inputs wityh the outputs into a model

Then compile and fit

```python
# 1. Create base model with tf.keras.applications 
base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False) 

# 2. Freeze the base model (so the pre-learned patterns remain) 
base_model.trainable = False 

# 3. Create inputs into the base model 
inputs = tf.keras.layers.Input(shape=(224, 224, 3)) 

# 4. Normalize(if necessary)
x = tf.keras.layers.preprocessing.Rescaling(1./255)(inputs) 

# 5. Pass the inputs to the base_model 
x = base_model(inputs) 

# 6. Average pool the outputs of the base model 
x = tf.keras.layers.GlobalAveragePooling2D()(x) 

# 7. Create the output activation layer 
outputs = tf.keras.layers.Dense(10, activation="softmax")(x) 

# 8. Combine the inputs with the outputs into a model 
model_0 = tf.keras.Model(inputs, outputs)
```

# Fine-Tuning
We can unfreeze some layers so that it becomes trainable
- Since the early layers tends to learn larger scale of features and they often performs well, the *Rule of Thumb is to unfreeze upper(later) layers of model*
- Usually works best *after training a feature extraction model* for a few epochs with large amounts of custom data
- Still, those layers already trained somewhat, we don't want to change them drastically. Thus, we *lower the optimizer learning rate*

we can continue training on from where the previous model finished by passing the latest epoch to `initial_epoch` parameter of `fit()` method
**Example**
```python
fine_tune_epochs = initial_epochs+5

model.fit(...
		 epochs = fine_tune_epochs,
		 ...
		 initial_epoch = previous_model_history.epoch[-1],
		 ...)
```

If we are fine-tuning in different way but with same feature extraction model, we need to go back to feature extraction model, not continue from the previous fine-tune model
- Thus, we need to set [[AI & TensorFlow/TensorFlow Bootcamp/Extras#Callbacks#Model Checkpoint|checkpoint]] of the feature extraction model

# Serial Experimentation
![[Pasted image 20240613091424.png]]

# Finding the Most Worng Predictions
Viewing the worng predictions with the highest prediction probability can reveal
- Data issues (e.g.wrong labels)
- Confusing classes

**Steps**
1. Get all image file paths in the test dataset using `list_files()`
2. Create Pandas DataFrame (`ground_truth`, `predicted_classes`, `max_prediction_probs`)
3. Find all the wrong predictions
4. Sort the DataFrame based on the wrong predictions
5. Visualize the images with the highest prediction probs have the wrong prediction



---
# Print Information of Base Model
we can peek pre-trained model by `enumerate()`
**Example**
```python
for i, layer in enumerate(model):
	print(i, layer.name, layer.trainable)
```

