# What is Transfer Learning?
Technique that brings existing model's architecture and apply it to our own problem

## Why use Transfer Learning
- Can use proved architecture
- Can use working architecture which has already leanred patterns

## Types
`As is`
- Using an existing model with *no changes* what so ever
`Feature Extraction`
- Use the prelearned patterns of an existing model and *adjust the output layer* for own problem
`Fine-tuning`
- Use the prelearned patterns of an existing model and *"fine-tune" many or all of the underlying layers*, including output layers
- Usually requires more data than feature extraction

# Keras Functional API
Functional API is another way to construct the model than Sequential API
- Compiling, fitting, evaluating process do not change
- More flexible and able to produce more sophisticated models

**Steps**
1. Create base model with `tf.keras.applications`
	- With `include_top=False`
2. Freeze the base model
3. Create inputs into model
4. Normalize inputs (If necessary)
5. Pass the input
6. Average pool the outputs of the base model
7. Create the output activation layers
8. Combine the inputs wityh the outputs into a model

Then compile and fit

```python
# 1. Create base model with tf.keras.applications 
base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False) 

# 2. Freeze the base model (so the pre-learned patterns remain) 
base_model.trainable = False 

# 3. Create inputs into the base model 
inputs = tf.keras.layers.Input(shape=(224, 224, 3), name="input_layer") 

# 4. Normalize
x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs) 

# 5. Pass the inputs to the base_model 
x = base_model(inputs) 

# 6. Average pool the outputs of the base model 
x = tf.keras.layers.GlobalAveragePooling2D(name="global_average_pooling_layer")(x) 

# 7. Create the output activation layer 
outputs = tf.keras.layers.Dense(10, activation="softmax", name="output_layer")(x) 

# 8. Combine the inputs with the outputs into a model 
model_0 = tf.keras.Model(inputs, outputs)
```

# Fine-Tuning
We can unfreeze some layers so that it becomes trainable
- Since the early layers tends to learn larger scale of features and they often performs well, the *Rule of Thumb is to unfreeze later layers of model*
- Usually works best *after training a feature extraction model* for a few epochs with large amounts of custom data

---
# Print Information of Base Model
Pre-trained model

```python
for i, layer in enumerate(model):
	print(i)
```