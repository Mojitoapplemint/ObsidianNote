# Types of Classification
1. Binary Classification
	- One or the other
2. Multi-class Classification
	- One of the class
3. Mull-label Classification
	- One class can have various labels

# Features and Labels
*"Can be varied depends on the problem"*

**Features**
- Photo `=>` Combination of width, height and colour channels(RGB)
	- Shape = `[batch_size, width, height, colour_channels]`
	- `None` for arbitrary batch_size, and `32` for common use

**Label**
- Shape = a vector that has the elements of `Number of classes`

# Hyperparameter
| Hyperparameter           | Binary Classification                          | Multiclass/MultiLabel Classification |
| ------------------------ | ---------------------------------------------- | ------------------------------------ |
| Input layer shape        | Same shape as number of features               | =                                    |
| Hidden layer(s)          | Problem specific, minimum=1, maximum=unlimited | =                                    |
| Neurons per hidden layer | Problem specific; generally 10 to 100          | =                                    |
| Output layer shape       | 1 (One class or the other)                     | one  per class                       |
| Hidden Activation        | Usually `ReLU` (Rectified linear unit)         | =                                    |
| Output Activation        | `Sigmoid`                                      | `Softmax`                            |
| Loss Function            | Binary Cross Entropy                           | Categorical Cross Entropy            |
| Optimizer                | SGD, Adam                                      | =                                    |

# Visualize Prediction
Create function `plot_decision_boundary()`, this will
- Take in a trained model, features and labels
- Create a meshgrid of the different X values
- Make predictions accross the meshgrid
- Plot the predictions as well as a line between zones(where each unique class falls)

[Additional Source: CS231n](https://cs231n.github.io/neural-networks-case-study/)
[Additional Source: ML Basics](https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb)
## Example
```python
def plot_decision_boundary(model, X, y): 
	# Define the axis boundaries of the plot and create a meshgrid 
	x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1 
	y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1 
	xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100)) 
	
	# Create X values (we're going to predict on all of these) 
	x_in = np.c_[xx.ravel(), yy.ravel()] 
	
	# stack 2D arrays together: https://numpy.org/devdocs/reference/generated/numpy.c_.html 
	
	# Make predictions using the trained model 
	y_pred = model.predict(x_in) 
	
	# Check for multi-class: checks the final dimension of the model's output shape, if this is > (greater than) 1, it's multi-class 
	if model.output_shape[-1] > 1:
		print("doing multiclass classification...") 
		y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape) 
	else: 
		print("doing binary classifcation...") 
		y_pred = np.round(np.max(y_pred, axis=1)).reshape(xx.shape) 
		
	# Plot decision boundary 
	plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7) 
	plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu) 
	plt.xlim(xx.min(), xx.max()) 
	plt.ylim(yy.min(), yy.max())
	plt.show()
```

# Giving a Non-Linearity to Model
For non-linear data, linear regression model won't work
- If activation function is not specified, it will use *linear activation* `a(x)=x` as default

By choosing **non-linear activation fucntion**, we can give Non-Linearity to the model
- Output layer requires correct activation function, depends on the type of the classification: [[Activation Functions#Sigmoid|sigmoid]] or `softmax`

**Notes:** The *Combination of linear and non-linear* activation functions is one of the key fundamentals of Neural Networks

# Classification Evaluation Method
[[Data Science/MATH1311/True, False, Positive, Negative|Additional Terminology: True, False, Positive, Negative]]
## Accuracy
$$\text{Accuracy}=\frac{tp+tn}{tp+tn+fp+fn}$$
- Default metric for classification problem
- Not the best for imbalanced classe

## Precision
$$\text{Precision}=\frac{tp}{tp+fp}$$
- Higher precision leads to less false positives

## Recall
$$\text{Recall}=\frac{tp}{tp+fn}$$
- Higher recall leads to less false negatives

## Precision & Recall Tradeoff
Precision and Recall can't be both high
- If one increase, the other will decrease

## F1-Score
$$\text{F1}=2\cdot\frac{\text{precision}\cdot\text{recall}}{\text{precision+recall}}$$
- Combination of precisioin and recall
- Usually good overall metric for a classification model

## Confusion Matrix
Tool to evaluate the performance of a nmodel and visualize as a table
- `sklearn.metrics.confusion_matrix`
- When comparing predictions to truth labels to see where model get confused 
- Can be hard to use with large numbers of classes
![[Pasted image 20240525110501.png|300]]

**Note:** [[Activation Functions#Sigmoid|sigmoid]] or softmax activation function returns prediction probability array, thus we need to convert them to binary
- `tf.round(y_preds)`