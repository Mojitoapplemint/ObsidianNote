[Additional Video: MIT CNN]([MIT 6.S191 (2023): Convolutional Neural Networks (youtube.com)](https://www.youtube.com/watch?v=NmLK_WQBxB4&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=8))

# Convolutional Neural Network(CNN)
## Architecture
![[Pasted image 20240527124337.png]]

| Hyperparameter        | What does it do?                                                  | Typical values                                                                             |
| --------------------- | ----------------------------------------------------------------- | ------------------------------------------------------------------------------------------ |
| Input Images          | Target images you'd like to discover patterns in                  | Whatever you can take a photo of                                                           |
| Input Layers          | Takes in target images and preprocesses them for further layers   | `input_shape = (batch_size, height, width, colour_channels)`                               |
| Convolutional Layers  | Extracts/learns the most importnat features from target images    | Multiple, can create with `tf.keras.layers.ConvXD (X can be multiple values)`              |
| Hidden Activation     | Adds non-linearity to learned features                            | Usually [[Activation Functions#ReLU (Rectified Linear Units)\|ReLU]]                       |
| Pooling Layer         | Reduces the dimensionality of learned image features              | Average (`tf.keras.layers.AvgPool2D`)or Max(`tf.keras.layers.MaxPool2D`)                   |
| Fully Connected Layer | Further refines learned features from convolutional layers        | `tf.keras.layers.Dense`                                                                    |
| Output Layer          | Takes learned features and outputs them in shape of target labels | `output_shape=[# images]`                                                                  |
| Output Activation     | Adds non-linearities to output layer                              | [[Activation Functions#Sigmoid\|Sigmoid]] or [[Activation Functions#Softmax\|Softmax]]<br> |

## Compare to Normal NN
CNN requires less **trainable parameters** than NN, but still performs better
- Trainable Parameters can be interpreted as *patterns a model can learn from data*
- `NN`: Series of Dense layer with learnable parameters connected to each other -> higher number of possible learnable patterns
- `CNN`: Seeks to sort out and *learn the most important patterns* in an image

# Computer Vision
Practice of writing algorithms which can discover patterns in visual data
- Such as camera of a self-diriving car recognizing the car in front

# Importing & Preprocessing Images
```python
from keras.preprocessing.image import ImageDataGenerator
import zipfile

# Extracting zipfile
archieve = zipfile.ZipFile("folder_path", 'r')
archieve.extractall()
archieve.close()

tf.random.set_seed(42)

# Preprocessing data (Numerical encoding, Normalization)
train_dategen = ImageDataGenerator(rescale=1./255)

train_dir = "train_data folder path"

# importing data from data generator
train_data = train_datagen.flow_from_directory(train_dir,
											  batch_size=32,
											  target_size=(224,224),
											  class_mode="binary",
											  seed=42)
```

# Build a Model
[CNN Explainer]([CNN Explainer (poloclub.github.io)](https://poloclub.github.io/cnn-explainer/))
## Convolutional Layer
**Contains the learned weights**, which extract features that distinguish different images from one another

Hyperparameters
1. `Padding`
	- Conserves data at the border of activation maps
	- Preserve the input's spatial size
	- `valid`: default / `same`: maintain the size
1. `Kernel Size`
	- Filtering size of the data
	- Smaller kernel size leads to deeper architecture, takes longer
	- larger kernel size leads to poor architecture, takes less; suites for larger features
2. `Strider`
	- Indicates how many pixels the kernel should be shited over at a time
	- Smaller strider let model learn more; leads to larger label layer
	- kernel needs to slide across the input symmetrically

## Pooling Layers
Gradually decreasing spatial extent of the network
- Reduces the parameters and overall computation of the network

Max-Pooling
- *Downsamples the input by taking the maximum value* over the window defined by `pool_size`
- Requires kernel size and stride length selection during design
- Once selected, the operation slides the kernel with the specified stride over the input while only selecting the largest value at each kernel slice from the input to yield a value for the output.

## Flatten Layer
Converts a 3D layer in the network into a 1D vector
- Because [[Activation Functions#Softmax|Softmax]] activation requires 1D vector

# Fit the Model
```python
history = model.fit(train_data,
				   epochs = 'int',
				   steps_per_epoch = len(train_data),
				   validation_data = valid_data,
				   validation_steps=len(valid_data))
```
- `len(data)` = `total # of images / batch_size`

# Evaluate the Model
[[AI & TensorFlow/TensorFlow Bootcamp/3. Neural Network Classification in TF#Classification Evaluation Method|Classification Evaluation]]
## Induce Overfitting
After Creating the baseline, we need to beat it by overfitting a larger model
1. Increase the number of `Conv` layers
2. Increase the number of `Conv` filters
3. Add another dense layer to the output of flattened layer

## Reduce Overfitting(Regularization)
1. Add data argumentation
2. Simplify the model
	- Reduce # of layers or reduce # hidden units in layers
3. Add regularization layers (such as `MaxPool2D`)
4. Add more data
	- Having more data gives a model more opportunity to learn diverse patterns
5. Use [[5. Transfer Learning with TF|Transfer Learning]]
	- Transfer Learning leverages the patterns another model has learned on similar data to your own and allows you to use those patterns on your own dataset
# Data Argumentation
Process of altering the training data, leading it to have more diversity and in turn allowing our models to learn more generalizable patterns.
- Usually performed on the training data

## With `ImageDataGenerator()`
This can be done while making training set from `ImageDataGenerator()` by specifying parameters
- Uses `CPU`

```python
train_datagen_augmented = ImageDataGenerator(rescale=1/255.,
                                             rotation_range=20, 
                                             shear_range=0.2, 
                                             zoom_range=0.2, 
                                             width_shift_range=0.2,
                                             height_shift_range=0.2, 
                                             horizontal_flip=True) 
```

## With `tf.keras.layers()`
Adding a **data augmentation "layer"** into model
- **Uses GPU**

 [`tf.keras.layers.RandomFlip`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomFlip)
 - flips image on horizontal or vertical axis.
 [`tf.keras.layersRandomRotation`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomRotation) 
 - randomly rotates image by a specified amount.
 [`tf.keras.layers.RandomZoom`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomZoom) 
 - randomly zooms into an image by specified amount.
 [`tf.keras.layers.RandomHeight`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomHeight) 
 - randomly shifts image height by  specified amount.
 [`tf.keras.layers.RandomWidth`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomWidth) 
 - randomly shifts image width by a specified amount.
 [`tf.keras.layers.Rescaling`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Rescaling) 
 - normalizes the image pixel values to be between 0 and 1, this is worth mentioning because it is required for some image models but since we're using 
```python
data_augmentation = keras.Sequential([
	tf.keras.layers.RandomFlip("horizontal"), 
	tf.keras.layers.RandomRotation(0.2), 
	tf.keras.layers.RandomZoom(0.2), 
	tf.keras.layers.RandomHeight(0.2), 
	tf.keras.layers.RandomWidth(0.2)], name ="data_augmentation")
```


# Making a Prediction
When making aprediction with a custom image, it has to be in the same shape as the mdoel has been trained on
```python
def load_and_prep_image(filename, img_shape=224):
  """
  Reads an image from filename, turns it into a tensor
  and reshapes it to (img_shape, img_shape, colour_channel).
  """
  # Read in target file (an image)
  img = tf.io.read_file(filename)

  # Decode the read file into a tensor & ensure 3 colour channels 
  # (our model is trained on images with 3 colour channels and sometimes images have 4 colour channels)
  img = tf.image.decode_image(img, channels=3)

  # Resize the image (to the same size our model was trained on)
  img = tf.image.resize(img, size = [img_shape, img_shape])

  # Rescale the image (get all values between 0 and 1)
  img = img/255.
  return **img**
```

```python
def pred_and_plot(model, filename, class_names):
  """
  Imports an image located at filename, makes a prediction on it with
  a trained model and plots the image with the predicted class as the title.
  """
  # Import the target image and preprocess it
  img = load_and_prep_image(filename)

  # Make a prediction
  pred = model.predict(tf.expand_dims(img, axis=0))

  # Get the predicted class
  pred_class = class_names[int(tf.round(pred)[0][0])]

  # Plot the image and predicted class
  plt.imshow(img)
  plt.title(f"Prediction: {pred_class}")
  plt.axis(False);
```

# Multi-class Image Classification
