# Regression
Set of statistical process for estimating the relationships between dependent variable and one or more independent variable
- Dependent variable: `outcome variable`, `labels`
- Independent variable: `predictors`, `covariates`, `features`

| Hyperparameter           | Typical Values                                                                                |
| ------------------------ | --------------------------------------------------------------------------------------------- |
| Input layer shape        | Same shape as number of features                                                              |
| Hidden layer(s)          | Problem specific, minimum=1, maximum=unlimited                                                |
| Neurons per hidden layer | Problem specific; generally 10 to 100                                                         |
| Output layer shape       | Same shape as desired prediction shape                                                        |
| Hidden Activation        | Usually `ReLU` (Rectified linear unit)                                                        |
| Output Activation        | None, `ReLU`, logistic/tanh                                                                   |
| Loss Function            | `MSE(Mean Square Error)` `MAE(Mean Absolute Error)/Huber(Combination of MAE/MSE)` if outliers |
| Optimizer                | `SGD(Stochastic Gradient Descent)`, `Adam`                                                    |

# Steps in Modelling with `tf.keras.Sequential`
`Sequential` groups a linear stack of layers into a `tf.keras.Model`

## 1. Creating a Model
Define the input, output and hidden layers
- `tf.keras.layers.Dense('number of neurons')`
- Use **`tf.float32` as default**

```python
# Set random seed
tf.random.set_seed(42)

# Optionally, the first layer can recieve an 'input_shape' argument
model=tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8,input_shape=(16)))

# Afterwards, we do automatic shape inference
 nmodel.add(tf.jkeras.layers.Dense(100))

# Instead of using tf.add() method, we can put layers as a list
mode = tf.keras.Sequential([tf.keras.layers.Dense(8), tf.keras...])

```

## 2. Compiling a model
Define 
- A loss function 
- Optimizer: Tells our model how to imporve the patterns its learning 
	- `Learning Rate(lr)`: How much the model should improve at each step(higher `lr`, the optimizer pushes the model more)
- Evaluation metrics: What we can use to interpret the performance of our model
```python
mode.compile(loss=tf.keras.losses.mae,
			 optimizer=tf.keras.optimizers.SGD(),
			 metrics=["mae"])
# or
model.compile(loss=tf.keras.losses.mae,
			 optimizer=tf.keras.optimizers.Adam(lr=0.0001),
			 metrics=["mae"])

# We can specify parameters using String by ""
 => optimizer="sgd"
```

## 3. Fitting the model
Letting the model try to find pattern
```python
model.fit('features', 'labels', epochs='int') 
```
- `Epochs` refers to the number of opportunity for the model to find pattern
- The features is required to be rank of higher than 1, thus, if the input is a vector, we need to [[AI & TensorFlow/TensorFlow Bootcamp/1. TF Fundamentals#Using `tf.expand_dims`|expand it]]

Make a Prediction from Model
```python
model.predict('list of inputs')
```

## 3.5 Improve Model
We can improve the model by altering the previous steps

1. Creating the model
	- Add more layers
	- Increase number of hidden units(neurons)
	- Change activation function
2. Compiling a model
	- Change optimization function
	- **Change the learning rate of the optimization**
3. Fitting a model
	- More epoch
	- Fitting on more data

**Note)**
- Not all of them lead to improve the model
- Metrics sometimes can't be a good representative of how good the model is
	- If the model is over-trained by features, it can't predict from the input that it haven't seen before

## 4. Evaluating the Model
The total data set is divided into three sets
- *To test the ability for a ML model to perform well on data it hasn't seen before*

Training Set (Course Materials)
- The model **learns** from this data
- *70-80%* of the total data available
- Features: `X_train` / Labels: `y_train`

Validation Set (Practice Exam)
- The model gets **tuned** on this data
- *10-15%* of the data available

Test Set (Final Exam)
- The model gets **evaluated** on this data to test what is has learned
- *10-15%* of the total data available
- Features: `X_test` / Labels: `y_test`

### Example
```python
# Plot training data in blue
plt.scatter(X_train, y_train, c='b', label="Training Data")

# Plot test data in green
plt.scatter(X_test, y_test, c='g', label="Testiong data")

# Show Legend
plt.legend()

plt.show()
```

 