# Regression
Set of statistical process for estimating the relationships between dependent variable and one or more independent variable
- Dependent variable: `outcome variable`, `labels`
- Independent variable: `predictors`, `covariates`, `features`

| Hyperparameter           | Typical Values                                                                                |
| ------------------------ | --------------------------------------------------------------------------------------------- |
| Input layer shape        | Same shape as number of features                                                              |
| Hidden layer(s)          | Problem specific, minimum=1, maximum=unlimited                                                |
| Neurons per hidden layer | Problem specific; generally 10 to 100                                                         |
| Output layer shape       | Same shape as desired prediction shape                                                        |
| Hidden Activation        | Usually `ReLU` (Rectified linear unit)                                                        |
| Output Activation        | None, `ReLU`, logistic/tanh                                                                   |
| Loss Function            | `MSE(Mean Square Error)` `MAE(Mean Absolute Error)/Huber(Combination of MAE/MSE)` if outliers |
| Optimizer                | `SGD(Stochastic Gradient Descent)`, `Adam`                                                    |

# Steps in Modelling with `tf.keras.Sequential`
`Sequential` groups a linear stack of layers into a `tf.keras.Model`

## 1. Creating a Model
Define the input, output and hidden layers
- `tf.keras.layers.Dense('number of neurons')`
- Use **`tf.float32` as default**
- Name the layer with `name` parameter

```python
# Set random seed
tf.random.set_seed(42)

# Optionally, the first layer can recieve an 'input_shape' argument
model=tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8,input_shape=(16)))

# Afterwards, we do automatic shape inference
 nmodel.add(tf.jkeras.layers.Dense(100))

# Instead of using tf.add() method, we can put layers as a list
mode = tf.keras.Sequential([tf.keras.layers.Dense(8), tf.keras...])

```

## 2. Compiling a model
Define 
- A loss function 
- Optimizer: Tells our model how to imporve the patterns its learning 
	- `Learning Rate(lr)`: How much the model should improve at each step(higher `lr`, the optimizer pushes the model more)
- Evaluation metrics: What we can use to interpret the performance of our model
```python
mode.compile(loss=tf.keras.losses.mae,
			 optimizer=tf.keras.optimizers.SGD(),
			 metrics=["mae"])
# or
model.compile(loss=tf.keras.losses.mae,
			 optimizer=tf.keras.optimizers.Adam(lr=0.0001),
			 metrics=["mae"])

# We can specify parameters using String by ""
 => optimizer="sgd"
```

## 3. Fitting the model
Letting the model try to find pattern
```python
model.fit('features', 'labels', epochs='int') 
```
- `Epochs` refers to the number of opportunity for the model to find pattern
- The features is required to be rank of higher than 1, thus, if the input is a vector, we need to [[AI & TensorFlow/TensorFlow Bootcamp/1. TF Fundamentals#Using `tf.expand_dims`|expand it]]

### Make a Prediction from Model
To visualize the prediction, it is a good idea to plot them against the ground truth labels
- `y_test / y_true` versus `y_pred` 
```python
y_pred=model.predict(X_test)
```

### `tf.summary()`
We can see the current status of the model
- `Total Param`: Total number of parameters in the model
- `Trainable Params`: Parameter that the model can update as it trains
- `Non-Trainable Params`: Parameter that won't be updated; when trained model participates during transfer learning

[Additional Video{MIT Introduction to Deep Learning)](https://www.youtube.com/watch?v=ErnWZxJovaM)

## 3.5 Improve Model
We can improve the model by altering the previous steps

1. Creating the model
	- Add more layers
	- Increase number of hidden units(neurons)
	- Change activation function
2. Compiling a model
	- Change optimization function
	- **Change the learning rate of the optimization**
3. Fitting a model
	- More epoch
	- Fitting on more data

**Note)**
- Not all of them lead to improve the model
- Metrics sometimes can't be a good representative of how good the model is
	- If the model is over-trained by features, it can't predict from the input that it haven't seen before

## 4. Evaluating the Model
The total data set is divided into three sets
- *To test the ability for a ML model to perform well on data it hasn't seen before*

Training Set (Course Materials)
- The model **learns** from this data
- *70-80%* of the total data available
- Features: `X_train` / Labels: `y_train`

Validation Set (Practice Exam)
- The model gets **tuned** on this data
- *10-15%* of the data available

Test Set (Final Exam)
- The model gets **evaluated** on this data to test what is has learned
- *10-15%* of the total data available
- Features: `X_test` / Labels: `y_test`

### Evaluating Model's Predictions with Regression Evaluation Metrics
1. MAE (Mean Absolute Error)
	- On average, how wrong is each of my model's prediction
	- `tf.keras.losses.MAE()` or `tf.metrics.ean_absolute_error('true_label', 'prediction')`
	- Great starter metric for any regression problem
2. MSE (Mean Square Error)
	- Square the average errors
	- `tf.keras.losses.MSE()` or `tf.metric.mean_square_error('true_label', 'prediction')`
	- When larger errors are more significant than smaller errors
3. Huber
	- Combination of MSE and MAE
	- `tf.keras.losses.Huber()`
	- Less sensitive to outliers than MSE

**Make sure to make two sets have the same shape**

`tf.evaluate(X_test, y_test)`
- This will automatically use the evaluation metrics from the model


# Plotting Model
```python
from tf.keras.utils import plot_model

# Create a diagram of Neural Network
plot_model(model='model', show_shape=True)

```

