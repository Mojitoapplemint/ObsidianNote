# Definition
$$\begin{bmatrix}
a_{11}&a_{12}&a_{13}&\dots &a_{1m} \\
a_{21}&a_{22}&a_{23}&\dots &a_{2m} \\
\vdots &\vdots&\vdots&&\vdots \\
a_{n1}&a_{n2}&a_{n3}&\dots &a_{nm}
\end{bmatrix}$$

An $n\times m$ matrix is a 2D array of numbers with $n$ rows and $m$ columnes. The row and column must line up each other and spot in the matrix must have an entry
- $a_{ij}$ : The element at $i$'th row and $j$'th column
- Shorter way to denote metrix: $A=[a_{ij}]$
- Two matrices are the same if all entries in the same spot are equal

## Kinds of Matrix
1. Square Matrix
	- When $n=m$
	- $a_{ii}$ is called diagonal entries
2. Identity Matrix
	- 1's along the diagonal, 0's everywhere else
	- Always Square
	- Denoted as $I_{n}$
	- Ex) $I_{3}=\begin{bmatrix}1&0&0\\0&1&0\\0&0&1\end{bmatrix}$
3. Zero Matrix
	- All entries are 0
	- Doesn't have to be square

# Matrix Arithmetic
1. Scalar Multiplication
	- Multiplying all entries by the scalar
	- $c\cdot A = [c\cdot a_{ij}]$
2. Matrix Addition
	- Two matrices must have the same size
	- $A+B = [a_{ij}+b_{ij}]$
3. $A+0=A$
	- Adding zero matrix

## Matrix Operation
### 1. $A+B=B+A$
### 2. $(A+B)+C = A+(B+C)$
### 3. $c(A+B)=cA+cB$
### 4. $(c+d)A = cA+dA$
### 5. $c(dA)=d(cA)=(cd)A$
### 6. $A+0=A$
### 7. $A-A=0$
### 8. $1\cdot A=A$

# Matrix Multiplication
If $A$ is $m\times n$ matrix and $B$ is $n\times l$ matrix, then $AB$ is $m\times l$ matrix, $[c_{ij}]$ where
$$c_{ij}=\sum^{j}_{n=1}a_{in}\cdot b_{ni}= a_{i1}\cdot b_{1i}+a_{i 2}\cdot b_{2i}\dots +a_{ij}\cdot b_{ji}$$
- Dot product of $i$ th row of $A$ and $j$ th column of $B$

**Note)**
Matrix Multiplication can also be used to represent linear system
$$\begin{matrix}
x_{1}-2x_{2}+3x_{3}=5 \\
-x_{1}+3x_{2}+x_{3}=1 \\
2x_{1}-x_{2}+4x_{3}=14
\end{matrix}\implies \begin{bmatrix}
1 &-2&3 \\
-1&3&1 \\
2&-1&4
\end{bmatrix}\cdot \begin{bmatrix}
x_{1} \\
x_{2} \\
x_{3}
\end{bmatrix}=\begin{bmatrix}
5 \\
1 \\
14
\end{bmatrix}$$

## Matrix Multiplication Properties
Let $A,B,C$ be valid matricex for matrix multiplication and let $k$ be scalar
- Let $I$ be identity matrix
### 1. $A(BC)=(AB)C$
### 2. $A(B+C)=AB+AC$
### 3. $(A+B)C=AC+BC$
### 4. $k(AB)=(kA)B+A(kB)$
### 5. $IA=AI=A$
### 6. $0\cdot A=A\cdot 0=0$
### 7. $A^{k}=A\cdot A\cdot A\dots A$
- Works for square matrix only
### 8. $A^{0}=I$
### 9. $A^{k+l}=A^{k}\cdot A^{l}$

**Note)** $AB=BA$ is *not guaranteed*

# Transpose
A Transpose of $m\times n$ matrix $A$ is the $n\times m$ matrix $A^{T}$, obtained by interchanging the rows and columns of $A$
- $A=[a_{ij}]$, then $A^{T}=[a_{ji}]$

A square matrix is symmetric if $A=A^{T}$

## Properties
### 1. $(A^{T})^{T}=A$
### 2. $(A+B)^{T}=A^{T}+B^{T}$
### 3. $(cA)^{T}=c(A^{T})$
### 4. $(A^{k})^{T}=(A^{T})^{k}$
### 5. $(AB)^{T}=B^{T}\cdot A^{T}$
Not $A^{T}\cdot B^{T}$

**Proof)**
We will calculate the each $(i,j)$ entry of $(AB)^{T}$ and $B^{T}\cdot A^{T}$
- $[(AB)^{T}]_{ij}=[AB]_{ji}=[j^{th}\text{ row of }A]\cdot[i^{th}\text{ column of }B]$
- $[(B^{T}\cdot A^{T})]_{ij}=[i^{th}\text{ row of }B^{T}]\cdot[j^{th}\text{ column of }A^{T}]$
	- $=[i^{th}\text{ column of }B]\cdot[j^{th}\text{ row of }A]$
	- $=[j^{th}\text{ row of }A]\cdot[i^{th}\text{ column of }B]$

# Inverse Matrix
Let $A$ be $n\times n$ matrix, then define Inverse of $A$ is an $n\times n$ matrix $A^{-1}$, such that
$$A^{-1}\cdot A=I_{n}=A\cdot A^{-1}$$
- $A$ is called invertible
- Not all square matrix has an inverse
## Inverse Matrix is unique
Suppose $A'$ and $\hat{A}$ are both inverses to $A$
- $A'\cdot A=A\cdot A'=I$
- $A\cdot \hat{A}=\hat{A}\cdot A=I$

$\hat{A}=\hat{A}\cdot I$
- $=\hat{A}\cdot(AA')$
- $=(\hat{A}A)A'$
- $=IA'$
- $=A'$

## Corollary: if $BA=I$ and $AC=I$, then $B=C=A^{-1}$
In finite dimensions, if a matrix has either a left or a right inverse, then the matrix is invertible

## How to find an inverse : Gauss-Jordan Method
Construct augmented matrix
$$[A\text{ | }I]$$
1. Do row reduction to get to R/REF on the LHS of the augmented matrix
2. If it eventually leads to $[I\text{ | }B]$, then $B=A^{-1}$
3. Otherwise, $A$ is not invertible

**Ex)**
Find inverse of $A=\begin{bmatrix}1&2&-1\\2&2&4\\1&3&-3\end{bmatrix}$

$[A\text{ | }I]=\begin{bmatrix}1&2&-1&|&1&0&0\\2&2&4&|&0&1&0\\1&3&-3&|&0&0&1\end{bmatrix} \implies\begin{bmatrix}1&0&0&|&9&-\frac{3}{2}&-5\\0&1&0&|&-5&1&3\\0&0&1&|&-2&\frac{1}{2}&1\end{bmatrix}$

$\therefore A^{-1}=\begin{bmatrix}9&-\frac{3}{2}&-5\\-5&1&3\\-2&\frac{1}{2}&1\end{bmatrix}$

# Fundamental Theorem of Invertible Matrices
Let $A$ be an $n\times n$ that is invertible iff
1. $A \vec{x}=\vec{b}$  /  $[A\text{ | }b]$ has a unique solution for every $\vec{b}\in \mathbb{R}^{n}$
2. $A\vec{x}=\vec{0}$  /  $[A\text{ | }0]$ has only the trivial solution
3. R/REF of $A$ is $I_{n}$
4. $\text{rank}(A)=n$
5. $\text{dim(null(}A))=0$
6. Row vectors of $A$ are linearly independent
	6'. Column vectors of $A$ are linearly independent
7. $\text{row}(A)$ spans $\mathbb{R}^{n}$
	7'. $\text{col}(A)$ spans $\mathbb{R}^{n}$
8. Row vectors of $A$ form a basis for $\mathbb{R}^{n}$
	8'. Column vectors of $A$ form a basis for $\mathbb{R}^{n}$
9. A is a product of elementary matrices
	- Elementary Matrix: Matrix can be obtained by doing one elementary row operation on an identity matrix
10. $\text{det}(A)\neq0$
11. $\lambda=0$ is not eigen value

## Example) If $A$ is invertible, then $A\vec{x}=\vec{b}$ has a unique solution
We need to show
1. $\vec{x}=A^{-1}\vec{b}$ is a solution
2. $A^{-1}\vec{b}$ is the only solution

i) $\vec{x}=A^{-1}\vec{b}$ is a solution
- $A\vec{x}=A(A^{-1}\vec{b})=(AA^{-1})\vec{b}=I\vec{b}=\vec{b}$

ii) $A^{-1}\vec{b}$ is the only solution
- Suppose $\vec{y}$ is also a solution : $A\vec{y}=\vec{b}$
- $A^{-1}(A\vec{y})=A^{-1}(\vec{b})$
- $\vec{y}=A^{-1}\vec{b}=\vec{x}$
- $\vec{y}=\vec{x}$

Due to this, if we know $A^{-1}$, then the solution of $[A\text{ | }\vec{b}]$ is $A^{-1}\vec{b}$

## Example 2) Is $A=\begin{bmatrix}1&2 \\ 3&4\end{bmatrix}$ invertible?
Let's utilize 6'
- $c_{1}\begin{bmatrix}1\\3\end{bmatrix}+c_{2}\begin{bmatrix}2\\4\end{bmatrix}=\begin{bmatrix}0\\0\end{bmatrix}\implies \begin{matrix}c_{1}+2c_{2}=0\\3c_{1}+4c_{2}=0\end{matrix}\implies \begin{matrix}c_{1}=0\\c_{2}=0\end{matrix}$
- Columns are linearly independent
- Since 6' is true, it is invertible

# Linear Transformation
A transformation $T:\mathbb{R}^{n}\longrightarrow \mathbb{R}^{m}$ is called a linear transformation if
- $T(\vec{x}+\vec{y})=T\cdot\vec{x}+T\cdot \vec{y}$
- $T(c\cdot \vec{x})=c\cdot T\vec{x}$

**Ex)** 
Is $T:\mathbb{R}^{2}\longrightarrow \mathbb{R}^{2} : [x,y]\longmapsto[-y, x]$ a linear transformation?

i) $T\left(\begin{bmatrix}a_{1}\\a_{2}\end{bmatrix}+\begin{bmatrix}b_{1}\\b_{2}\end{bmatrix}\right)=T\left(\begin{bmatrix}a_{1}+b_{1}\\a_{2}+b_{2}\end{bmatrix}\right)=\begin{bmatrix}-a_{2}-b_{2}\\a_{1}+b_{1}\end{bmatrix}=\begin{bmatrix}-a_{2}\\a_{1}\end{bmatrix}+\begin{bmatrix}-b_{2}\\b_{1}\end{bmatrix}=T\left(\begin{bmatrix}a_{1}\\a_{2}\end{bmatrix}\right)+T\left(\begin{bmatrix}b_{1}\\b_{2}\end{bmatrix}\right)$
ii) 
$T\left(c\cdot \begin{bmatrix}a_{1}\\a_{2}\end{bmatrix}\right)=T\left(\begin{bmatrix}ca_{1}\\ca_{2}\end{bmatrix}\right)=\begin{bmatrix}-ca_{2}\\ca_{1}\end{bmatrix}=c\begin{bmatrix}-a_{2}\\a_{1}\end{bmatrix}=c\cdot T\left(\begin{bmatrix}a_{1}\\a_{2}\end{bmatrix}\right)$

Yes, indeed it is a linear transformation


## MatMul as Linear Transformation
Matrix Multiplication is linear transformation
- Let $T_{A}(X)=A\cdot X$ 

i) $T_{A}(X+Y)=A(X+Y)=AX+AY=T_{A}(X)+T_{A}(Y)$

ii) $T_{A}(cX)=A(c\cdot X)=c(A\cdot X)=c\cdot T_{A}(X)$

Not only matrix multiplication is a linear transformation, if we have a linear transformation $T$, then there exists some matrix $A$ such that $T(X)=AX$

How can we find $A$?
- Suppose $T:\mathbb{R}^{n}\to \mathbb{R}^{m}$ is linear transformation, let $\{ \vec{e_{i}} \}$ be a standard unit vectors for $\mathbb{R}^{n}$
- Then, the matrix representing the linear transformation $T$ is $A=[T(\vec{e_{1}}), T(\vec{e_{2}})\dots T(\vec{e_{n}})]$

**Proof)**
Let $T:\mathbb{R}^{n}\to\mathbb{R}^{m}:\vec{x}=[x_{1},x_{2}\dots x_{n}]\mapsto \vec{y}$

Since $\vec{x}=x_{1}\vec{e_{1}}+x_{2}\vec{e_{2}}\dots x_{n}\vec{e_{n}}$ , 
$\vec{y}=T(\vec{x})=T(x_{1}\vec{e_{1}}+x_{2}\vec{e_{2}}\dots x_{n}\vec{e_{n}})$
   $=x_{1}T(\vec{e_{1}})+x_{2}T(\vec{e_{2}})\dots x_{n}T(\vec{e_{n}})$
   $=[T(\vec{e_{1}}),T(\vec{e_{2}})\dots T(\vec{e_{n}})]\cdot[x_{1}, x_{2},\dots x_{n}]=A\cdot \vec{x}$

$\therefore A=[T(\vec{e_{1}}),T(\vec{e_{2}})\dots T(\vec{e_{n}})]$
