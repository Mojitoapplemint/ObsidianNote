Matrix Decompositions are ways of writing down a matrix as a product of matrices, such as diagonalization

If $A\in \mathcal{M}_{n}(\mathbb{F})$ then the following are equivalent
- $\exists \text{ }P,D\in \mathcal{M}_{n}(\mathbb{F})$ s.t $A=PDP^{-1}$ where $P$ is invertible and $D$ is diagonal
- There is a basis of $\mathbb{F}^{n}$ consisting of eigenvectors of $A$

Q) How close to diagonal can $D$ be if we require that $P$ is unitary?

# Theorem 7.1 Schur Triangularization
Suppose $A \in M_n(\mathbb{C})$. Then there exists a unitary matrix $U \in M_n(\mathbb{C})$ and an upper triangular matrix $T \in M_n(\mathbb{C})$ such that  
$$A = U T U^*$$

**Proof)** 
Proof by induction on the size of $A$

For the base case, we simply notice that the result is trivial if $n = 1$: every $1 \times 1$ matrix is upper triangular.

Assume that theorem is true for matrices in $M_{n-1}(\mathbb{C})$. 
Let $A \in M_n(\mathbb{C})$, and let $\lambda \in \mathbb{C}$ and $\vec{v} \in \mathbb{C}^n$ be an eigenvalue/vector pair 
- (i.e., $Aa = \lambda a$) with $\|\vec{v}\| = 1$.

Let $U = [\vec{v} \mid V]$ for some $V \in M_{n,n-1}(\mathbb{C})$ that makes $U$ unitary.
- We can apply Gram-Schmidt to find $V$
- This means that $\vec{v}$ and all column vectors in $V$ are orthogonal

Let's compute $U^*AU$:

$U^*AU = [\vec{v} \mid V]^*A[\vec{v} \mid V] = \begin{bmatrix}\vec{v}^* \\V^*\end{bmatrix}A\,[\vec{v} \mid V]$

$= \begin{bmatrix}\vec{v}^* \\V^*\end{bmatrix}[A\vec{v} \mid AV] = \begin{bmatrix}\vec{v}^*A\vec{v} & \vec{v}^*AV \\V^*A\vec{v} & V^*AV\end{bmatrix}$
- Each entry is block matrix multiplication
- Consider bottom-left entry
	- $A\vec{v} = \lambda \vec{v}$
	- Then, $\lambda V^{*}\vec{v}$
	- Each entry of $V^{*}\vec{v}$ is a dot product of row vectors of $V^{*}$(column vectors of $V$). Since $V$'s column vectors are orthogonal to $\vec{v}$, each entry will be zero
	- $\therefore V^{*}\vec{v}=\vec{0}$

$= \begin{bmatrix}\lambda\vec{v}^*\vec{v} &\vec{v}^*AV \\\vec{0} & V^*AV\end{bmatrix}$

$=\begin{bmatrix} *&*&*&\dots&*\\0&*&*&\dots&* \\ \vdots & \vdots&\vdots&\ddots&\vdots \\ 0&*&*&\dots&*\end{bmatrix}$

Consider $V^{*}AV\in \mathcal{M}_{n-1}(\mathbb{C})$
- By inductive hypothesis, there exists $U_{2}$ and $T_{2}$ such that
$$V^{*}AV=U_{2}T_{2}U_{2}^{*}$$
where $U_{2}$ is unitary and $T_{2}$ is upper triangle.

Then, Consider $\left(U\begin{bmatrix}1& \vec{0}^{*\\}\\\vec{0}& U_{2}\end{bmatrix}\right)^{*} A \left(U\begin{bmatrix}1& \vec{0}^{*\\}\\\vec{0}& U_{2}\end{bmatrix}\right)$
- $\left(U\begin{bmatrix}1& \vec{0}^{*\\}\\\vec{0}& U_{2}\end{bmatrix}\right)$ is unitary since $U$ and $\begin{bmatrix}1& \vec{0}^{*\\}\\\vec{0}& U_{2}\end{bmatrix}$ are all unitary

$=\begin{bmatrix}1& \vec{0}^{*\\}\\\vec{0}& U_{2}\end{bmatrix}^{*}U^{*} A U\begin{bmatrix}1& \vec{0}^{*\\}\\\vec{0}& U_{2}\end{bmatrix}$

$=\begin{bmatrix}1& \vec{0}^{*\\}\\\vec{0}& U_{2}\end{bmatrix}^{*}\begin{bmatrix}\lambda\vec{v}^*\vec{v} &\vec{v}^*AV \\\vec{0} & V^*AV\end{bmatrix}\begin{bmatrix}1& \vec{0}^{*\\}\\\vec{0}& U_{2}\end{bmatrix}$

$=\begin{bmatrix}1& \vec{0}^{*\\}\\\vec{0}& U_{2}^{*}\end{bmatrix}\begin{bmatrix}\lambda\vec{v}^*\vec{v} &\vec{v}^*AV \\\vec{0} & V^*AV\end{bmatrix}\begin{bmatrix}1& \vec{0}^{*\\}\\\vec{0}& U_{2}\end{bmatrix}$

$=\begin{bmatrix}\lambda\vec{v}^*\vec{v} &\vec{v}^*AVU_{2} \\\vec{0} & U_{2}^{*}V^*AVU_{2}\end{bmatrix}$

$=\begin{bmatrix}\lambda\vec{v}^*\vec{v} &\vec{v}^*AVU_{2} \\\vec{0} & T_{2}\end{bmatrix}$

Since $T_{2}$ is upper triangle, we've found some upper triangular matrix

$\left(U\begin{bmatrix}1& \vec{0}^{*\\}\\\vec{0}& U_{2}\end{bmatrix}\right)^{*} A \left(U\begin{bmatrix}1& \vec{0}^{*\\}\\\vec{0}& U_{2}\end{bmatrix}\right)=\begin{bmatrix}\lambda\vec{v}^*\vec{v} &\vec{v}^*AVU_{2} \\\vec{0} & T_{2}\end{bmatrix}$

$\therefore A=\left(U\begin{bmatrix}1& \vec{0}^{*\\}\\\vec{0}& U_{2}\end{bmatrix}\right)\begin{bmatrix}\lambda\vec{v}^*\vec{v} &\vec{v}^*AVU_{2} \\\vec{0} & T_{2}\end{bmatrix}\left(U\begin{bmatrix}1& \vec{0}^{*\\}\\\vec{0}& U_{2}\end{bmatrix}\right)^{*}$

# Characteristic of Schur triangularization
## 1. Diagonal entries of $T$ are the eigenvalues of $A$
Recall that the eigenvalues of triangular matrix are its diagonal entries

Consider characteristic polynomial of $A$ : $P_{A}(\lambda)=\det(A-\lambda I)$

$=\det(UTU^{*}-\lambda I)=\det(UTU^{*}-U\lambda IU^{*})=\det(U(T-\lambda I)U^{*})$

$=\det(U)\det(T-\lambda I)\det(U^{*})$

Since $\det(U^{*})=\frac{1}{\det(U)}$, $=\det(T-\lambda I)=P_{T}(\lambda)$

Therefore, eigenvalues of $A$ are eigenvalues of $T$, which are the diagonal entries of $T$

## 2. Not Unique
Entries in $U$ and the strictly upper-triangular entries in $T$ have lots of freedom

# Theorem 7.2 Trace and Determinant in Terms of Eigenvalues
Suppose $A \in M_n(\mathbb{C})$ has eigenvalues $\lambda_1, \lambda_2, \ldots, \lambda_n$. Then  
$$\det(A) = \lambda_1 \lambda_2 \cdots \lambda_n \quad \text{and} \quad \text{tr}(A) = \lambda_1 + \lambda_2 + \cdots + \lambda_n.$$

**Proof)**
Since the diagonal entries of $T$ are $\lambda_1, \lambda_2, \ldots, \lambda_n,$

i) Consider $\det(A)$
$\det(A) = \det(UTU^*) = \det(U)\det(T)\det(U^*)=\det(T)$
- Determinant is multiplicative

Since $T$ is triangular, $= \lambda_1 \lambda_2 \cdots \lambda_n$

ii) Consider $\text{tr}(A)$
$\text{tr}(A) = \text{tr}(UTU^*) = \text{tr}(U^*UT) = \text{tr}(T) = \lambda_1 + \lambda_2 + \cdots + \lambda_n$

# Caley-Hamilton Theorem
Suppose $A\in \mathcal{M}_{n}(\mathbb{C})$ has a characteristic polynomial $p(\lambda)=\det(A-\lambda I)$. Then, $p(A)=O$

**Proof)**
By Fundamental Theorem of Algebra, we can factor characteristic polynomial as a product of linear terms

$p(\lambda) = (\lambda_1 - \lambda)(\lambda_2 - \lambda) \cdots (\lambda_n - \lambda)$

$\text{C-H} \quad \text{says} \quad p(A) = (\lambda_1 I - A)(\lambda_2 I - A) \cdots (\lambda_n I - A) = 0$

Well, let's Schur triangularize $A$:

$A = UTU^*$, then

$p(A) = (\lambda_1 I - UTU^*) (\lambda_2 I - UTU^*) \cdots (\lambda_n I - UTU^*)$

$= U(\lambda_1 I - T) U^* U(\lambda_2 I - T) U^* \cdots U(\lambda_n I - T) U^*$

$= U(\lambda_1 I - T) (\lambda_2 I - T) \cdots (\lambda_n I - T) U^*$

$=U\,p(T)\,U^{*}$

**Goal:** Show that $p(T)$ is 0

$T = \begin{bmatrix} \lambda_1 & * & * & * \\ 0 & \lambda_2 & * & * \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \lambda_n & * \end{bmatrix}$
so $\lambda_1 I - T = \begin{bmatrix} 0 & * & * & * \\ 0 & \lambda_1 - \lambda_2 & * & * \\ 0 & 0 & \lambda_1 - \lambda_3 & * \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \lambda_1 - \lambda_n \end{bmatrix}$
$\lambda_2 I - T = \begin{bmatrix} \lambda_2 - \lambda_1 & * & * & * \\ 0 & 0 & * & * \\ 0 & 0 & \lambda_2 - \lambda_3 & * \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \lambda_2 - \lambda_n \end{bmatrix}$

Claim: 
- $(\lambda_{1}I-T)(\lambda_{2}I-T)$ has first 2 columns $\vec{0}$
- $(\lambda_{k}I-T)(\lambda_{k}I-T)$ has first $k$ columns $\vec{0}$
- So, $(\lambda_{n}I-T)(\lambda_{n}I-T)=O$ 

$p(T)=(\lambda_1 I - T) (\lambda_2 I - T) \cdots (\lambda_n I - T)$

$=\begin{bmatrix} 0&*&*&\dots&*\\0&*&*&\dots&* \\ \vdots & \vdots&\vdots&\ddots&\vdots \\ 0&*&*&\dots&*\end{bmatrix}\begin{bmatrix} *&*&*&\dots&*\\0&0&*&\dots&* \\ \vdots & \vdots&\vdots&\ddots&\vdots \\ 0&*&*&\dots&*\end{bmatrix}\dots\begin{bmatrix} *&*&*&\dots&*\\0&*&*&\dots&* \\ \vdots & \vdots&\vdots&\ddots&\vdots \\ 0&*&*&\dots&0\end{bmatrix}$

$=\begin{bmatrix} 0&0&*&\dots&*\\0&0&*&\dots&* \\ \vdots & \vdots&\vdots&\ddots&\vdots \\ 0&*&*&\dots&*\end{bmatrix}\dots\begin{bmatrix} *&*&*&\dots&*\\0&*&*&\dots&* \\ \vdots & \vdots&\vdots&\ddots&\vdots \\ 0&*&*&\dots&0\end{bmatrix}$

$\dots =O$

## Express $A^{n}$ as a linear combination of $\{ I, A\dots A^{n-1} \}$
$\text{span}(I,A,A^{2}\dots A^{n})$ has dimension that is $\le n$

**Ex)**
Use the Cayley–Hamilton theorem to come up with a formula for $A^4$ as a linear combination of $A$ and $I$, where  $A = \begin{bmatrix} 5 & 9 \\ 2 & -8 \end{bmatrix}$

$\rho(\lambda) = \det(A - \lambda I) = \det\left( \begin{bmatrix} 5 - \lambda & 9 \\ 2 & -8 - \lambda \end{bmatrix} \right) = (5 - \lambda)(-8 - \lambda) - 18 = \lambda^2 + 3\lambda - 58$
C-H says  
$A^2 + 3A - 58I = 0$
- $A^2 = 58I - 3A$  

Then,
$\Rightarrow \quad A^3 = 58A - 3A^2 = 58A - 3(58I - 3A)= -174I + 67A$  
$\Rightarrow \quad A^4 = -174A + 67A^2 = -174A + 67(58I - 3A)= 67 \cdot 58I - 375A$

Use the Cayley–Hamilton theorem to find the inverse of the same matrix.
- Recall that inverse of the matrix has the power of -1

From above  
$A^2 + 3A - 58I = 0$  
$\Rightarrow A^2 + 3A = 58I$  
$\Rightarrow \frac{1}{58}(A^2 + 3A) = I$  
$\Rightarrow A\left(\frac{1}{58}(A + 3I)\right) = I$  
$\therefore A^{-1}=\frac{1}{58}(A+3I)$
