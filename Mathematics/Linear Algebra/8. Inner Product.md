Generalization  of the dot product to arbitrary vector spaces

# Definition
Suppose that $\mathbb{F} = \mathbb{R}$ or $\mathbb{F} = \mathbb{C}$, and $\mathcal{V}$ is a vector space over $\mathbb{F}$. Then an **inner product** on $\mathcal{V}$ is a function $\langle\cdot,\cdot\rangle : \mathcal{V} \times \mathcal{V} \to \mathbb{F}$ such that the following three properties hold for all $c \in \mathbb{F}$ and all $\vec{v}, \vec{w}, \vec{x}\in \mathcal{V}$:
- Conjugate Symmetry : $\langle \vec{v}, \vec{w}\rangle = \overline{\langle\vec{w}, \vec{v}\rangle}$ 
- Linearity in 2nd Entry : $\langle\vec{v}, \vec{w} + c\cdot\vec{x}\rangle = \langle\vec{v}, \vec{w}\rangle + c\langle\vec{v}, \vec{x}\rangle$
- Positive Definiteness : $\langle\vec{v}, \vec{v}\rangle \geq 0$, with equality if and only if $\vec{v} = 0$

## Linearity in their first argument
Inner products are not linear in their first argument
$\langle\vec{v}+c\vec{x}, \vec{w}\rangle = \overline{\langle\vec{w}, \vec{v}+c\vec{x}\rangle}=\overline{\langle\vec{w}, \vec{v}\rangle+c\langle\vec{w}, \vec{x}\rangle}$
	$=\overline{\langle\vec{w}, \vec{v}\rangle}+\bar{c} \overline{\langle\vec{w}, \vec{x}\rangle}=\langle\vec{v}, \vec{w}\rangle+\bar{c}\langle\vec{x}, \vec{w}\rangle$

## Purpose of conjugate symmetry
If we don't have that, then for some nonzero vector $\vec{v}$
$0 < \langle i\vec{v}, i\vec{v}\rangle=(i)(i)\langle\vec{v}, \vec{v}\rangle=-\langle\vec{v}, \vec{v}\rangle<0$
- Thus, we need that

# Examples
## Standard Inner product in $\mathbb{C}^{n}$
As known as dot product in $\mathbb{C}^{n}$
$$\langle \vec{v}, \vec{w}\rangle=\vec{v}^{*}\vec{w} = \sum^{n}_{i=1}\overline{v_{i}}\cdot w_{i}\quad \text{for all}\quad \vec{v}, \vec{w}\in\mathbb{C}^{n}$$

## Standard Inner product in $\mathcal{C}$
Let $a < b$ be real numbers and let $\mathcal{C}[a, b]$ be the vector space of continuous functions on the interval $[a, b]$. 
$$\langle f, g \rangle = \int_a^b f(x)g(x) \, dx \quad \text{for all} \quad f, g \in \mathcal{C}[a, b]$$

# Trace
Let $A\in \mathcal{M}_{n}$ be a square matrix. Then the trace of $A$, denoted by $\text{tr}(A)$, is th sum of its diagonal entries
$$\text{tr}(A)=a_{1,1}+a_{2,2}\dots a_{n,n}$$

## Properties
- $\text{tr}(A+B)=\text{tr(A)}+\text{tr}(B)$
- $\text{tr}(cA)=c\cdot\text{tr}(A)$
- $\text{tr}(A^{T})=\text{tr}(A)$
- $\text{tr}(AB)=\text{tr}(BA)$

## Frobenius Inner Product
Let $A,B\in \mathcal{M}_{m,n}$. Then
$$\langle A,B\rangle=\text{tr}(A^{*}B)$$
where $A^{*}=(\bar{A})^{T}$, called conjugate transpose, is an inner product 

**Proof)** 
We could prove this is an inner product directly, but let's try different way

$\langle A,B\rangle=\text{tr}(A^{*}B)=\displaystyle\sum^{m}_{i=1}\displaystyle\sum^{n}_{j=1}\overline{a_{i,j}}\cdot b_{i,j}=[A]_{E}\cdot[B]_{E}$
- Where $E=\{ E_{1,1}\dots E_{m,n} \}$
- This looks like a standard inner product on $\mathbb{C}^{mn}$ of coordinate vectors

# Norm Induced by the Inner Product
Suppose that $\mathcal{V}$ is an inner product space. Then the norm induced by the inner product is the function $||\cdot|| : \mathcal{V}\to \mathbb{R}$ defined by
$$\forall \text{ }\vec{v}\in \mathcal{V},\quad||\vec{v}||=\sqrt{ \langle \vec{v}, \vec{v} \rangle }$$

**Ex)**
Norm induced by standard inner product on... 
- $\mathcal{C}[a,b]$
$$||f||=\sqrt{ \int^{b}_{a}f(x)^{2} d{x}  }$$
- $\mathbb{C}^{n}$
$$||\vec{v}|| =\sqrt{ \sum^{n}_{i=1}\overline{v_{i}}v_{i}}=\sqrt{ \sum^{n}_{i=1} |v_{i}|^{2}  }$$
- Frobenius inner product
$$||A||_{F}=\sqrt{ \text{tr}(A^{*}A) }=\sqrt{ \sum^{m}_{i=1}\sum^{n}_{j=1}|a_{i,j}|^{2}   }$$
This is called Frobenius norm

## Properties of the Norm Induced by the IP
Suppose that $\mathcal{V}$ is an inner product space, $\mathbf{v} \in \mathcal{V}$ is a vector, and $c \in \mathbb{F}$ is a scalar. 
1. Absolute Homogeneity : $\|c\vec{v}\| = |c|\|\vec{v}\|$
2. Pos. definiteness : $\|\vec{v}\| \geq 0$, with equality if and only if $\vec{v} = \vec{0}$.

## Cauchy-Schwarz Inequality
If $\mathcal{V}$ is an inner product space then  
$$|\langle \vec{v}, \vec{w} \rangle| \leq \| \vec{v} \| \| \vec{w} \| \quad \text{for all} \quad \vec{v}, \vec{w} \in \mathcal{V}$$
Furthermore, equality holds if and only if $\vec{v}$ and $\vec{w}$ are collinear (i.e., $\{\vec{v}, \vec{w}\}$ is a linearly dependent set).

**Proof)**
We start by letting $c, d \in \mathbb{F}$ be arbitrary scalars and expanding the quantity $\|c\vec{v} + d\vec{w}\|^2$ in terms of the inner product:

$0 \leq \|c\vec{v} + d\vec{w}\|^2$
$= \langle c\vec{v} + d\vec{w}, c\vec{v} + d\vec{w} \rangle$
$= |c|^2 \langle \vec{v}, \vec{v} \rangle + \overline{c}d \langle\vec{v}, \vec{w} \rangle + c\overline{d} \langle \vec{w}, \vec{v} \rangle + |d|^2 \langle \vec{w}, \vec{w} \rangle$
$= |c|^2 \|\vec{v}\|^2 + 2Re(\overline{c}d\langle \vec{v}, \vec{w}\rangle) + |d|^2 \|\vec{w}\|^2$
- Since any $z\in\mathbb{C}, z+\bar{z}=2\cdot\text{Re}(z)$ and $\overline{c}d \langle\vec{v}, \vec{w} \rangle$ and $c\overline{d} \langle \vec{w}, \vec{v} \rangle$ are complex conjugate of each other

If $\vec{w} = 0$ then the Cauchy–Schwarz inequality holds trivially (it just says $0 \leq 0$), so we can assume that $\vec{w} \neq 0$. We can thus choose $c = \|\vec{w}\|$ and $d =-\frac{\langle \vec{w}, \vec{v} \rangle}{\|\vec{w}\|}$, which tells us that

$0 \leq \|\vec{v}\|^2 \|\vec{w}\|^2 - 2Re\left(\|\vec{w}\|\cdot\frac{ \langle \vec{w}, \vec{v} \rangle }{\|\vec{w}\|}\cdot\langle \vec{v}, \vec{w} \rangle \right) + \frac{|\langle \vec{w}, \vec{v} \rangle|^2}{\|\vec{w}\|^2} \cdot\|\vec{w}\|^2$
$= \|\vec{v}\|^2 \|\vec{w}\|^2 - |\langle \vec{w}, \vec{v} \rangle|^2$

Therefore, $|\langle \vec{v}, \vec{w} \rangle| \leq \| \vec{v} \| \| \vec{w} \|$

**Proof for equality)**
$(\Rightarrow)$ Suppose that $|\langle \vec{v}, \vec{w}\rangle| = \|\vec{v}\| \|\vec{w}\|$. 
We can then follow the above proof backward to see that $0 = \|c\vec{v} + d\vec{w}\|^2$, so $c\vec{v} + d\vec{w} = 0$ (where $c = \|\vec{w}\| \neq 0$), so $\{\vec{v}, \vec{w}\}$ is linearly dependent. 

$(\Leftarrow)$ Suppose $\{\vec{v}, \vec{w}\}$ is linearly dependent. 
Then either $\vec{v} = 0$ (in which case equality clearly holds in the Cauchy–Schwarz inequality since both sides equal 0) or $\vec{w} = c\vec{v}$ for some $c \in \mathbb{F}$. Then

$|\langle \vec{v}, \vec{w}\rangle| = |\langle \vec{v}, c\vec{v}\rangle| = |c|\|\vec{v}\|^2 = \|\vec{v}\| \|c\vec{v}\| = \|\vec{v}\| \|\vec{w}\|$

## Triangular Inequality
If $\mathcal{V}$ is an inner product space then
$$
\|\vec{v} + \vec{w}\| \leq \|\vec{v}\| + \|\vec{w}\| \quad \text{for all} \quad \vec{v}, \vec{w} \in \mathcal{V}.
$$

Furthermore, equality holds if and only if $\vec{v}$ and $\vec{w}$ point in the same direction (i.e., $\vec{v} = 0$ or $\vec{w} = c\vec{v}$ for some $0 \leq c \in \mathbb{R}$).

**Proof)** We start by expanding $\|\vec{v} + \vec{w}\|^2$ in terms of the inner product:

Consider $\|\vec{v} + \vec{w}\|^2 = \langle\vec{v} + \vec{w}, \vec{v} + \vec{w}\rangle$
	$= \langle\vec{v}, \vec{v}\rangle + \langle\vec{v}, \vec{w}\rangle + \langle\vec{w}, \vec{v}\rangle + \langle\vec{w}, \vec{w}\rangle$
	$= \|\vec{v}\|^2 + 2\text{Re}(\langle\vec{v}, \vec{w}\rangle) + \|\vec{w}\|^2$

For any $z=x+iy\in\mathbb{C}$, $Re(z)=x\leq \sqrt{ x^{2}+y^{2} }=|z|$, thus
	$\leq \|\vec{v}\|^2 + 2|\langle\vec{v}, \vec{w}\rangle| + \|\vec{w}\|^2$

By Cauchy-Schwarz,
	$\leq \|\vec{v}\|^2 + 2\|\vec{v}\|\|\vec{w}\| + \|\vec{w}\|^2$
	$= (\|\vec{v}\| + \|\vec{w}\|)^2$

# Orthogonality
Suppose $\mathcal{V}$ is an inner product space. Then two vectors $\vec{v}, \vec{w} \in \mathcal{V}$ are called orthogonal if $\langle \vec{v}, \vec{w} \rangle = 0$.

## Geometrical Representation of Linear In/dependence and Orthogonality
![[Pasted image 20250224180934.png|400]]
Intuitively, we can say that Orthogonality is a stronger version of Linear Independence

## Orthogonal Bases
If Orthogonality is stronger than Linear Independence, then we can specialize some basis

A basis $B$ of an inner product space $\mathcal{V}$ is called an orthonormal basis of $\mathcal{V}$ if  
- $\langle \vec{v}, \vec{w} \rangle = 0$ for all $\vec{v} \neq \vec{w} \in B$
	- We can use standard inner product for this
- $\|\vec{v}\| = 1$ for all $\vec{v} \in B$.
	- Normalization

**Ex)**
The standard basis in $\mathbb{R}^{n}$ or $\mathbb{C}^{n}$
- Using Dot product

The standard basis in $\mathcal{M}_{m,n}$
- Using Frobenius inner product

## Theorem 5.5 Orthogonality implies Independence
Let $\mathcal{V}$ be an inner product space and suppose that the set $B = \{ \vec{v}_1, \vec{v}_2, \ldots, \vec{v}_n \} \subset \mathcal{V}$ consists of non-zero mutually orthogonal vectors (i.e., $\langle \vec{v}_i, \vec{v}_j \rangle = 0$ whenever $i \neq j$). Then $B$ is linearly independent.

**Proof)**
Suppose $c_1 \vec{v}_1 + c_2 \vec{v}_2 + \cdots + c_n \vec{v}_n = \vec{0}$. Then... for all $1 \leq j \leq n$,

$\vec{0} = \langle \vec{v}_j, \vec{0} \rangle = \langle \vec{v}_j, c_1 \vec{v}_1 + c_2 \vec{v}_2 + \cdots + c_n \vec{v}_n \rangle$
   $= c_1 \langle \vec{v}_j, \vec{v}_1 \rangle + c_2 \langle \vec{v}_j, \vec{v}_2 \rangle + \cdots + c_n \langle \vec{v}_j, \vec{v}_n \rangle$

Since, $\langle \vec{v}_j, \vec{v}_i \rangle = 0 \text{ if } i \neq j$
   $\vec{0}= c_j \langle \vec{v}_j, \vec{v}_j \rangle$

Since $\vec{v_{j}}$ is non zero, by property of inner product, $\langle \vec{v}_j, \vec{v}_j \rangle > 0$

$\therefore c_j = 0$

Since $j$ is arbitrary, all $c_{j}$'s are zero. Thus, $B$ is linear Independent

## Corollary
If a set of non-zero vectors is mutually orthogonal, and their number matches the dimension of the vector space, then they must form an orthogonal basis of that space

**Ex)** 
Show that the set of Pauli matrices
$B = \left\{ \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}, \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}, \begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix}, \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} \right\}$
is an orthogonal basis of $M_2(\mathbb{C})$. How could you turn it into an orthonormal basis?

We just need to check all 6 inner products equals zero
- Then, normalize it


# All inner products look like the dot product
Suppose that $B$ is an orthonormal basis of a finite-dimensional inner product space $\mathcal{V}$. Then  
$\langle \vec{v}, \vec{w} \rangle = [\vec{v}]_B \cdot [\vec{w}]_B \quad \text{for all} \quad \vec{v}, \vec{w} \in \mathcal{V}.$

**Proof)**
Write $B = \{\vec{u}_1, \vec{u}_2, \ldots, \vec{u}_n\}$. Since $B$ is a basis of $\mathcal{V}$, we can write $\vec{v} = c_1 \vec{u}_1 + \cdots + c_n \vec{u}_n$ and $\vec{w} = d_1 \vec{u}_1 + \cdots + d_n \vec{u}_n$. Then...

$\langle \vec{v}, \vec{w} \rangle = \left\langle \sum_{j=1}^n c_j \vec{u}_j, \sum_{i=1}^n d_i \vec{u}_i \right\rangle = \sum_{i=1}^n d_i \left\langle \sum_{j=1}^n c_j \vec{u}_j, \vec{u}_i \right\rangle$

   $= \displaystyle\sum_{i=1}^n d_i \sum_{j=1}^n c_j \langle \vec{u}_j, \vec{u}_i \rangle$

By property of inner product, $\langle \vec{u}_j, \vec{u}_i \rangle = 0 \quad \text{if } i \neq j$
   $= \displaystyle\sum_{i=1}^n d_i c_i \langle \vec{u}_i, \vec{u}_i \rangle$  

Since $\langle \vec{u}_j, \vec{u}_i \rangle = 1 \quad \text{if } i = j$
   $= \displaystyle\sum_{i=1}^n d_i c_i$

$= \begin{bmatrix}c_1 \\\vdots \\c_n\end{bmatrix}\cdot\begin{bmatrix}d_1 \\\vdots \\d_n\end{bmatrix}= [\vec{v}]_B \cdot [\vec{w}]_B$

## Specialization for $\mathbb{F}=\mathbb{C}^{n}$
Let $E$ be a standard basis of $\mathbb{C}^{n}$ and $B$ is any basis of $\mathbb{C}^{n}$
$\forall \text{ }\vec{v}, \vec{w}\in\mathbb{C}^{n}, \quad[\vec{v}]_{E}=\vec{v}\text{ }\&\text{ }[\vec{w}]_{E}=\vec{w}$
- $[\vec{v}]_{B}=P_{B\leftarrow E}[\vec{v}]=P_{B\leftarrow E}\cdot\vec{v}$
- $[\vec{w}]_{B}=P_{B\leftarrow E}[\vec{w}]=P_{B\leftarrow E}\cdot\vec{w}$

$\langle\vec{v}, \vec{w}\rangle=[\vec{v}]_B \cdot [\vec{w}]_B=(P_{B\leftarrow E}\cdot\vec{v})\cdot(P_{B\leftarrow E}\cdot\vec{w})=\vec{v}^{*}\cdot P_{B\leftarrow E}^{*}\cdot P_{B\leftarrow E}\cdot\vec{w}$

## Corollary 5.7 Invertible Matrices Make Inner Products
A function $\langle \cdot, \cdot \rangle : \mathbb{F}^n \times \mathbb{F}^n \to \mathbb{F}$ is an inner product if and only if there exists an invertible matrix $P \in \mathcal{M}_n(\mathbb{F})$ such that
$$\langle \vec{v}, \vec{w} \rangle = \vec{v}^*(P^*P)\vec{w} \quad \text{for all} \quad \vec{v}, \vec{w} \in \mathbb{F}^n$$

