# Field
A set on which we can add, subtract, multiply and divide according to the usual laws of arithmetic
- Denoted as $\mathbb{F}$

**Ex)** $\mathbb{R}, \mathbb{C},\mathbb{Z}_{p}$ ($p$ is prime), etc

## Properties
1. 0 is in the field
2. 1 is in the field
3. Associativity
4. Commutativity

# Addition and Scalar Multiplication
Let $\mathcal{V}$ be a set and let $\mathbb{F}$ be a field. Let $\vec{v}, \vec{w}\in \mathcal{V}$ and $c\in\mathbb{F}$ and suppose we have defined two operations called addition and scalar multiplication on $\mathcal{V}$.

We write the addition of $\vec{v}$ and $\vec{w}$ as $\vec{v}+\vec{w}$, and the scalar multiplication of $c$ and $\vec{v}$ as $c\vec{v}$

# Vector Space
If the following ten conditions hold for all $\vec{v}, \vec{w}, \vec{x}\in\mathcal{V}$ and all $c,d\in \mathbb{F}$, then $\mathcal{V}$ is called **vector space** and its elements are called **vectors**
1. $\vec{v}+\vec{w}\in\mathcal{V}$
2. $\vec{v}+\vec{w}=\vec{w}+\vec{v}$
3. $(\vec{v}+\vec{w})+\vec{x}=\vec{v}+(\vec{w}+\vec{x})$
4. There exists a zero vector $\vec{0}\in\mathcal{V}$ such that $\vec{v}+\vec{0}=\vec{v}$
5. There exists a vector $-\vec{v}$ such that $\vec{v}+(-\vec{v})=\vec{0}$
6. $c\vec{v}\in\mathcal{V}$
7. $c(\vec{v}+\vec{w})=c\vec{v}+c\vec{w}$
8. $(c+d)\vec{v}=c\vec{v}+d\vec{v}$
9. $c(d\vec{v})=(cd)\vec{v}$
10. $1\vec{v}=\vec{v}$

**Note)** At intro linear algebra, $\mathbb{F}=\mathbb{R}$

**Ex)** $\mathbb{R}^{n}$

**Ex)** $\mathcal{F}$, the set of all functions $f:\mathbb{R}\to\mathbb{R}$
4. Is there $\vec{0}\in\mathcal{F}$ with $\forall \text{ }f\in\mathcal{F}, f+\vec{0}=f$
- Let $\vec{0}$ be $0(x)=0$

5. Is there $-f\in\mathcal{F}$ for any $f\in\mathcal{F}$ such that $f+(-f)=\vec{0}$
- Define $(-f)(x)=-f(x)$

**Ex)** $\mathcal{M}_{m,n}(\mathbb{F})$, the set of all $m\times n$ matrices with entries from $\mathbb{F}$

# Subspace
If $\mathcal{V}$ is a vector space and $\mathcal{S}\subseteq\mathcal{V}$, then $\mathcal{S}$ is a subspace of $\mathcal{V}$ if $\mathcal{S}$ is itself a vector space with the same addition and scalar multiplication as $\mathcal{V}$

## Theorem 1.1 Determining if a Set is a Subspace
Let $\mathcal{V}$ be a vector space and let $\mathcal{S}\subseteq\mathcal{V}$ be non-empty. Then $\mathcal{S}$ is a subspace iff the following two conditions hold for all $\vec{v}, \vec{w}\in\mathcal{S}$ and all $c\in\mathbb{F}$
1. $\vec{v}+ \vec{w}\in\mathcal{S}$
2. $c\vec{v}\in\mathcal{S}$

Where addition and scalar multiplication is from $\mathcal{V}$

"If this two hold, then rest 8 hold automatically"

**Proof)**
i) $(\longrightarrow)$ Direction
Let $\mathcal{S}$ is subspace of $\mathcal{V}$. Then $\mathcal{S}$ is also a vector space by definition of subspace 
- Then by axiom 1), $\vec{v}+\vec{w}\in\mathcal{S}$
- By axiom 6), $c\vec{v}\in\mathcal{S}$

ii) $(\longleftarrow)$ Direction
Let $\mathcal{S}$ is a set that two conditions above hold for all elements in it.
- Let $\vec{v}, \vec{w}, \vec{x}\in\mathcal{S}$

We need to prove that $\mathcal{S}$ is a vector space with the same addition and scalar multiplication as $\mathcal{V}$ 
- Let's prove all 10 axioms for vector 

By condition 1), $\vec{v}+\vec{w}\in\mathcal{S}$, thus axiom 1 hold

By condition 2), $c\vec{v}\in\mathcal{S}$, thus axiom 6 hold

For axiom 2,3,7,8,9,10. Since $\mathcal{V}$ is a vector space, axiom 2,3,7,8,9,10 hold for $\mathcal{V}$. Since if $\vec{v}, \vec{w}, \vec{x}\in\mathcal{S}$, then $\vec{v}, \vec{w}, \vec{x}\in\mathcal{V}$, axioms hold for $\mathcal{S}$ as well

For axiom 5, since $\mathcal{V}$ is a vector space, $\exists \text{ }-\vec{v}\in\mathcal{V}$ such that $\forall \text{ }-\vec{v}\in\mathcal{V}$, $\vec{v}+(-\vec{v})=\vec{0}$
- Since $\forall \text{ }\vec{w}\in\mathcal{S}, \vec{w}\in \mathcal{V}$, thus $\exists \text{ }-\vec{w}\in\mathcal{V}$
- Then by condition 2), $-\vec{w}\in\mathcal{S}$

For axiom 4, from axiom 5), $\forall \text{ }\vec{v}\in\mathcal{S}$, $\exists \text{ }-\vec{v}\in\mathcal{S}$ such that $\vec{v}+(-\vec{v})=\vec{0}$
- And by condition 1) , $\vec{0}\in\mathcal{S}$

## Example) Is $\mathcal{P}^{p}$, the set of real-valued polynomials of degree at most $p$, a subspace of $\mathcal{F}$?
Yes, sum of polynomials is polynomial and scalar multiplication of polynomial is polynomial

## Example) Is the set of $2\times2$ matrices with determinant of 0 a subspace of $\mathcal{M}_{2}$
Define $\mathcal{M}_{n}(\mathbb{F})$ a set of all $n\times n$ matrices where all entries came from $\mathbb{F}$
- No

Counter-example)
$\begin{vmatrix}1&0\\0&0\end{vmatrix}=0$ and $\begin{vmatrix}0&0\\0&1\end{vmatrix}=0$

But $\det(\begin{bmatrix}1&0\\0&0\end{bmatrix}+\begin{bmatrix}0&0\\0&1\end{bmatrix})=\det(\begin{bmatrix}1&0\\0&1\end{bmatrix})=1\neq{0}$

# Linear Combination
Let $\mathcal{V}$ a vector space over the field $\mathbb{F}$. Let $\vec{v_{1}}, \vec{v_{2}}\dots \vec{v_{k}}\in\mathcal{V}$ and let $c_{1}, c_{2}\dots c_{n}\in\mathbb{F}$. Then every vector of the form
$$ c_{1}\vec{v_{1}}+c_{2}\vec{v_{2}}+\dots+c_{k}\vec{v_{k}}$$
- $k$ has to be finite
# Spans
Let $\mathcal{V}$ be a vector space and let $B\subseteq\mathcal{V}$ be a set of vectors. Then the span of $B$, denoted as $\text{span}(B)$, is the set of all **finite** linear combination of vectors from $B$
- If $\text{span}(B)=\mathcal{V}$, then $\mathcal{V}$ is said to be spanned by $B$, or $B$ spans $\mathcal{V}$

**Ex)**
$\mathcal{P}^{p}=\text{span}( 1,x,x^{2}\dots x^{p})$

**Ex)**
Is $e^{x}\in \text{span}(1,x,x^{2}\dots)$ ?
- By Taylor Series, it seems that $e^{x}$ can be written as linear combination, but actually this statement is not true
- Even though it is okay to put infinite sum in $\text{span}$ operation, a vector must be able to written as **finite** linear combination
- Intuitively, all element in the spanning set would be polynomial whose derivative eventually reach zero. However, derivative of $e^{x}$ never reaches zero

## Theorem 1.2 Spans are Subspace
Let $\mathcal{V}$ be a vector space and let $B\subseteq\mathcal{V}$. Then $\text{span}(B)$ is a subspace of $\mathcal{V}$

**Proof)**
By Theorem 1.1, we just need to prove two properties

Let $\vec{v}, \vec{w}\in \text{span}(B)$

i) Prove $\vec{v}+\vec{w}\in \text{span}(B)$
Since $\vec{v}, \vec{w}\in \text{span}(B)$, there are some scalars $c_{i}$'s and $d_{i}$'s such that
- $\vec{v}=c_{1}\vec{b_{1}}+c_{2}\vec{b_{2}}\dots c_{k}\vec{b_{k}}$
- $\vec{w}=d_{1}\vec{b_{1}}+d_{2}\vec{b_{2}}\dots d_{k}\vec{b_{k}}$

Then, $\vec{v}+\vec{w}=c_{1}\vec{b_{1}}+c_{2}\vec{b_{2}}\dots c_{k}\vec{b_{k}}+d_{1}\vec{b_{1}}+d_{2}\vec{b_{2}}\dots d_{k}\vec{b_{k}}$
		   $=(c_{1}+d_{1})\vec{b_{1}}+(c_{2}+d_{2})\vec{b_{2}}\dots (c_{k}+d_{k})\vec{b_{k}}$
- Since $c_{i}+d_{i}$'s are all valid scalar, $\vec{v}+\vec{w}\in \text{span}(B)$

ii) Prove $c\vec{v}\in \text{span}(B)$
Let $c\in\mathbb{F}$
Since $\vec{v}\in \text{span}(B)$, there are some scalars $c_{i}$'s such that
- $\vec{v}=c_{1}\vec{b_{1}}+c_{2}\vec{b_{2}}\dots c_{k}\vec{b_{k}}$

Then, $c\vec{v}=cc_{1}\vec{b_{1}}+cc_{2}\vec{b_{2}}\dots cc_{k}\vec{b_{k}}$
- Since $c\cdot c_{i}$'s are all valid scalar, $c\vec{v}\in \text{span}(B)$

# Linear Dependence and Independence
Let $\mathcal{V}$ a vector space over the field $\mathbb{F}$ and let $B\subseteq\mathcal{V}$ be a set of vectors. Then, $B$ is **linearly dependent** if there exists scalars $c_{1}, c_{2}\dots c_{n}\in\mathbb{F}$, at least one of which is not zero, and vectors $\vec{v_{1}}, \vec{v_{2}}\dots \vec{v_{k}}\in B$ such that
$$ c_{1}\vec{v_{1}}+c_{2}\vec{v_{2}}+\dots+c_{k}\vec{v_{k}}=\vec{0}$$

If $B$ is not dependent, then it is called **linearly independent**
- All scalar must be zero to satisfy above

$B$ is linearly dependent iff there exists a vector in $B$ that is linear combination of rest of vectors in $B$

**Ex)**
Is the set of functions $\{ \sin^{2}(x), \cos^{2}(x), \cos(2x) \}\subset\mathcal{F}$ linearly dependent or independent?

Check
- Does $c_{1}\sin^{2}(x)+ c_{2}\cos^{2}(x)+c_{3} \cos(2x)=0$ implies $c_{1}=c_{2}=c_{3}=0$?

From triangualr identity, $\cos(2x)=\cos^{2}(x)-\sin^{2}(x)$
- Then, $\sin^{2}(x)-\cos^{2}(x)+\cos(2x)=0$

Since scalars are not all zero, this set is linearly dependent

# Bases
A basis of a vector space $\mathcal{V}$ is a set of vectors in $\mathcal{V}$ that
1. Spans $\mathcal{V}$
2. Is linearly independent

$\implies$ Big enough to represent all vectors in the set but also small enough not to have any redundancies

$\implies$ Each basis represents the *direction that we can go in that vector space*

## Standard Basis of $\mathbb{R}^{n}$
Let $e_{i}$ be the vector in $\mathbb{R}^{n}$ with a $1$ in its $i^{th}$ entry and zeros else where. Then $\{ e_{1},e_{2},\dots e_{n} \}$ is a standard basis of $\mathbb{R}^{n}$

## Standard Basis of $\mathcal{M}_{m,n}$
Let $E_{i,j}\in\mathcal{M}_{m,n}$ be the matrix with a $1$ in its $(i,j)$ entry and zeros elsewhere. Then, $\{ E_{1,1}, E_{1,2}\dots E_{m,n} \}$ is a standard basis of $\mathcal{M}_{m,n}$


## Standard Basis of $\mathcal{P}^{p}$
Set of polynomials $\{ 1,x,x^{2}\dots x^{p} \}$ is a standard basis of $\mathcal{P}^{p}$

# Coordinate Vectors
## Theorem 2.1: Uniqueness of Linear Combination
Let $\mathcal{V}$ be a vector space and let $B$ be a basis for $\mathcal{V}$. Then for every $\vec{v}\in\mathcal{V}$, there is only one way to write $\vec{v}$ as a linear combination of the basis vectors in $B$

**Proof)**
Suppose there is scalar $c_{i}$'s and $d_{i}$'s such that 
- $\vec{v}=c_{1}\vec{v_{1}}+c_{2}\vec{v_{2}}\dots c_{n}\vec{v_{n}}$
- $\vec{v}=d_{1}\vec{v_{1}}+d_{2}\vec{v_{2}}\dots d_{n}\vec{v_{n}}$

Consider $\vec{v}-\vec{v}$
	$=c_{1}\vec{v_{1}}+c_{2}\vec{v_{2}}\dots c_{n}\vec{v_{n}}-d_{1}\vec{v_{1}}+d_{2}\vec{v_{2}}\dots d_{n}\vec{v_{n}}$
	$=(c_{1}-d_{1})\vec{v_{1}}+(c_{2}-d_{2})\vec{v_{2}}\dots (c_{n}-d_{n})\vec{v_{n}}=\vec{0}$

By linear independence of $B$, $c_{i}-d_{i}$ must be 0
- Therefore, $c_{i}=d_{i}$

## Coordinate Vectors
Suppose $\mathcal{V}$ is a vector sapce over a field $\mathbb{F}$ with a finite ordered basis $B=\{ \vec{v_{1}}, \vec{v_{2}}\dots \vec{v_{n}} \}$ and $\vec{v}\in\mathcal{V}$. Then the unique scalars $c_{1},c_{2},\dots c_{n}\in\mathbb{F}$ for which 
$$\vec{v}=c_{1}\vec{v_{1}}+c_{2}\vec{v_{2}}\dots c_{n}\vec{v_{n}}$$
are called Coordinates of $\vec{v}$ with respect to $B$, and the vector
$$[\vec{v}]_{B}=(c_{1},c_{2},\dots c_{n})$$
is called Coordinate vectors of $\vec{v}$ with respect to $B$

**Note)** The *order* in which the basis vectors appear in $B$ affects the order of the entries in the coordinate vector

**Ex)** Find the coordinate vector of $p(x)=1-x+3x^{2}$ with respect to the basis $B=\{ x^{2},x,1 \}$
- $[p]_{B}=(3,-1,4)$
## Arithmetic of Coordinate Vectors
Let $\vec{v}, \vec{w}\in\mathcal{V}$ and $c\in\mathbb{F}$
- $[\vec{v}+\vec{w}]_{B}=[\vec{v}]_{B}+[\vec{w}]_{B}$
- $[c\vec{v}]_{B}=c[\vec{v}]_{B}$

# Dimension of Vector Space

## Theorem 2.2 Linearly Independent Set versus Spanning Set
Let $\mathcal{V}$ be a vector space with basis $B$ of size $n$. Then
1. Any set of more than $n$ vectors in $\mathcal{V}$ must be linearly dependent
2. Any set of fewer than $n$ vectors cannot span $\mathcal{V}$

**Proof)**
i) Any set of more than $n$ vectors in $\mathcal{V}$ must be linearly dependent
Suppose for $m>n$ vectors, which we call $\vec{v_{1}}, \vec{v_{2}}\dots \vec{v_{m}}$ we want to solve $c_{1}\vec{v_{1}}+c_{2} \vec{v_{2}}\dots +c_{m}\vec{v_{m}}=\vec{0}$. 

This is the same as $[c_{1}\vec{v_{1}}+c_{2} \vec{v_{2}}\dots +c_{m}\vec{v_{m}}]_{B}=[\vec{0}]_{B}=\vec{0}$

However, direct calculation shows that $[c_{1}\vec{v_{1}}+c_{2} \vec{v_{2}}\dots +c_{m}\vec{v_{m}}]_{B}=c_{1}[\vec{v_{1}}]_{B}+c_{2} [\vec{v_{2}}]_{B}\dots +c_{m}[\vec{v_{m}}]_{B}$
- This can be written as matrix multiplication : $\big[[\vec{v_{1}}]_{B}, [\vec{v_{2}}]_{B}\dots ,[\vec{v_{m}}]_{B}\big]\cdot \begin{bmatrix}c_{1}\\c_{2}\\ \vdots\\c_{m}\end{bmatrix}$
	- Note that $\big[[\vec{v_{1}}]_{B}, [\vec{v_{2}}]_{B}\dots ,[\vec{v_{m}}]_{B}\big]$ is $n\times m$ matrix

Therefore, $\big[[\vec{v_{1}}]_{B}, [\vec{v_{2}}]_{B}\dots ,[\vec{v_{m}}]_{B}\big]\cdot \begin{bmatrix}c_{1}\\c_{2}\\ \vdots\\c_{m}\end{bmatrix}=\vec{0}$

Since this is a homogenous linear system with $m$ variables and $n$ equations and $m>n$, there is infinitely many solution
- Therefore, $\{ \vec{v_{1}}, \vec{v_{2}}\dots \vec{v_{m}} \}$ is linearly dependent

ii) Any set of fewer than $n$ vectors cannot span $\mathcal{V}$

**Exercise)**
## Corollary 2.3: Uniqueness of Size of Bases
Let $\mathcal{V}$ be a vector space that has a basis consisting of $n$ vectors. Then every basis of $\mathcal{V}$ has exactly $n$ vectors

**Proof)**
Let $c$ is arbitrary basis of $\mathcal{V}$ consisting of $m$ vectors
- If $m>n$, then it is not linearly independent, thus it cannot be a basis
- If $m<n$, then it cannot span $\mathcal{V}$, thus it cannot be a basis

Therefore, $m=n$

## Dimension of a Vector Space
A vector space $\mathcal{V}$ is called
- **Finite-Dimensional :** If it has a finite basis, and its dimension, denoted by $\text{dim}(\mathcal{V})$, is the number of vectors in one its bases
- **Infinite-Dimensional :** If it has no finite basis, and we way that $\text{dim}(\mathcal{V})=\infty$

# Change of Basis
## Change-of-Basis Matrix
Suppose $\mathcal{V}$ is a vector space with bases $B=\{ \vec{v_{1}}, \vec{v_{2}}, \dots, \vec{v_{n}} \}$ and $C$. Then, the change-of-basis Matrix from $B$ to $C$, denoted by $P_{C\leftarrow B}$, is the $n\times n$ matrix whose columns are the coordinate vectors $[\vec{v_{1}}]_{C}, [\vec{v_{2}}]_{C}\dots [\vec{v_{n}}]_{C}$
$$P_{C\leftarrow B}=\Big[[\vec{v_{1}}]_{C}, [\vec{v_{2}}]_{C}\dots [\vec{v_{n}}]_{C}\Big]$$
**Q)** Is this transition matrix?

## Theorem 2.4 Change-of-Basis Matrices
Suppose $B$ and $C$ are bases of finite-dimensional vector space $\mathcal{V}$, and let $P_{C\leftarrow B}$ be the change-of-basis matrix. Then,
1. $\forall \text{ }\vec{v}\in\mathcal{V}, \quad P_{C\leftarrow B}\cdot[\vec{v}]_{B}=[\vec{v}]_{C}$ 
2. $P_{C\leftarrow B}$ is invertible and $P^{-1}_{C\leftarrow B}=P_{B\leftarrow C}$

Furthermore, $P_{C\leftarrow B}$ is unique with property 1.

**Proof)**
i) Property 1
Suppose $\vec{v}\in\mathcal{V}$ and write $\vec{v}=c_{1}\vec{v_{1}}+c_{2}\vec{v_{2}}\dots c_{n}\vec{v_{n}}$ so that $[\vec{v}]_{B}=(c_{1},c_{2},\dots c_{n})$
Then, $P_{C\leftarrow B}\cdot[\vec{v}]_{B}=\Big[[\vec{v_{1}}]_{C}, [\vec{v_{2}}]_{C}\dots [\vec{v_{n}}]_{C}\Big]\cdot \begin{bmatrix}c_{1}\\c_{2}\\\vdots\\c_{n}\end{bmatrix}$
	$=c_{1}[\vec{v_{1}}]_{C}+ c_{2}[\vec{v_{2}}]_{C}+\dots+c_{n} [\vec{v_{n}}]_{C}$
	$=[c_{1}\vec{v_{1}}+c_{2}\vec{v_{2}}+\dots+ c_{n}\vec{v_{n}}]_{C}=[\vec{v}]_{C}$

ii) Property 2
Consider $P_{B\leftarrow C}P_{C\leftarrow B}[\vec{v}]_{B}$
	$=P_{B\leftarrow C}[\vec{v}]_{C}=[\vec{v}]_{B}$

Thus, $P_{B\leftarrow C}P_{C\leftarrow B}=I$
- $P^{-1}_{C\leftarrow B}=P_{B\leftarrow C}$

iii) Uniqueness of $P_{C\leftarrow B}$
Suppose $P\in \mathcal{M}_{n}$ such that $P[\vec{v}]_{B}=[\vec{v}]_{C}$ for all $\vec{v}\in\mathcal{V}$

Let $\vec{v_{j}}\in B$
- Since $\vec{v_{j}}$ is in $B$, $[\vec{v_{j}}]_{B}$ would look like there is 1 in $j^{th}$ entry and 0 elsewhere
- $\therefore [\vec{v_{j}}]_{B}=\vec{e_{j}}$

Consider $P[\vec{v_{j}}]_{B}$
- By above, $=P\cdot \vec{e_{j}}$

Since $P[\vec{v}]_{B}=[\vec{v}]_{C}$ by initial supposition, $P[\vec{v_{j}}]_{B}=P\cdot \vec{e_{j}}=[\vec{v_{j}}]_{C}$
- Which means that $[\vec{v_{j}}]_{C}$ is $j^{th}$ column of $P$
- Which is just an definition of $P_{C\leftarrow B}$

Therefore, $P=P_{C\leftarrow B}$

### Example)
Find the change-of-basis matrix $P_{C\leftarrow B}$ for the bases $B=\{ 1,x,x^{2} \}$ and $C=\{ 1+x, 1+x^{2}, x+x^{2} \}$ of $\mathcal{P}^{2}$. Then find the coordinate vector of $p(x)=4-x+3x^{2}$ with respect to $C$

Rather than calculaint $[p]_{C}$, since calculating coordinate vector with $B$ is easier, we will calculate $P_{C\leftarrow B}\cdot[p]_{B}$ instead

$[p]_{B}=(4,-1,3)$ and $P_{C\leftarrow B}=\Big[[1]_{C}, [x]_{C}, [x^{2}]_{C}\Big]$
- However, since calculating coordinate vector with $B$ is easier than $C$, instead of calculating $P_{C\leftarrow B}$, we will calculate $P^{-1}_{B\leftarrow C}$

$P^{-1}_{B\leftarrow C}=\Big[[1+x]_{B}, [1+x^{2}]_{B}, [x+x^{2}]_{B}\Big]^{-1}=\begin{bmatrix}1&1&0\\1&0&1\\0&1&1\end{bmatrix}^{-1}=\frac{1}{2}\begin{bmatrix}1&1&-1\\1&-1&1\\-1&1&1\end{bmatrix}$

$\therefore [p]_{C}=P_{C\leftarrow B}[p]_{B}=P^{-1}_{B\leftarrow C}[p]_{B}=\frac{1}{2}\begin{bmatrix}1&1&-1\\1&-1&1\\-1&1&1\end{bmatrix}\cdot \begin{bmatrix}4\\-1\\3\end{bmatrix}=\frac{1}{2}\begin{bmatrix}0\\8\\-2\end{bmatrix}=\begin{bmatrix}0\\4\\-1\end{bmatrix}$

## Theorem 2.5 Computing Change-of-Basis- Matrices
Let $\mathcal{V}$ be a finite-dimensional vector space with bases $B,C$ and $E$. Then the reduced row echelon form of the augmented matrix
$$[P_{E\leftarrow C}\text{ | }P_{E\leftarrow B}]\quad\text{is}\quad [I\text{ | }P_{C\leftarrow B}]$$
- $E$ can be any matrix, but It is easier to compute if $E$ is a standard basis

**Proof)**
Let $B=\{ \vec{v_{1}}, \vec{v_{2}}, \dots , \vec{v_{n}} \}$

Suppose we want to compute $[\vec{v_{j}}]_{C}$ for some $j\in[1,n]$
- $j^{th}$ column of $P_{C\leftarrow B}$

Consider $P_{E\leftarrow C}[\vec{v_{j}}]_{C}=[\vec{v_{j}}]_{E}$
- We can think this as a linear system that we could use to solve for $[\vec{v_{j}}]_{C}$ from $[\vec{v_{j}}]_{E}$
- i.e. $[\vec{v_{j}}]_{C}$ is solution to the linear system $[P_{E\leftarrow C}\text{ | }[\vec{v_{j}}]_{E}]$

By Theorem 2.4, $P_{E\leftarrow C}$ is invertible
- And by [[4. Matrices#Fundamental Theorem of Invertible Matrices|1) of FToIM]], since $P_{E\leftarrow C}$ is invertible, linear system $[P_{E\leftarrow C}\text{ | }[\vec{v_{j}}]_{E}]$ has R/REF of $[I\text{ | }[\vec{v_{j}}]_{C}]$

Therefore, $\forall \text{ }j\in[1,n]$, $[P_{E\leftarrow C}\text{ | }[\vec{v_{j}}]_{E}]$ has R/REF of $[I\text{ | }[\vec{v_{j}}]_{C}]$
- Let's put all $[\vec{v_{j}}]_{E}$ at the right hand side of the system as a one whole matrix

Consider $\Big[P_{E\leftarrow C}\Huge\text{ | }\normalsize\big[[\vec{v_{1}}]_{E}, [\vec{v_{2}}]_{E}\dots, [\vec{v_{n}}]_{E}\big]\Big]$
- The same sequence of row operations can be used to reduce each $[\vec{v_{j}}]_{E}$, thus $\Big[P_{E\leftarrow C}\Huge\text{ | }\normalsize\big[[\vec{v_{1}}]_{E}, [\vec{v_{2}}]_{E}\dots, [\vec{v_{n}}]_{E}\big]\Big]$ would have R/REF $\Big[I\Huge\text{ | }\normalsize\big[[\vec{v_{1}}]_{C}, [\vec{v_{2}}]_{C}\dots, [\vec{v_{n}}]_{C}\big]\Big]$

Since $P_{E\leftarrow B}=\big[[\vec{v_{1}}]_{E}, [\vec{v_{2}}]_{E}\dots, [\vec{v_{n}}]_{E}\big]$ and $P_{C\leftarrow B}=\big[[\vec{v_{1}}]_{C}, [\vec{v_{2}}]_{C}\dots, [\vec{v_{n}}]_{C}\big]$, $[P_{E\leftarrow C}\text{ | } P_{E\leftarrow B} ]$ has R/REF $[I\text{ | } P_{C\leftarrow B}]$

## Example)
Find the change-of-basis matrix $P_{C\leftarrow B}$, where 
$$B=\{ -x+x^{2}, 3+2x^{2, 5} \}\quad\text{and}\quad C=\{ 3x, 1-2x+x^{2}, -x^{2} \}$$
are basis of $\mathcal{P}^{2}$. Then, compute $[\vec{v}]_{C}$ if $[\vec{v}]_{B}=(1,2,3)$

Let $E=\{ 1,x,x^{2} \}$. Then
$$P_{E\leftarrow B}=\begin{bmatrix}
0&3&5 \\
-1&0&0 \\
1&2&0
\end{bmatrix}\quad\text{and}\quad P_{E\leftarrow C}=\begin{bmatrix}
0&1&0 \\
3&-2&0 \\
0&1&-1
\end{bmatrix}$$

Consider $[P_{E\leftarrow C}\text{ | }P_{E\leftarrow B}]$

$\implies\begin{bmatrix}0&3&5&|&0&1&0 \\-1&0&0&|&3&-2&0 \\1&2&0&|&0&1&-1\end{bmatrix}\xrightarrow{\text{row operations}}\begin{bmatrix}1&0&0&|&-\frac{1}{3}&2&\frac{10}{3} \\0&1&0&|&0&3&5 \\0&0&1&|&-1&1&5\end{bmatrix}$

$\therefore P_{C\leftarrow B}=\begin{bmatrix}-\frac{1}{3}&2&\frac{10}{3} \\0&3&5 \\-1&1&5\end{bmatrix}$

$[\vec{v}]_{C}=P_{C\leftarrow B}\cdot[\vec{v}]_{B}=\begin{bmatrix}-\frac{1}{3}&2&\frac{10}{3} \\0&3&5 \\-1&1&5\end{bmatrix}\cdot \begin{bmatrix}1\\2\\3\end{bmatrix}=\begin{bmatrix}\frac{41}{3}\\21\\16\end{bmatrix}$

# Linear Transformation
Let $\mathcal{V}$ and $\mathcal{W}$ be vector spaces over the same field $\mathbb{F}$. A Linear Transformation is a function $T:\mathcal{V}\to \mathcal{W}$ that satisfies the following two properties
1. $\forall \text{ }\vec{v}, \vec{w}\in\mathcal{V}$, $T(\vec{v}+\vec{w})$
2. $\forall \text{ }\vec{v}\in\mathcal{V}$ and $\forall \text{ }c\in\mathbb{F}$, $T(c\cdot\vec{v})=c\cdot T(\vec{v})$

**Ex)** 
Every matrix transformation is a linear transformation
- $A\in \mathcal{M}_{m,n}$ , then the function$T_{A}:\mathbb{R}^{n}\to\mathbb{R}^{m}$, defined by $T_{A}(\vec{v})=A\vec{v}$ is linear transformation

**Ex)**
Function $T:\mathcal{M}_{m,n}\to\mathcal{M}_{n, m}$ that sends a matrix to its transpose is a linear transformation

**Ex)**
Let $\mathcal{D}$ be a vector space of all differentiable function. Then, differentiation map $D:\mathcal{D}\to\mathcal{F}$, which sends s differentiable function to its derivative, is a linear transformation

## For all Linear Transformation , $T(\vec{0})=\vec{0}$
$T(\vec{0})=T(0\cdot\vec{v})=0\cdot T(\vec{v})=\vec{0}$

## Special Transformation
Zero Transformation
- $O:\mathcal{V}\to \mathcal{W}$
- $\forall \text{ }\vec{v}\in\mathcal{V}, \quad O(\vec{v})=\vec{0}$

Identity Transformation
- $I:\mathcal{V}\to \mathcal{V}$
- $\forall \text{ }\vec{v}\in\mathcal{V}, \quad O(\vec{v})=\vec{v}$

## Linear Transformation in different vector spaces
Single Linear Transformation can be considered as acting different vector spaces
- Depends on intension towards that LT

**Ex)** LT $D$ above can be considered as a LT from $\mathcal{P}^{3}$ to $\mathcal{P}^{2}$

# Theorem 3.1 Standard Matrix of Linear Transformation
Let $\mathcal{V}$ and $\mathcal{W}$ be vector spaces with bases $B$ and $D$, where $B=\{ \vec{v_{1}}, \vec{v_{2}}\dots \vec{v_{n}} \}$ and $D$ is $m$ dimensional.

A function $T:\mathcal{V}\to \mathcal{W}$ is a linear transformation iff $\exists \text{ }$a matrix $[T]_{D\leftarrow B}\in \mathcal{M}_{m,n}$ such that
$$\forall \text{ }\vec{v}\in\mathcal{V},\quad[T(\vec{v})]_{D}=[T]_{D\leftarrow B}[\vec{v}]_{B}$$

Furthermore, the unique matrix $[T]_{D\leftarrow B}$ with this property is called the standard matrix of $T$ with respect to the bases $B$ and $D$, and it it
$$[T]_{D\leftarrow B}=\Big[[T(\vec{v_{1}})]_{D}, [T(\vec{v_{2}})]_{D}\dots[T(\vec{v_{n}})]_{D}\Big]$$


