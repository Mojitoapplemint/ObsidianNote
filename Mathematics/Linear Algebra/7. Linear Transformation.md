 # Linear Transformation
Let $\mathcal{V}$ and $\mathcal{W}$ be vector spaces over the same field $\mathbb{F}$. A Linear Transformation is a function $T:\mathcal{V}\to \mathcal{W}$ that satisfies the following two properties
1. $\forall \text{ }\vec{v}, \vec{w}\in\mathcal{V}$, $T(\vec{v}+\vec{w})$
2. $\forall \text{ }\vec{v}\in\mathcal{V}$ and $\forall \text{ }c\in\mathbb{F}$, $T(c\cdot\vec{v})=c\cdot T(\vec{v})$

**Ex)** 
Every matrix transformation is a linear transformation
- $A\in \mathcal{M}_{m,n}$ , then the function$T_{A}:\mathbb{R}^{n}\to\mathbb{R}^{m}$, defined by $T_{A}(\vec{v})=A\vec{v}$ is linear transformation

**Ex)**
Function $T:\mathcal{M}_{m,n}\to\mathcal{M}_{n, m}$ that sends a matrix to its transpose is a linear transformation

**Ex)**
Let $\mathcal{D}$ be a vector space of all differentiable function. Then, differentiation map $D:\mathcal{D}\to\mathcal{F}$, which sends s differentiable function to its derivative, is a linear transformation

## For all Linear Transformation , $T(\vec{0})=\vec{0}$
$T(\vec{0})=T(0\cdot\vec{v})=0\cdot T(\vec{v})=\vec{0}$

## Special Transformation
Zero Transformation
- $O:\mathcal{V}\to \mathcal{W}$
- $\forall \text{ }\vec{v}\in\mathcal{V}, \quad O(\vec{v})=\vec{0}$

Identity Transformation
- $I:\mathcal{V}\to \mathcal{V}$
- $\forall \text{ }\vec{v}\in\mathcal{V}, \quad O(\vec{v})=\vec{v}$

## Linear Transformation in different vector spaces
Single Linear Transformation can be considered as acting different vector spaces
- Depends on intension towards that LT

**Ex)** LT $D$ above can be considered as a LT from $\mathcal{P}^{3}$ to $\mathcal{P}^{2}$

## Theorem 3.1 Standard Matrix of Linear Transformation
Let $\mathcal{V}$ and $\mathcal{W}$ be vector spaces with bases $B$ and $D$, where $B=\{ \vec{v_{1}}, \vec{v_{2}}\dots \vec{v_{n}} \}$ and $D$ is $m$ dimensional.

A function $T:\mathcal{V}\to \mathcal{W}$ is a linear transformation iff $\exists \text{ }$a matrix $[T]_{D\leftarrow B}\in \mathcal{M}_{m,n}$ such that
$$\forall \text{ }\vec{v}\in\mathcal{V},\quad[T(\vec{v})]_{D}=[T]_{D\leftarrow B}[\vec{v}]_{B}$$

Furthermore, the unique matrix $[T]_{D\leftarrow B}$ with this property is called the **standard matrix** of $T$ with respect to the bases $B$ and $D$, and it is
$$[T]_{D\leftarrow B}=\Big[[T(\vec{v_{1}})]_{D}, [T(\vec{v_{2}})]_{D}\dots[T(\vec{v_{n}})]_{D}\Big]$$

**Note)**
- The matrix $[T]_{D\leftarrow B}$ tells us how to convery coordinate vectors of $\vec{v}\in\mathcal{V}$ to coordinate vectors of $T(\vec{v})\in \mathcal{W}$
- Using this theorem, we can think of every linear transformation $T:\mathcal{V}\to \mathcal{W}$ as a matrix
- The standard matrix looks differnt depending on the bases $B$ and $D$

### Proof
Let $\vec{v}=c_{1}\vec{v_{1}}+c_{2}\vec{v_{2}}+\dots+c_{n}\vec{v_{n}}$ so $[\vec{v}]_{B}=(c_{1},c_{2},\dots c_{n})$

Then, $[T]_{D\leftarrow B}[\vec{v}]_{B}=\Big[[T(\vec{v_{1}})]_{D}, [T(\vec{v_{2}})]_{D}\dots[T(\vec{v_{n}})]_{D}\Big]\cdot \begin{bmatrix}c_{1}\\c_{2}\\ \vdots\\c_{n}\end{bmatrix}$
	$=c_{1}[T(\vec{v_{1}})]_{D}+c_{2} [T(\vec{v_{2}})]_{D}\dots c_{n}[T(\vec{v_{n}})]_{D}$
	$=\Big[c_{1}T(\vec{v_{1}})+c_{2}T(\vec{v_{2}})\dots c_{n}T(\vec{v_{n}})\Big]_{D}$
	$=\Big[T(c_{1}\vec{v_{1}}+c_{2}\vec{v_{2}}\dots c_{n}\vec{v_{n}})\Big]_{D}$
	$=[T(\vec{v})]_{D}$

## Composition of Linear Transformation
When applying two or more linear transformation to a vector, we can combine two linear transformation into a single new function that is called their composition
$$\forall \text{ }\vec{v}\in\mathcal{V}\quad(S\circ T)(\vec{v})=S(T(\vec{v}))$$

### Theorem 3.2 Composition of LT
Suppose $\mathcal{V}, \mathcal{W}, \mathcal{X}$ are finite-dimensional vector spaces with bases $B,C$ and $D$, respectively. If $T:\mathcal{V}\to \mathcal{W}$ and $S:\mathcal{W}\to \mathcal{X}$ are linear transformations thn $S\circ T:\mathcal{V}\to \mathcal{X}$ is linear transformation, and its standard matrix is
$$[S\circ T]_{D\leftarrow B}=[S]_{D\leftarrow C}[T]_{C\leftarrow B}$$

**Proof)** Let $\vec{v}\in\mathcal{V}$. 

Consider $[S]_{D\leftarrow C}[T]_{C\leftarrow B}[\vec{v}]_{B}$

Since $T$ is LT, $[T]_{C\leftarrow B}\cdot[\vec{v}]_{B}=[T(\vec{v})]_{C}$ by Theorem 3.1
- $[S]_{D\leftarrow C}[T]_{C\leftarrow B}[\vec{v}]_{B}=[S]_{D\leftarrow C}[T(\vec{v})]_{C}$

Since $S$ is LT, $[S]_{D\leftarrow C}[T(\vec{v})]_{C}=[S(T(\vec{v}))]_{D}= [S\circ T(\vec{v})]_{D}$ by Theorem 3.1

Since $S\circ T$ is linear tranformation, $[S\circ T(\vec{v})]_{D}=[S\circ T]_{D\leftarrow B}[\vec{v}]_{B}$ by Theorem 3.1
- Therefore, $[S]_{D\leftarrow C}[T]_{C\leftarrow B}[\vec{v}]_{B}=[S\circ T]_{D\leftarrow B}[\vec{v}]_{B}$

$\therefore [S]_{D\leftarrow C}[T]_{C\leftarrow B}=[S\circ T]_{D\leftarrow B}$

## Power of Linear Transformation
Composing the same LT to each other 
$$T^{k}=T\circ T\circ T\dots\circ T $$

### Example
Use standard matrices to compute the fourth derivative of $x^{2}e^{x}+2xe^{x}$

Let $B=\{ e^{x}, xe^{x}, x^{2}e^{x} \}$, $\mathcal{V}=\text{span}(B)$ and $D:\mathcal{V}\to\mathcal{V}$ be the differentiation map
- Since we know that derivatives of $x^{2}e^{x}+2xe^{x}$ would always consist of $e^{x}, xe^{x}, x^{2}e^{x}$, we will express $D^{4}$ with respect to $B$, which means $[D^{4}]_{B}$
- $[D^{4}]_{B}$ represents a linear transformation corresponds to fourth derivative, thus we want to calculate $[D^{4}(x^{2}e^{x}+2xe^{x})]_{B}$

Since $D(e^{x})=e^{x}$, $D(xe^{x})=e^{x}+xe^{x}$ and $D(x^{2}e^{x})=2xe^{x}+x^{2}e^{x}$
- $[D(e^{x})]_{B}=(1,0,0)$
- $[D(xe^{x})]_{B}=(1,1,0)$
- $[D(x^{2}e^{x})]_{B}=(0,2,1)$

Thus, $[D]_{B}=\Big[[D(e^{x})]_{B},[D(xe^{x})]_{B}, [D(x^{2}e^{x})]_{B} \Big]=\begin{bmatrix}1&1&1\\0&1&2\\0&0&1\end{bmatrix}$

$[D^{4}]_{B}=[D^{2}]_{B}^{2}=\begin{bmatrix}1&4&12\\0&1&8\\0&0&1\end{bmatrix}$

$[D^{4}(x^{2}e^{x}+2xe^{x})]_{B}=[D^{4}]_{B}[x^{2}e^{x}+2xe^{x}]_{B}=\begin{bmatrix}1&4&12\\0&1&8\\0&0&1\end{bmatrix}\cdot \begin{bmatrix}0\\2\\1\end{bmatrix}=\begin{bmatrix}20\\10\\1\end{bmatrix}$

$\therefore D^{4}(x^{2}e^{x}+2xe^{x})=20e^{x}+10xe^{x}+x^{2}e^{x}$

## Theorem 3.3 Change of Basis for Linear Transformation
Let $T:\mathcal{V}\to \mathcal{W}$ be a linear transformation between finite-dimensional vector spaces $\mathcal{V}$ and $\mathcal{W}$, and let $B$ and $C$ be bases of $\mathcal{V}$, while $D$ and $E$ are bases of $\mathcal{W}$. Then,
$$[T]_{E\leftarrow C}=P_{E\leftarrow D}[T]_{D\leftarrow B}P_{B\leftarrow C}$$
Where $P_{E\leftarrow D}$ and $P_{B\leftarrow C}$ is change-of-basis matrices

Let $\vec{v}\in V$

Consider $P_{E\leftarrow D}[T]_{D\leftarrow B}P_{B\leftarrow C}[\vec{v}]_{C}$
	$=P_{E\leftarrow D}[T]_{D\leftarrow B}[\vec{v}]_{B}$
	$=P_{E\leftarrow D}[T(\vec{v})]_{D}$
	$=[T(\vec{v})]_{E}$

Consider  $[T]_{E\leftarrow C}[\vec{v}]_{C}$
	$=[T(\vec{v})]_{E}$

$\therefore[T]_{E\leftarrow C}=P_{E\leftarrow D}[T]_{D\leftarrow B}P_{B\leftarrow C}$

# Invertibility of Linear Transformation
A linear tranformation $T:\mathcal{V}\to \mathcal{W}$ is called invertible if there is a linear transformation $T^{-1}:\mathcal{W}\to\mathcal{V}$ such that 
$$\forall \text{ }\vec{v}\in\mathcal{V}, \quad T^{-1}(T(\vec{v}))=\vec{v}$$
and
$$\forall \text{ }\vec{w}\in\mathcal{W}, \quad T(T^{-1}(\vec{w}))=\vec{w}$$
That is 
$T\circ T^{-1}=I_{\mathcal{V}}$ and $T\circ T^{-1}=I_{\mathcal{W}}$
- $I_{\mathcal{V}}$: Identity on $\mathcal{V}$
- $I_{\mathcal{W}}$: Identity on $\mathcal{W}$

## Theorem 4.1 Invertibility of Linear Transformation
Let $T:\mathcal{V}\to \mathcal{W}$ be a linear transformation between $n$-dimensional vector spaces $\mathcal{V}$ and $\mathcal{W}$, which have bases $B$ and $D$. Then $T$ is invertible iff the $[T]_{D\leftarrow B}$ is invertible and
$$([T]_{D\leftarrow B})^{-1}=[T^{-1}]_{B\leftarrow D}$$
- i.e. Linear Transformation is invertible iff its standard matrix is invertible


**Proof)**
i) $(\longrightarrow)$ Direction: Suppose $T$ is invertible

Consider $[T^{-1}]_{B\leftarrow D}[T]_{D\leftarrow B}$

by Theorem 3.2,
	$=[T^{-1}\circ T]_{B}$ 
	$=[I_{\mathcal{V}}]_{B}=I$

If two matrices multiply to $I$, they are inverse
- $[T]_{D\leftarrow B}$ is invertible and $([T]_{D\leftarrow B})^{-1}=[T^{-1}]_{B\leftarrow D}$

ii) $(\longleftarrow)$: Suppose $[T]_{D\leftarrow B}$ is invertible. Let $A=([T]_{D\leftarrow B})^{-1}$

Since Matrix multiplication is always linear transformation, there is some linear transformation $S:\mathcal{W}\to\mathcal{V}$ such that $A=[S]_{B\leftarrow D}$
- Is this true?

Let $\vec{v}\in\mathcal{V}$
$[\vec{v}]_{B}=I\cdot[\vec{v}]_{B}=A\cdot[T]_{D\leftarrow B}\cdot[\vec{v}]_{B}=[S]_{B\leftarrow D}\cdot[T]_{D\leftarrow B}\cdot[\vec{v}]_{B}=[S]_{B\leftarrow D}[T(\vec{v})]_{D}=[S\circ T(\vec{v})]_{B}$

Since $[\vec{v}]_{B}=[I_{\mathcal{V}}(\vec{v})]_{B}=[S\circ T(\vec{v})]_{B}$
- $S\circ T=I_{\mathcal{V}}$

Similarly, 
$T\circ S=I_{\mathcal{W}}$

Therefore $T$ is invertible, and its inverse is $S$
- $[T^{-1}]_{B\leftarrow D}=[S]_{B\leftarrow D}=A=([T]_{D\leftarrow B})^{-1}$

## Another way to check Invertibility
If $\mathcal{V}$ and $\mathcal{W}$ are finite-dimensional with $\text{dim}(\mathcal{V})=\text{dim}(\mathcal{W})$, then $T:\mathcal{V}\to \mathcal{W}$ is invertible iff $T(\vec{v})=\vec{0}$ implies $\vec{v}=\vec{0}$

# Properties of Linear Transformation
## Definition
$\text{range(T)}$ = $\{ T(\vec{v}):\vec{v}\in \mathcal{V} \}$
- Set of all outputs
- Note that this is subspace of $\mathcal{W}$ if $T:\mathcal{V}\to \mathcal{W}$

$\text{rank}(T)=\text{dim}(\text{range}(T))=\text{rank}([T]_{D\leftarrow B})$

$\text{null}(T)=\{ \vec{v}\in \mathcal{V}:T(\vec{v})=\vec{0} \}$
- This is a subspace of $\mathcal{V}$

$\text{nullity}(T)=\text{dim}(\text{null(T)})=\text{nullity}([T]_{D\leftarrow B})$
- Number of leading variable

$\lambda$ is an eigenvalue of $T$ with corresponding eigenvector $\vec{v}$ if $T(\vec{v})=\lambda \vec{v}$ and $\vec{v}\neq \vec{0}$

**Note)** These properties can all computed from the standard matrix

# Isomorphism
Suppose $\mathcal{V}$ and $\mathcal{W}$ are vector spaces over the same field. We say that $\mathcal{V}$ and $\mathcal{W}$ are isomorphic, denoted by $\mathcal{V}\cong\mathcal{W}$, if there are exists an invertible linear transofmration $T:\mathcal{V}\to \mathcal{W}$
- $T$ is called isomorphism from $\mathcal{V}$ to $\mathcal{W}$
- They have the same structure as each other
	- The only difference is the label given to their members
	- If $\vec{v}\in \mathcal{V}$, then $T(\vec{v})\in \mathcal{W}$

**Ex)**
$\mathcal{M}_{1,n}\cong \mathcal{M}_{n,1}$ are isomorphic
- Let $T$ be the tranpose map
- $T$ is clearly invertible

## Theorem 4.2 : Isomorphisms of Finite-Dimensional Vector
Suppose $\mathcal{V}$ is an $n$-dimensional vector space over a field $\mathbb{F}$. Then $\mathcal{V} \cong \mathbb{F}^n$

**Proof)**
Pick some basis $B$ of $\mathcal{V}$ and consider the function $T:\mathcal{V}\to\mathbb{F}^{n}$

Consider $T(\vec{v})=[\vec{v}]_{B}$. For all $\vec{v}, \vec{w} \in \mathcal{V}$ and $c \in \mathbb{F}$,  
- $[\vec{v} + \vec{w}]_B = [\vec{v}]_B + [\vec{w}]_B$
- $[c\vec{v}]_B = c[\vec{v}]_B$ 
so $T$ is a linear transformation.  

It's invertible because $\dim(\mathcal{V}) = \dim(\mathbb{F}^n)$ and  
$T(\vec{v}) = \vec{0}$ implies $\vec{v} = \vec{0}$  
(since $[\vec{v}]_B = \vec{0}$ means  
$\vec{v} = 0 \vec{v}_1 + 0 \vec{v}_2 + \dots + 0 \vec{v}_n = \vec{0}$).

Therefore, $\mathbb{F}$. Then $\mathcal{V} \cong \mathbb{F}^n$

## Isomorphic is Transitive
If $\mathcal{V}\cong\mathcal{W}$ and $\mathcal{W}\cong\mathcal{X}$, then $\mathcal{V}\cong\mathcal{X}$
- Linear Transformation is transitive

## Corollary : Finite Dimensional Vector spaces and Isomorphism
Suppose $\mathcal{V}$ and $\mathcal{W}$ are vector spaces over the same field and $\mathcal{V}$ is finite dimensional. Then $\mathcal{V} \cong \mathcal{W}$ if and only if $\dim(\mathcal{V}) = \dim(\mathcal{W})$.

**Proof)**
$(\to)$

$(\leftarrow)$
Proved by Theorem 4.1

# Roots of Linear Transformation
## Recall: Diagonalization
For square matrix $A\in \mathcal{M}_{n}$ we can write
$$A=PDP^{-1}$$
where $D\in \mathcal{M}_{n}$ is diagonal and $P\in \mathcal{M}_{n}$ is invertible
- $D$'s diagonal entries must be eigenvalues of $A$, not order sensitive
- $P$'s columns must be eigenvectors of $A$, matching order with $D$

Then, power of $A$ is
$$A^{r} = PDP^{-1}\cdot PDP^{-1}\dots PDP^{-1}=PD^{r}P^{-1}$$
where $D^{r}$ can simply be computerd entrywise

**Note)** Due to standard matrix, we can do the samething for most linear transformation

## Application 1 : Finding a square root LT
Consisder square root of some linear transformation $T$ as another linear transformation $S$ such that $S^{2}=T$
- If we apply $S$ twice, then it is equivalent to applying $T$ once

**Ex)** Find a square root of the transpose map acting on $\mathcal{M}_{2}$

We want $S:\mathcal{M}_{2}\to \mathcal{M}_{2}$ such that $S^{2}=T\text{ (transpose map})$
- Then, $S=T^{\frac{1}{2}}$

To use diagonal properties, we want $T$ to be diagonal
- If $B=\left\{ \begin{bmatrix}1&0\\0&1\end{bmatrix}, \begin{bmatrix}0&1\\1&0\end{bmatrix}, \begin{bmatrix}0&-i\\i&0\end{bmatrix}, \begin{bmatrix}1&0\\0&-1\end{bmatrix} \right\}$, then $[T]_{B}=\begin{bmatrix}1&0&0&0\\0&1&0&0\\0&0&-1&0\\0&0&0&1\end{bmatrix}$

Thus, $[T]^{\frac{1}{2}}_{B}=\begin{bmatrix}\sqrt{ 1 }&0&0&0\\0&\sqrt{ 1 }&0&0\\0&0&\sqrt{ -1 }&0\\0&0&0&\sqrt{ 1 }\end{bmatrix}=\begin{bmatrix}1&0&0&0\\0&1&0&0\\0&0&i&0\\0&0&0&1\end{bmatrix}$

Therefore, $S=T^{\frac{1}{2}}$ is the linear transformation for which 
- $S\left(\begin{bmatrix}1&0\\0&1\end{bmatrix}\right)=\begin{bmatrix}1&0\\0&1\end{bmatrix}$
- $S\left(\begin{bmatrix}0&1\\1&0\end{bmatrix}\right)=\begin{bmatrix}0&1\\1&0\end{bmatrix}$
- $S\left(\begin{bmatrix}0&-i\\i&0\end{bmatrix}\right)=i \begin{bmatrix}0&-i\\i&0\end{bmatrix}=\begin{bmatrix}0&1\\-1&0\end{bmatrix}$
- $S\left(\begin{bmatrix}1&0\\0&-1\end{bmatrix}\right)=\begin{bmatrix}1&0\\0&-1\end{bmatrix}$

